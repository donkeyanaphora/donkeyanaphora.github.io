<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <!-- mobile layout -->
  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <title></title>

  <!-- site look & feel -->
  <link rel="stylesheet" href="../../assets/css/main.css"/>

    <!-- ‚îÄ‚îÄ MathJax config: no indent + fix \text{} baseline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <script>
    window.MathJax = {
      /* remove default ‚Äì0.25 em display indent and keep left-aligned   */
      tex:   { displayIndent: '0em',  displayAlign: 'left' },

      /* inherit the surrounding page font for \text{‚Ä¶} so the baseline
         matches math italic on hi-DPI screens (stops 1-px jog)        */
      /* chtml: { mtextInheritFont: true } */
         chtml: { mtextFont: 'Times' }
    };
  </script>
  
  <!-- MathJax -->
  <script defer id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>
</head>

<body class="article">
  <!-- dark-mode toggle -->
  <button id="toggleDark" class="float-btn" aria-label="Toggle dark mode">üåô</button>

  <!-- Markdown content -->
  <main class="content">
    <article class="markdown-body">
      <h1 id="using-linear-models-in-modern-applications">Using Linear
      Models in Modern Applications</h1>
      <p><strong>COLLINS WESTNEDGE</strong><br />
      <em>MAY 16, 2025</em></p>
      <h2 id="introduction">Introduction</h2>
      <p>I‚Äôve been really busy so this will be a little informal but
      <strong>STILL WORTH THE READ</strong> because in the day to day
      landscape of people talking agentic AI, RAG systems, vector
      databases etc things like least squares regression and linear
      models tend to feel like dusty sklearn imports or the stuff of
      technical coding interviews. Despite the heavy focus on foundation
      model integration classic linear tools like ordinary least squares
      regression and its projection formula corrolary remain
      surprisingly powerful. By framing modern tasks like semantic
      filtering and bias removal as geometric problems, it becomes clear
      why simple formulas like <span class="math display">\[
      X(X^{\mathsf T}X)^{+}X^{\mathsf T}
      \]</span> work so well. The focus of this article is show how
      ordinary least squares offers insights as well as stable and
      theoretically grounded solution to semantic filtering in modern
      search and retrieval systems.</p>
      <h2 id="linalg-recap">Linalg Recap</h2>
      <p><em>lorem ipsum</em></p>
      <h3 id="key-formulas">Key Formulas</h3>
      <table>
      <colgroup>
      <col style="width: 33%" />
      <col style="width: 33%" />
      <col style="width: 33%" />
      </colgroup>
      <thead>
      <tr>
      <th>Formula</th>
      <th>Intuitive role</th>
      <th>Visual cue</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>(X^{+} = (X<sup>{!}X)</sup>{-1}X^{!})</td>
      <td>Recipe for weights ‚Äî solves (_{}y - X_2^{,2}).</td>
      <td>‚ÄúHow much of each arrow do I need?‚Äù</td>
      </tr>
      <tr>
      <td>(=X^{+}y)</td>
      <td>Coordinates along the arrows (scalar steps to reach the
      closest point).</td>
      <td>Labels on the sub-space axes.</td>
      </tr>
      <tr>
      <td>(P_X = X,X^{+})</td>
      <td>Snap-to-span operator. Orthogonal projector (symmetric,
      idempotent).</td>
      <td>Elevator lowering you to the sheet.</td>
      </tr>
      <tr>
      <td>(=P_X,y = X)</td>
      <td>Foot of the perpendicular: the unique point in ((X))
      minimizing (y-_2).</td>
      <td>Black dot where the plumb-line hits.</td>
      </tr>
      <tr>
      <td>(r = y- = (I-P_X),y)</td>
      <td>Residual vector, always orthogonal to every column of (X)
      ((X^{!}r=0)).</td>
      <td>Red straight-down arrow.</td>
      </tr>
      </tbody>
      </table>
      <h3 id="theorems">Theorems</h3>
      <p><strong>Best Approximation Theorem</strong><br />
      Let (W^n), (y^n), and (=_W(y)). Then [ y-_2 ;&lt;; y-v_2, ,vW,;v.
      ] Hence (y-W^{}) and is the closest vector in (W^{}) to (y).</p>
      <h3 id="take-away-corollary">Take-away Corollary</h3>
      <p>If (<em>{} = <em>V()), then [ </em>{} ;=; - </em>{} ;;V^{} ] is
      the closest-possible embedding to () that is perfectly orthogonal
      to (V).</p>
      <h4 id="tldr">TL;DR</h4>
      <p>Define [ <em>{} ;=; </em>{wV^{}}-w ;=; _{V^{}}(). ]</p>
      <ol type="1">
      <li><strong>Minimal distortion:</strong> (_{}) is the closest
      vector in (V^{}) to ().<br />
      </li>
      <li><strong>Guaranteed orthogonality:</strong> (_{},v= 0) for
      every (vV).</li>
      </ol>
      <h2 id="visual-intuition">Visual Intuition</h2>
      <h2 id="mean-subtraction-vs.-projection-residuals">Mean
      Subtraction vs.¬†Projection Residuals</h2>
      <h2 id="examples">Examples</h2>
      <h2 id="conclusion">Conclusion</h2>
    </article>
  </main>

  <!-- site JS -->
  <script defer src="../../assets/js/site.js"></script>
</body>
</html>
