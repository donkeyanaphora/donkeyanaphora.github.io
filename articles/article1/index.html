<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <!-- mobile layout -->
  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <title></title>

  <!-- site look & feel -->
  <link rel="stylesheet" href="../../assets/css/main.css"/>

  <!-- MathJax -->
  <script defer id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body class="article">
  <!-- dark-mode toggle -->
  <button id="toggleDark" class="float-btn" aria-label="Toggle dark mode">üåô</button>

  <!-- Markdown content -->
  <main class="content">
    <article class="markdown-body">
      <h1
      id="ai-integration-beyond-the-buzz-a-technical-exploration-through-shallow-fusion">AI
      Integration Beyond the Buzz: A Technical Exploration through
      Shallow Fusion</h1>
      <p><strong>Collins Westnedge</strong><br />
      <em>April 16, 2025</em></p>
      <h2 id="introduction">Introduction</h2>
      <p>AI adoption and integration have become focal points in
      seemingly every earnings call, linkedin post, townhall and
      industry keynote. However, most of these conversations exist to
      highlight revenue potential, promote products and services, or
      bolster positive consumer sentiment, which is likely why they tend
      to gloss over or abstract away the technical challenges that stand
      in the way of effective adoption.</p>
      <p>The fundamental challenge is the gap between available data and
      the data needed for a domain-specific task this is known as the
      the <a
      href="https://situational-awareness.ai/from-gpt-4-to-agi/#The_data_wall">data
      wall</a>. Consider for example applying a large generalist model
      to a highly specialized task that barely surfaces in its
      pretraining data if at all. For the generalist model to succeed,
      it must first grasp dense company prospectuses, specialized
      jargon, and the nuances of the business problem itself. To address
      this gap companies often resort to standard recipes
      e.g.¬†‚Äúexciting‚Äù the right activations through few-shot examples,
      dumping streams of internal documents into the model‚Äôs context, or
      ambitious attempts at fine-tuning on small internal datasets.
      However, with these approaches there‚Äôs often no optimization
      signal or gradient to follow and progress if there‚Äôs any to be had
      involves a good deal of guesswork, trial, and error.</p>
      <p>In contrast to the popular narrative, my day-to-day experience
      has shown me that these discussions routinely overlook the many
      technical challenges mentioned above. As someone stuck in the
      generalist specialist divide, it‚Äôs hard not to feel frustration
      when leaders boast an AI integration victory that is nothing more
      than an API call. This disconnect motivated me to develop a case
      study leveraging <strong>shallow fusion</strong>. The goal of
      which is twofold: highlight the technical challenges of AI
      integration and demonstrate how bridging this divide can close the
      gap between AI‚Äôs marketing promises and its operational
      reality.</p>
      <h2 id="a-formal-example-shallow-fusion">A Formal Example: Shallow
      Fusion</h2>
      <p>But first, what is shallow fusion? Consider for example, a
      person listening to audio of a phone call with a customer and
      customer service agent at an insurance claims calls center. The
      sole function of this person is to transcribe what they hear into
      text. The caveat, however, is that they only know very little
      about the domain and the types of technical issues and medical
      terminology e.g.¬†(procedures diagnoses etc) that representatives
      and customers are mentioning. Now consider a second person who has
      worked in this industry for many years and has a deep
      understanding of the domain, but is hard of hearing.</p>
      <p>Shallow fusion can be thought of as a process of integrating
      each person‚Äôs expertise to offset the errors of one another and
      bridge modalities the other does not have access to. With this
      analogy we can now formally describe this process. In the example
      below think of <span class="math inline">\(P_{\text{ASR}}\)</span>
      as the person listening to the audio and <span
      class="math inline">\(P_{\text{LM}}\)</span> as the domain expert
      that is hard of hearing but deeply understands the context.</p>
      <h4 id="mathematical-formulation">Mathematical Formulation</h4>
      <p>At each decoding step for some audio input, we select the most
      probable token <span class="math inline">\(y_{t}\)</span> using
      information from the Automatic Speech Recognition model (ASR) and
      the Language Model (LM)</p>
      <p><span class="math display">\[
      y^* = \arg\max_{y_t}\;
      \left[
      \log P_{\text{ASR}}\!\bigl(y_t \mid x,\, y_{\lt t}\bigr)
      \;+\;
      \lambda\,\log P_{\text{LM}}\!\bigl(y_t \mid y_{\lt t}\bigr)
      \right]
      \]</span></p>
      <p>where:<br />
      - <span class="math inline">\(t\)</span> is the decoding step
      (0-based).<br />
      - <span class="math inline">\(y_t\)</span> is the chosen token at
      step <span class="math inline">\(t\)</span> and <span
      class="math inline">\(y_{\lt  t}\)</span> are previously generated
      tokens.<br />
      - <span class="math inline">\(x\)</span> represents the acoustic
      features (e.g.¬†raw audio input).<br />
      - <span class="math inline">\(P_{\text{ASR}}\)</span> depends on
      both <span class="math inline">\(x\)</span> and <span
      class="math inline">\(y_{\lt  t}\)</span>, while <span
      class="math inline">\(P_{\text{LM}}\)</span> depends on <span
      class="math inline">\(y_{\lt  t}\)</span> only.<br />
      - <span class="math inline">\(\lambda\)</span> is the weighting
      factor to determine the language model‚Äôs influence.</p>
      <p>The idea is that the ASR model understands phonetics and
      language in a general sense while the LM model understands the
      specialized domain in its written form, but has no access to the
      audio signal. Just like in the analogy from earlier by fusing
      their predictions, we combine phonetic understand with domain
      expertise, leading to more accurate transcriptions for our domain.
      Without careful integration or synergy between the two, both
      models carry major limitations.</p>
      <h4 id="process-diagram">Process Diagram:</h4>
      <p><img src="assets/viz.png" alt="diagram" /> Reference: <a
      href="https://arxiv.org/pdf/1712.01996">Kannan et al.¬†2017</a></p>
      <h4 id="in-practice">In Practice</h4>
      <p>In the workflow that I dive into, token prediction is
      implemented in stages. Early on we rely solely on the ASR model
      and then gradually introduce the language model as more context
      becomes available. For example:</p>
      <p><strong>Token Selection at Step (t)</strong></p>
      <p><span class="math display">\[
      \mathbf{F}(x,t)=
      \begin{cases}
      \displaystyle
      \operatorname*{arg\,max}\limits_{y_t}\;
            \log P_{\text{ASR}}\!\left(y_t \mid x,\; y_{&lt;t}\right),
            &amp; t &lt; \text{initial_steps},\\[0.75em]
      \displaystyle
      \operatorname*{arg\,max}\limits_{y_t}\;
            \Bigl[
              \log P_{\text{ASR}}\!\left(y_t \mid x,\; y_{&lt;t}\right)
              + \lambda\,\log P_{\text{LM}}\!\left(y_t \mid
      y_{&lt;t}\right)
            \Bigr],
            &amp; t \ge \text{initial_steps}.
      \end{cases}
      \]</span></p>
      <p>This piecewise approach allows the system to build confidence
      from the raw audio transcription initially before incorporating
      the domain expert corrections, namely because our starting point
      should be conditionalized on something observed e.g.¬†we gotta
      start somewhere.</p>
      <h2 id="case-study-in-transcription-a-concrete-example">Case Study
      in Transcription: A Concrete Example</h2>
      <p>For our models let‚Äôs take Whisper to be our ASR model and GTP2
      to be our LM. In practice these models share a tokenizer making
      the process of integrating their predictions fairly seamless at
      least for the english version of Whisper (<a
      href="https://arxiv.org/pdf/2212.04356">Radford 2.2</a>). Now
      let‚Äôs consider a claims call center transcript where an ASR model
      misinterprets a specialized medical term.</p>
      <p><strong>Input Audio (Ground Truth):</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs <code>melanoma</code>.‚Äù‚úîÔ∏è</p>
      <p><strong>Whisper Initial Output:</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs <code>diploma</code>.‚Äùüö´</p>
      <h4 id="whisper-initial-decoding">1. <strong>Whisper Initial
      Decoding:</strong></h4>
      <p>Whisper produces logits at each step:</p>
      <ul>
      <li>Token: ‚ÄúThe‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äúprocedure‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äúclaimant‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äú‚Äôs‚Äù ‚Üí high confidence<br />
      </li>
      <li>At the final subword, Whisper may exhibit uncertainty,
      spreading probabilities across candidates: ‚Äúdiploma‚Äù, ‚Äúaroma‚Äù,
      ‚Äúmelanoma‚Äù</li>
      </ul>
      <h4 id="domain-gpt-2-predictions">2. <strong>Domain GPT-2
      Predictions:</strong></h4>
      <p>At this ambiguous decoding step, GPT-2 (the domain-adapted LM)
      produces logits based on the following context:</p>
      <ul>
      <li><p>‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs <code>_____</code>‚Äù</p></li>
      <li><p>GPT-2 which as been fine tuned on medical literature
      strongly favors the correct token (produces log probabilities
      closer to 0 for melanoma) while Whisper, which had minimal access
      to medical terminology, assigns it a much lower likelihood (log
      probabilities that are more negative).</p></li>
      </ul>
      <table>
      <thead>
      <tr>
      <th>Next Token</th>
      <th>Whisper Log Probs</th>
      <th>GPT-2 Log Probs</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>melanoma</strong></td>
      <td><strong>‚Äì1.8</strong></td>
      <td><strong>‚Äì0.3</strong></td>
      </tr>
      <tr>
      <td>diploma</td>
      <td>‚Äì1.0</td>
      <td>‚Äì5.0</td>
      </tr>
      <tr>
      <td>aroma</td>
      <td>‚Äì3.5</td>
      <td>‚Äì3.8</td>
      </tr>
      </tbody>
      </table>
      <h4 id="shallow-fusion-combining-logits">3. <strong>Shallow Fusion
      (Combining Logits):</strong></h4>
      <p>We combine each model‚Äôs logits using a weighted sum in the
      following way:</p>
      <p><span class="math display">\[
      \log P_{\text{combined}}(y_t)=
      \log P_{\text{Whisper}}\!\bigl(y_t \mid x,\, y_{\lt t}\bigr)
      +\lambda\,\log P_{\text{GPT2}}\!\bigl(y_t \mid y_{\lt t}\bigr)
      \]</span></p>
      <table>
      <colgroup>
      <col style="width: 24%" />
      <col style="width: 24%" />
      <col style="width: 22%" />
      <col style="width: 28%" />
      </colgroup>
      <thead>
      <tr>
      <th>Next Token</th>
      <th>Whisper Score</th>
      <th>GPT-2 Score</th>
      <th>Combined Score where <span
      class="math inline">\(\lambda\)</span> = 0.2</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>melanoma</strong></td>
      <td><strong>‚Äì1.8</strong></td>
      <td><strong>‚Äì0.3</strong></td>
      <td><strong>‚Äì1.8 + 0.2 <span class="math inline">\(\times\)</span>
      (‚Äì0.3) = ‚Äì1.86</strong></td>
      </tr>
      <tr>
      <td>diploma</td>
      <td>‚Äì1.0</td>
      <td>‚Äì5.0</td>
      <td>‚Äì1.0 + 0.2 <span class="math inline">\(\times\)</span> (‚Äì5.0)
      = ‚Äì2.0</td>
      </tr>
      <tr>
      <td>aroma</td>
      <td>‚Äì3.5</td>
      <td>‚Äì3.8</td>
      <td>‚Äì3.5 + 0.2 <span class="math inline">\(\times\)</span> (‚Äì3.8)
      = ‚Äì4.26</td>
      </tr>
      </tbody>
      </table>
      <p><em>Note: The numbers are illustrative. In practice additional
      context and scaling would favor the correct token ‚Äúmelanoma‚Äù;
      additionally, rare words are likely split into multiple tokens but
      the intuition remains the same.</em></p>
      <p>‚Äúmelanoma‚Äù now has the highest combined score.</p>
      <h4 id="final-corrected-output"><strong>Final Corrected
      Output:</strong></h4>
      <p>‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs <code>melanoma</code>.‚Äù‚úîÔ∏è</p>
      <h4 id="key-takeaway">Key Takeaway:</h4>
      <ul>
      <li><strong>Without GPT-2:</strong> The model misrecognized
      domain-specific terms.<br />
      </li>
      <li><strong>With shallow fusion:</strong> The external domain LM
      (GPT-2) provided strong guidance toward the correct
      domain-specific vocabulary, correcting Whisper‚Äôs initial
      mistakes.</li>
      </ul>
      <p>This demonstrates how <strong>domain-aware shallow
      fusion</strong> can significantly improve ASR output in
      specialized contexts.</p>
      <h2 id="overcoming-the-data-wall">Overcoming the Data Wall</h2>
      <p>In the example above we demonstrated how the integration looks
      in practice, but we also assumed the language model (GPT-2) in our
      case has already been adapted to the appropriate domain. In
      practice we can fairly assume that out-of-the-box ASR models
      perform poorly on ‚Ä¶</p>
      <h2 id="reflection-and-future-directions">Reflection and Future
      Directions</h2>
      <ul>
      <li>learnable or dynamic <span
      class="math inline">\(\lambda\)</span><br />
      </li>
      <li>MoE<br />
      </li>
      <li>Cold Fusion<br />
      </li>
      <li>Deep Fusion</li>
      </ul>
      <h2 id="conclusion">Conclusion</h2>
      <ul>
      <li>articulate each model‚Äôs strength and weakness, emphasizing how
      each model hits its own data wall separately:
      <ul>
      <li>Whisper (generalist): broadly trained acoustic-to-text model,
      struggles with specialized terminology.<br />
      </li>
      <li>GPT-2 (specialist): trained in a self-supervised way solely on
      textual domain data, rich in domain-specific vocabulary but blind
      to acoustic signals.<br />
      </li>
      </ul></li>
      <li>reiterate how useless these models are without deliberate and
      challenging integration steps e.g.¬†adapting GPT-2 to your
      domain<br />
      </li>
      <li>illustrate how this process extends or relates to broader
      trends within AI e.g.¬†Ensemble Architectures like Mixture of
      Experts, multimodal integration, domain adaptation and evolution
      of fusion techniques (cold &amp; deep).<br />
      </li>
      <li>End with a note about out-of-the-box plug-and-play solutions
      not being competitive because it‚Äôs low-hanging fruit and everyone
      has the same boring RAG systems.</li>
      </ul>
      <h2 id="resources">Resources</h2>
      <ul>
      <li><a
      href="https://research.facebook.com/file/551805355910423/Deep-Shallow-Fusion-for-RNN-T-Personalization.pdf">Deep
      Shallow Fusion for RNN-T Personalization</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/1712.01996">Analysis of
      Incorporating an External Language Model‚Ä¶</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/2212.04356">Robust Speech
      Recognition via Large-Scale Weak Supervision</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/1503.03535">On Using
      Monolingual Corpora in Neural Machine Translation</a><br />
      </li>
      <li><a
      href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language
      Models are Unsupervised Multitask Learners</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/2005.14165">Language Models are
      Few-Shot Learners</a></li>
      </ul>
    </article>
  </main>

  <!-- site JS -->
  <script defer src="../../assets/js/site.js"></script>
</body>
</html>
