<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Shallow Fusion: Bridging Data Scarcity and AI Integration
Challenges - Collins Westnedge</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no, viewport-fit=cover">
  
  <!-- Mobile app settings -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- SEO essentials -->
  <meta name="google-site-verification" content="hmW81pKLkPLJXY0kFe-I1vNXno-xD9XbEWyZyGfz-SA" />
  <meta name="description" content="An exploration of shallow fusion as
a method to address data scarcity and integration in specialized
domains">
  <meta name="author" content="Collins Westnedge">
  <link rel="canonical" href="https://donkeyanaphora.github.io/articles/article1/">
  
  <!-- Hide drafts from search engines -->
  <meta name="robots" content="noindex, nofollow">

  <!-- Site look & feel -->
  <link rel="stylesheet" href="../../assets/css/main.css"/>
  <link rel="stylesheet" href="../../assets/css/article.css"/>
  
  <!-- Favicons -->
  <link rel="apple-touch-icon" sizes="180x180" href="../../favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../../favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../../favicon/favicon-16x16.png">
  <link rel="manifest" href="../../favicon/site.webmanifest">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://donkeyanaphora.github.io/articles/article1/">
  <meta property="og:title" content="Shallow Fusion: Bridging Data
Scarcity and AI Integration Challenges">
  <meta property="og:description" content="An exploration of shallow
fusion as a method to address data scarcity and integration in
specialized domains">
  <meta property="og:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">
  <meta property="og:site_name" content="The Latent Realm">
  <meta property="article:author" content="Collins Westnedge">
  <meta property="article:published_time" content="2025-08-26">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:url" content="https://donkeyanaphora.github.io/articles/article1/">
  <meta name="twitter:title" content="Shallow Fusion: Bridging Data
Scarcity and AI Integration Challenges">
  <meta name="twitter:description" content="An exploration of shallow
fusion as a method to address data scarcity and integration in
specialized domains">
  <meta name="twitter:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">

  <!-- MathJax config -->
  <!-- <script>
    window.MathJax = {
      tex:   { displayIndent: '0em',  displayAlign: 'center' },
      chtml: { mtextFont: 'Times' }
    };
  </script> -->
  
  <script>
    window.MathJax = {
      tex: {},  // Empty is fine
      chtml: { 
        mtextFont: 'Times',
        displayAlign: 'center',    // ‚úÖ Moved here
        displayIndent: '0em'       // ‚úÖ Moved here
      }
    };
  </script>
  <!-- MathJax -->
  <script defer id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js">
  </script>
</head>

<body class="article">
  <!-- floating buttons -->
  <button id="menuBtn" class="float-btn" aria-label="Open menu">üß∞</button>
  <button class="float-btn back-btn" onclick="window.location.href='../../'" aria-label="Back to home">üè°</button>
  <button id="toggleDark" class="float-btn" aria-label="Toggle dark mode">üåô</button>

  <!-- Markdown content -->
  <main class="content">
    <article class="markdown-body">
      <h1
      id="shallow-fusion-bridging-data-scarcity-and-ai-integration-challenges">üîó
      Shallow Fusion: Bridging Data Scarcity and AI Integration
      Challenges</h1>
      <p><strong>COLLINS WESTNEDGE</strong><br />
      <em>AUGUST 26, 2025</em></p>
      <h2 id="introduction">Introduction</h2>
      <p>AI adoption and integration have become focal points in
      seemingly every earnings call, LinkedIn post, townhall and
      industry keynote. However, most of these conversations exist to
      highlight revenue potential, promote products and services, or
      bolster positive consumer sentiment, which is likely why they tend
      to gloss over or abstract away the technical challenges that stand
      in the way of effective adoption. One of the fundamental
      challenges is the gap between available data and the data needed
      for a domain-specific task.</p>
      <p>Consider, for example, applying a large generalist model to a
      highly specialized task that barely surfaces in its pretraining
      data if at all. For the generalist model to succeed, it must first
      grasp dense company prospectuses, specialized jargon, and the
      nuances of the business problem itself. To address this gap
      companies often resort to standard recipes, e.g., ‚Äúexciting‚Äù the
      right activations through few-shot examples, dumping streams of
      internal documents into the model‚Äôs context, or ambitious attempts
      at fine-tuning on small internal datasets. However, with most of
      these approaches there‚Äôs often no optimization signal, or gradient
      to move against and progress, if any, involves a good deal of
      guesswork, trial, and error.</p>
      <p><strong>Automatic Speech Recognition (ASR)</strong> exemplifies
      this challenge. Many domains, such as medicine, law, and financial
      services contain specialized terminology that is typically outside
      the distribution or under-represented in the pretraining for
      general purpose models. A model trained on everyday speech will
      struggle with phrases like ‚Äúorthostatic tachycardia‚Äù or
      specialized homophones that are difficult to disambiguate, such as
      ‚ÄúICU‚Äù versus ‚ÄúI see you‚Äù. Traditional solutions to this issue
      involve collecting domain-specific audio and ground truth
      transcriptions (often hand labeled) which can be cost prohibitive.
      Open source datasets on specialized domains are becoming more
      common but their volume and variety remain limited, keeping them
      tangential to many business use cases.</p>
      <p>This distribution gap has motivated researchers and
      practitioners to explore the concept of <strong>shallow
      fusion</strong>: combining general-purpose ASR models with
      domain-specific language models during inference. Rather than
      requiring extensive retraining, shallow fusion leverages existing
      domain expertise from an external language model at inference
      time. While the approach has shown promise in various
      implementations, the questions I would like to explore in this
      article are: Can a language model trained on domain-specific text
      meaningfully improve speech-to-text transcription quality within
      an adjacent domain? And critically, what are the failure modes
      associated with this type of integration?</p>
      <h2 id="background-existing-approaches">Background &amp; Existing
      Approaches</h2>
      <p>The challenge of domain adaptation in ASR has prompted several
      approaches, each with distinct trade-offs in cost, performance,
      and implementation complexity. Before diving into my
      implementation, I‚Äôll examine how the research community has
      approached this domain mismatch problem and where shallow fusion
      fits among existing solutions.</p>
      <h3 id="traditional-domain-adaptation">Traditional Domain
      Adaptation</h3>
      <p><strong>Traditional domain adaptation</strong> typically
      requires collecting domain-specific audio paired with ground truth
      transcriptions, then fine-tuning pretrained models on this data.
      While effective, this approach faces significant barriers:
      domain-specific audio is expensive to collect, transcription
      labeling is labor-intensive, and the resulting datasets often
      remain small and brittle compared to the large scale datasets that
      the base model was trained on. This approach runs the risk of
      <strong>catastrophic forgetting</strong><sup>1</sup> where the
      model loses its general capabilities when adapting to the specific
      domain.</p>
      <h3 id="context-injection-methods">Context Injection Methods</h3>
      <p><strong>Context injection methods</strong> attempt to bridge
      the gap by incorporating domain-specific text directly into the
      model‚Äôs context window, essentially ‚Äúprompting‚Äù the ASR system
      with relevant terminology. However, these approaches offer no
      optimization signal and rely heavily on trial and error to achieve
      meaningful improvements. They are also architecture dependent and
      rely on the decoder‚Äôs prompting capacity, which may be limited in
      models not explicitly designed for such conditioning.</p>
      <h3 id="fusion-techniques">Fusion Techniques</h3>
      <p><strong>Fusion techniques</strong> represent a middle ground,
      combining predictions from multiple models during inference rather
      than requiring extensive retraining. The research community has
      explored three primary variants:</p>
      <ul>
      <li><p><strong>Shallow fusion</strong> combines model predictions
      at inference time via a weighted average of ASR and LM scores,
      requiring no additional training (Gulcehre et al., 2015).</p></li>
      <li><p><strong>Deep fusion</strong> augments the decoder with a
      small gating network that learns to merge hidden representations
      from the ASR and LM while keeping both models frozen (Gulcehre et
      al., 2015).</p></li>
      <li><p><strong>Cold fusion</strong> builds on the idea of deep
      fusion but with a key difference: instead of training the ASR
      model first and then adding a language model later, the ASR model
      is trained from scratch alongside a fixed, pretrained LM (Sriram
      et al., 2017).</p></li>
      </ul>
      <!-- Because the ASR model is exposed to the LM throughout training, it learns to rely on the LM for linguistic information while dedicating its own capacity to mapping acoustic features into text. This disentanglement allows even relatively small decoders to perform well.   -->
      <p>Shallow fusion‚Äôs appeal lies in its simplicity and flexibility,
      as it requires no additional training of the base ASR model.
      Instead, you incorporate predictions from an external language
      model directly at inference time, blending the acoustic model‚Äôs
      view of the audio with the language model‚Äôs understanding of
      domain-specific text. Importantly, the only data needed to build
      or adapt the external language model is unstructured text, which
      can be collected far more easily than audio transcriptions.</p>
      <p>However, the approach introduces its own challenges. If the
      language model is weighted too heavily, it may bias transcriptions
      toward plausible but incorrect tokens; too lightly, and the domain
      benefits are lost. Tuning the weighting factor for the external
      model often requires domain-specific adjustment. In addition,
      shallow fusion increases inference cost since predictions must run
      through a second model<sup>2</sup>. These trade-offs make it
      essential to understand the method‚Äôs failure modes before
      deploying it in practice.</p>
      <h2
      id="implementation-medical-domain-fusion-pipeline">Implementation:
      Medical Domain Fusion Pipeline</h2>
      <p>Having established the landscape of existing approaches, we can
      now detail the implementation of shallow fusion for medical ASR,
      combining Whisper (our ASR model) with a domain-adapted GPT-2
      model (our external language model). However, before going into
      the specifics let us first build some intuition on the topic by
      analogy.</p>
      <h3 id="conceptual-framework">Conceptual Framework</h3>
      <p>Consider, for example, a person tasked with transcribing audio
      from a phone call between a customer and a claims representative
      at an insurance call center. This transcriber can hear the
      conversation clearly, but they have very little knowledge of the
      domain, e.g., the technical issues, procedures, and medical
      terminology that often come up. Now imagine a second person who
      has worked in this industry for years and has deep familiarity
      with the jargon and context, but who is hard of hearing.</p>
      <p>In practice, the first person might hear a phrase like
      ‚Äúmyocardial infarction‚Äù but misrecognize or misspell it. The
      domain expert, although unable to hear the audio, would
      immediately recognize the intended term and correct the
      transcript.</p>
      <p>Shallow fusion can be thought of as a process of integrating
      each person‚Äôs expertise to offset the errors of one another and
      bridge modalities the other does not have access to. With this
      analogy, we can now formally describe this process. In the example
      below think of <span class="math inline">\(P_{\text{ASR}}\)</span>
      as the person listening to the audio and <span
      class="math inline">\(P_{\text{LM}}\)</span> as the domain expert
      that is hard of hearing but deeply understands the context.</p>
      <h3 id="mathematical-formulation">Mathematical Formulation</h3>
      <p>At each decoding step for some audio input, we select the most
      probable token <span class="math inline">\(y_{t}\)</span> using
      information from the Automatic Speech Recognition model (ASR) and
      the Language Model (LM)</p>
      <p><span class="math display">\[
      y^* = \arg\max_{y_t}\;
      \left[
      \log P_{\text{ASR}}\!\bigl(y_t \mid x,\, y_{&lt;t}\bigr)
      \;+\;
      \lambda\,\log P_{\text{LM}}\!\bigl(y_t \mid y_{&lt;t}\bigr)
      \right]
      \]</span></p>
      <p>where:</p>
      <ul>
      <li><span class="math inline">\(t\)</span> is the decoding step
      (0-based).<br />
      </li>
      <li><span class="math inline">\(y_t\)</span> is the chosen token
      at step <span class="math inline">\(t\)</span> and <span
      class="math inline">\(y_{&lt;t}\)</span> are previously generated
      tokens.<br />
      </li>
      <li><span class="math inline">\(x\)</span> represents the acoustic
      features (e.g., raw audio input).<br />
      </li>
      <li><span class="math inline">\(P_{\text{ASR}}\)</span> depends on
      both <span class="math inline">\(x\)</span> and <span
      class="math inline">\(y_{&lt;t}\)</span>, while <span
      class="math inline">\(P_{\text{LM}}\)</span> depends on <span
      class="math inline">\(y_{&lt;t}\)</span> only.<br />
      </li>
      <li><span class="math inline">\(\lambda\)</span> is the weighting
      factor to determine the language model‚Äôs influence.</li>
      </ul>
      <p>The idea is that the ASR model understands phonetics and
      language in a general sense while the LM model understands the
      specialized domain in its written form, but has no access to the
      audio signal. Just like in the analogy from earlier by fusing
      their predictions, we combine phonetic understanding with domain
      expertise, aiming to improve the quality of transcriptions for
      domain-specific terms. Without careful integration or synergy
      between the two, both models can carry major limitations.</p>
      <h4 id="process-diagram">Process Diagram:</h4>
      <p><img src="assets/viz.png" alt="diagram" /> Reference: Kannan et
      al.¬†(2017)</p>
      <h3 id="practical-example">Practical Example</h3>
      <p>Consider an example where Whisper serves as our listening
      expert and GPT-2 as our domain-language expert. In practice these
      models share a tokenizer making the process of integrating their
      predictions fairly seamless at least for the English version of
      Whisper (Radford et al., 2022). Now let‚Äôs consider a claims call
      center transcript where an ASR model misinterprets a specialized
      medical term.</p>
      <p><strong>Input Audio (Ground Truth):</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs Tetralogy of Fallot.‚Äù‚úîÔ∏è</p>
      <p><strong>Whisper Initial Output:</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs Tetralogy of below.‚Äùüö´</p>
      <h4 id="step-by-step-fusion-process">Step-by-Step Fusion
      Process</h4>
      <p><strong>1. Whisper Initial Decoding:</strong></p>
      <p>Whisper produces logits at each step:</p>
      <ul>
      <li>Token: ‚ÄúThe‚Äù -&gt; high confidence<br />
      </li>
      <li>Token: ‚Äúprocedure‚Äù -&gt; high confidence<br />
      </li>
      <li>Token: ‚Äúclaimant‚Äù -&gt; high confidence<br />
      </li>
      <li>Token: ‚Äú‚Äôs‚Äù -&gt; high confidence<br />
      </li>
      <li>At the final subword, Whisper may exhibit uncertainty,
      spreading probabilities across candidates: ‚Äúbelow‚Äù, ‚Äúfollow‚Äù,
      ‚ÄúFallot‚Äù</li>
      </ul>
      <p><strong>2. Domain GPT-2 Predictions:</strong><br />
      At the ambiguous decoding step in ‚ÄúThe procedure was medically
      necessary for the treatment of claimant‚Äôs Tetralogy of ____‚Äù, each
      model produces different log probabilities:</p>
      <table>
      <thead>
      <tr>
      <th>Next Token</th>
      <th>Whisper Log Probs</th>
      <th>GPT-2 Log Probs</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>Fallot</strong></td>
      <td><strong>‚Äì1.8</strong></td>
      <td><strong>‚Äì0.3</strong></td>
      </tr>
      <tr>
      <td>below</td>
      <td>‚Äì1.0</td>
      <td>‚Äì5.0</td>
      </tr>
      <tr>
      <td>follow</td>
      <td>‚Äì3.5</td>
      <td>‚Äì3.8</td>
      </tr>
      </tbody>
      </table>
      <p><br></p>
      <blockquote>
      <p><em>Note: GPT-2, which has been fine tuned on medical
      literature, strongly favors the correct token (produces log
      probabilities closer to 0 for Fallot) while Whisper, which had
      minimal access to medical terminology, assigns it a much lower
      likelihood (log probabilities that are more negative).</em></p>
      </blockquote>
      <p><strong>3. Shallow Fusion (Combining Logits):</strong></p>
      <p><strong>Fusion Equation:</strong></p>
      <p>We combine each model‚Äôs logits using a weighted sum in the
      following way:</p>
      <p><span class="math display">\[
      \log P_{\text{combined}}(y_t) = \log P_{\text{Whisper}}(y_t \mid
      x, y_{&lt;t}) + \lambda \log P_{\text{GPT2}}(y_t \mid y_{&lt;t})
      \]</span></p>
      <p><strong>Example:</strong></p>
      <table>
      <colgroup>
      <col style="width: 24%" />
      <col style="width: 24%" />
      <col style="width: 22%" />
      <col style="width: 28%" />
      </colgroup>
      <thead>
      <tr>
      <th>Next Token</th>
      <th>Whisper Score</th>
      <th>GPT-2 Score</th>
      <th>Combined Score (Œª = 0.2)</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>Fallot</strong></td>
      <td><strong>‚Äì1.8</strong></td>
      <td><strong>‚Äì0.3</strong></td>
      <td><strong>‚Äì1.8 + 0.2 √ó (‚Äì0.3) = ‚Äì1.86</strong></td>
      </tr>
      <tr>
      <td>below</td>
      <td>‚Äì1.0</td>
      <td>‚Äì5.0</td>
      <td>‚Äì1.0 + 0.2 √ó (‚Äì5.0) = ‚Äì2.0</td>
      </tr>
      <tr>
      <td>follow</td>
      <td>‚Äì3.5</td>
      <td>‚Äì3.8</td>
      <td>‚Äì3.5 + 0.2 √ó (‚Äì3.8) = ‚Äì4.26</td>
      </tr>
      </tbody>
      </table>
      <blockquote>
      <p><em>Note: These values are illustrative. In practice, rare
      medical terms may be split across multiple tokens, but the
      principle remains the same, the domain model‚Äôs confidence helps
      disambiguate uncertain acoustic predictions.</em></p>
      </blockquote>
      <p><strong>Final Corrected Output:</strong><br />
      Since ‚ÄúFallot‚Äù now has the highest combined score the final output
      reads: ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs Tetralogy of Fallot.‚Äù‚úîÔ∏è</p>
      <p>This illustrates how <strong>domain-aware shallow
      fusion</strong> could potentially improve ASR output in
      specialized contexts.</p>
      <h2 id="experimental-setup">Experimental Setup</h2>
      <h3 id="model-selection-and-preparation">Model Selection and
      Preparation</h3>
      <p>For this implementation, I chose <strong>Whisper</strong> as
      the base ASR model due to its strong general-purpose performance
      and <strong>GPT-2</strong> as the domain-specific language model.
      The external models selected for this fusion process were GPT-2
      small, medium, and large. The reason for selecting these models
      was partly due to the shared tokenizer/vocabulary they have with
      Whisper‚Äôs decoder. The shared vocabulary means we do not have to
      learn a mapping from one model‚Äôs vocabulary to another. While
      Bio-GPT represents an existing medical language model, it uses a
      different tokenizer that would require learning a mapping function
      between tokenization schemes. To avoid potential errors and
      implementation complexity, I opted to train custom GPT-2 variants
      on medical data thus preserving Whisper‚Äôs tokenizer
      compatibility.</p>
      <h3 id="training-domain-specific-language-models">Training
      Domain-Specific Language Models</h3>
      <p>To adapt an external language model to the medical domain, the
      PubMed dataset was used. Multiple versions of GPT-2 were tuned on
      roughly <strong>3.63 billion tokens</strong> from PubMed
      abstracts. Three GPT-2 variants were trained to create the
      following domain-adapted language models:</p>
      <ul>
      <li><strong>GPT-2 Small</strong> (124M parameters)</li>
      <li><strong>GPT-2 Medium</strong> (355M parameters)</li>
      <li><strong>GPT-2 Large</strong> (774M parameters)</li>
      </ul>
      <p>The models were trained using standard autoregressive language
      modeling objectives on this large corpus of medical abstracts.
      Retaining Whisper‚Äôs tokenizer ensured seamless fusion, eliminating
      any need for token mapping or vocabulary alignment.</p>
      <p><strong>Training pipeline and tuned models:</strong></p>
      <ul>
      <li><a
      href="https://github.com/donkeyanaphora/SHALLOW_FUSION">View on
      GitHub</a><br />
      </li>
      <li><a href="https://huggingface.co/cwestnedge/models">Hugging
      Face Models</a></li>
      </ul>
      <h3 id="fusion-pipeline-architecture">Fusion Pipeline
      Architecture</h3>
      <p>The fusion pipeline operates selectively on relevant tokens
      only. Whisper contains special task-related tokens (language
      identifiers, task specifiers, timestamps) that are outside the
      scope of GPT-2‚Äôs vocabulary and training domain. However, for the
      English transcription task, Whisper should not emit these special
      tokens during normal operation, making this a non-issue in
      practice.</p>
      <p>The implementation performs fusion by:</p>
      <ol type="1">
      <li>Running Whisper‚Äôs encoder to generate audio features</li>
      <li>At each decoding step, computing logit distributions from both
      Whisper‚Äôs decoder and the domain-adapted GPT-2</li>
      <li>Combining logits for shared tokens using the weighted sum
      formulation described earlier</li>
      <li>Selecting tokens based on the fused probability
      distribution</li>
      </ol>
      <h3 id="evaluation-framework">Evaluation Framework</h3>
      <p><strong>Dataset Generation:</strong> Testing was conducted on
      358 synthetic radiology dictations (each under 30 seconds)
      generated by two large language models. Texts were produced by
      GPT-5 (Thinking) and Claude 4.1 using the following prompt:</p>
      <table>
      <colgroup>
      <col style="width: 100%" />
      </colgroup>
      <thead>
      <tr>
      <th><strong>Prompt</strong></th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>Please generate 150 unique radiology report dictations.
      Dictations must<br>contain realistic medical terminology with
      correct usage and spelling.<br>Additionally, these sentences must
      be text to speech friendly.<br><br>Data should be provided in a
      markdown cell and formatted as
      such:<br>{<br>¬†¬†‚Äúradiology_dictations‚Äù: [<br>¬†¬†¬†¬†{‚Äútext‚Äù:
      ‚Äú‚Ä¶‚Äù},<br>¬†¬†¬†¬†‚Ä¶,<br>¬†¬†¬†¬†{‚Äútext‚Äù: ‚Äú‚Ä¶‚Äù}<br>¬†¬†]<br>}</td>
      </tr>
      </tbody>
      </table>
      <p><br> Both models exceeded the target, yielding 358 unique
      sequences. Each sequence was synthesized to mono WAV format using
      OpenAI‚Äôs text-to-speech system (TTS) at a constant sample rate,
      with no added noise or reverberation.</p>
      <p><strong>Dataset Limitations:</strong> Results are based on
      LLM-written text rendered with clean TTS audio, which
      under-represents real dictation variability (accents,
      disfluencies, noise); gains may not generalize to clinical speech
      without proper validation (see future directions section).</p>
      <p><strong>Evaluation Methodology:</strong> The primary evaluation
      metric was Word Error Rate (WER)<sup>3</sup>, which measures the
      percentage of incorrectly transcribed words. Testing compared
      transcriptions from:</p>
      <ul>
      <li>Whisper-only baseline</li>
      <li>Shallow fusion with vanilla GPT-2 models (small, medium)</li>
      <li>Shallow fusion with GPT-2 models fine-tuned on PubMed
      abstracts (small, medium)</li>
      <li>Various Œª weighting values to optimize fusion performance</li>
      </ul>
      <p>The comparison between vanilla and PubMed-tuned GPT-2 models
      tests whether domain-specific language modeling provides
      additional benefits for medical transcription accuracy.</p>
      <h2 id="results-analysis">Results &amp; Analysis</h2>
      <!-- ### Overall Performance

      In preliminary synthetic evaluations, shallow fusion demonstrated modest WER reductions across different model sizes on the synthetic radiology dictations. 

      - **Whisper Small + GPT-2 PubMed Small:** WER decreased from 6.86% to 6.28% at Œª = 0.24 (an 8.5% relative reduction)
      - **Whisper Medium + GPT-2 PubMed Medium:** WER decreased from 5.16% to 4.80% at Œª = 0.24 (a 7.0% relative reduction)

      These preliminary results align with prior work (e.g., Kannan et al., 2017, 9.1% relative reduction on Google Voice Search with shallow fusion). While these improvements are modest, analysis suggests that fusion particularly excels at correcting medical terminology errors, successfully recovering terms like "scapholunate" from "scaffolunate" and "cholecystitis" from "colosceitis" though the fused model occasionally introduces hyphenation artifacts in compound medical terms which hurts WER. -->
      <h3 id="overall-performance">Overall Performance</h3>
      <p>Shallow fusion‚Äôs effectiveness depends critically on domain
      expertise. Testing with both medical and generic language models
      reveals a striking divergence:</p>
      <div style="background: white; padding: 20px; border-radius: 8px; margin: 20px 0;">
        <canvas id="fusionChart" style="max-width: 100%; height: 400px;"></canvas>
      </div>

      <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
      <script>
      // Shallow fusion performance data
      const data = {
        labels: [0.03, 0.06, 0.09, 0.12, 0.15, 0.18, 0.21, 0.24, 0.27, 0.3, 0.33, 0.36],
        datasets: [
          {
            label: 'Baseline (Whisper Only)',
            data: [6.86, 6.86, 6.86, 6.86, 6.86, 6.86, 6.86, 6.86, 6.86, 6.86, 6.86, 6.86],
            borderColor: '#9CA3AF',
            borderDash: [5, 5],
            fill: false,
            pointRadius: 0,
            borderWidth: 2
          },
          {
            label: '+ Medical GPT-2',
            data: [6.66, 6.67, 6.6, 6.5, 6.56, 6.43, 6.43, 6.28, 6.48, 6.67, 6.98, 7.0],
            borderColor: '#10B981',
            backgroundColor: '#10B98120',
            fill: '+1',
            pointRadius: 3,
            borderWidth: 2.5
          },
          {
            label: '+ Generic GPT-2',
            data: [6.84, 6.96, 7.09, 7.21, 7.41, 7.38, 7.43, 7.45, 7.64, 7.98, 8.23, 8.27],
            borderColor: '#EF4444',
            backgroundColor: '#EF444420',
            fill: '-1',
            pointRadius: 3,
            borderWidth: 2.5
          }
        ]
      };

      // Create interactive chart
      new Chart(document.getElementById('fusionChart'), {
        type: 'line',
        data: data,
        options: {
          responsive: true,
          maintainAspectRatio: false,
          interaction: { mode: 'index', intersect: false },
          plugins: {
            title: {
              display: true,
              text: 'Domain Expertise is Critical for Shallow Fusion',
              font: { size: 16, weight: 'bold' }
            },
            legend: {
              position: 'top',
            },
            tooltip: {
              callbacks: {
                title: (ctx) => 'Œª = ' + ctx[0].label,
                label: (ctx) => {
                  const val = ctx.parsed.y.toFixed(2);
                  const baseline = 6.86;
                  const change = ((val - baseline) / baseline * 100).toFixed(1);
                  if (ctx.dataset.label.includes('Baseline')) {
                    return ctx.dataset.label + ': ' + val + '%';
                  }
                  return ctx.dataset.label + ': ' + val + '% (' + 
                         (change > 0 ? '+' : '') + change + '% change)';
                }
              }
            }
          },
          scales: {
            x: { 
              title: { display: true, text: 'Fusion Weight (Œª)' }
            },
            y: { 
              title: { display: true, text: 'Word Error Rate (%)' },
              min: 5.8,
              max: 8.8
            }
          }
        }
      });
      </script>
      <blockquote>
      <p><em>Figure 2: Word error rates across fusion weights (Œª) in
      preliminary testing. Medical GPT-2 (green) reduced WER below
      baseline while generic GPT-2 (red) increased it in this synthetic
      evaluation. Interactive hover for details</em></p>
      </blockquote>
      <p>The medical models achieved optimal performance at Œª =
      0.24-0.30, with the small configuration reducing WER from 6.86% to
      6.28% (8.5% relative improvement) and the medium configuration
      from 5.16% to 4.80% (7.0% relative improvement). In contrast,
      generic GPT-2 models consistently increased error rates,
      demonstrating that domain alignment, not just language modeling,
      drives improvement. These results align with prior work, Kannan et
      al.¬†(2017), who reported a 9.1% relative WER reduction on Google
      Voice Search using shallow fusion. Overall, The medical models
      excel at correcting specialized terminology like ‚Äúscapholunate‚Äù
      from ‚Äúscaffolunate,‚Äù while generic models introduce errors by
      biasing transcriptions toward everyday language.</p>
      <h3 id="hyperparameter-sensitivity-Œª-lambda-weight">Hyperparameter
      Sensitivity (Œª / Lambda Weight)</h3>
      <p>The fusion weight Œª controls the balance between acoustic and
      language model predictions. Testing Œª values from 0.03 to 0.36
      revealed consistent patterns across model sizes:</p>
      <p><strong>Table 1: WER Performance Across Fusion
      Weights</strong></p>
      <table>
      <colgroup>
      <col style="width: 20%" />
      <col style="width: 20%" />
      <col style="width: 15%" />
      <col style="width: 14%" />
      <col style="width: 30%" />
      </colgroup>
      <thead>
      <tr>
      <th>Configuration</th>
      <th>Baseline WER</th>
      <th>Optimal Œª</th>
      <th>Best WER</th>
      <th>Relative Improvement</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>Whisper Small + Medical GPT-2 Small</td>
      <td>6.86%</td>
      <td>0.24</td>
      <td>6.28%</td>
      <td>8.5%</td>
      </tr>
      <tr>
      <td>Whisper Medium + Medical GPT-2 Medium</td>
      <td>5.16%</td>
      <td>0.30</td>
      <td>4.80%</td>
      <td>7.0%</td>
      </tr>
      </tbody>
      </table>
      <!-- > Note: Values are corpus (micro) WER on 358 synthetic radiology dictation sentences after the specified normalization. **Percentages are computed from the displayed WERs (rounded to 4 decimals) for consistency, full-precision calculations may differ slightly**. Œª was selected on this same set, so results may be optimistic; see the significance section for permutation-test p-values -->
      <!-- 
      In synthetic testing, the small model configuration showed optimal results at Œª = 0.24, yielding a **8.5%** relative WER reduction. The medium configuration achieved a **6.9%** relative reduction at the same Œª = 0.30. Error analysis suggests that the benefits of fusion lie in correcting domain-specific medical terminology, however, analysis would benefit from more data (see future work section).
       -->
      <p><br> Performance degrades at extreme Œª values, when too low
      (&lt; 0.1) fusion provides minimal domain benefit, while too high
      (&gt; 0.33) causes the language model to override valid acoustic
      evidence. The optimal range (0.24-0.30) suggests a consistent
      balance point where domain knowledge enhances without overwhelming
      acoustic information.</p>
      <h3 id="statistical-significance">Statistical Significance</h3>
      <p>The fused system and original Whisper only system were tested
      on 358 of the same audio clips.</p>
      <p><strong>For the small model</strong>, overall errors fell from
      6.86% to 6.28% (8.5% relative reduction). A permutation
      test<sup>4</sup> suggests a difference this size would happen by
      chance about 1 in 81 times if testing only for improvement
      (one-sided p = 0.012). With 44 improved utterances versus 21
      degraded ones, this pattern is consistent with a modest real
      effect on this set.</p>
      <p><strong>For the medium model</strong>, overall errors went from
      5.16% to 4.80% (7.0% relative reduction). The permutation test
      suggests a difference this size could occur about 1 in 30 times if
      testing only for improvement (one-sided p = 0.034). With 30
      improvements versus 14 degradations, this is promising but test
      would benefit from more data.</p>
      <p><strong>Bottom line:</strong> Shallow fusion results show
      modest, yet consistent error reductions on this dataset where
      fusion improvements are statistically significant. However, more
      data, ideally real clinical dictations, would make the conclusion
      more definitive.</p>
      <h3 id="error-pattern-analysis-and-failure-modes">Error Pattern
      Analysis and Failure Modes</h3>
      <p>While the synthetic evaluation showed promising patterns,
      analysis revealed specific failure modes that illuminate the
      method‚Äôs limitations:</p>
      <p><strong>1. Abbreviation Expansion Mismatches</strong><br />
      The fusion system frequently ‚Äúover-corrected‚Äù spoken abbreviations
      into their formal written equivalents. For example:</p>
      <ul>
      <li>Audio: ‚Äúcentimeters‚Äù -&gt; Whisper: ‚Äúcentimeters‚Äù -&gt; Fused
      output: ‚Äúcm‚Äù</li>
      <li>This reflects the domain language model‚Äôs bias toward written
      medical documentation style</li>
      </ul>
      <p><strong>2. Punctuation Insertion</strong><br />
      The GPT-2 model, trained on formatted medical abstracts,
      introduced punctuation that wasn‚Äôt present in the spoken audio.
      This created a stylistic mismatch between transcribed speech and
      formal written medical language, with inappropriate hyphenation of
      compound terms being particularly prominent (e.g.,
      ‚Äúhydronephrosis‚Äù -&gt; ‚Äúhydro-nephrosis‚Äù).</p>
      <p><strong>3. Premature Termination and Incomplete
      Transcripts</strong><br />
      When Œª (the LM weight) was set too high, beam search decoding
      often produced incomplete transcripts. Chorowski &amp; Jaitly
      (2016) reported that external LMs can cause seq2seq systems to
      skip words or drop parts of an utterance during decoding, unless a
      coverage term is added to the beam search criterion. In this
      experiment, higher Œª coupled with wide beam searches similarly led
      to premature terminations, with the LM assigning high probability
      to end-of-sequence tokens once a transcript appeared semantically
      complete, even while audio continued.</p>
      <h3 id="domain-specific-improvements">Domain-Specific
      Improvements</h3>
      <p>The fusion approach‚Äôs benefits were concentrated almost
      exclusively in medical terminology recognition. Examples of
      successful corrections included:</p>
      <ul>
      <li>Complex pharmaceutical names</li>
      <li>Anatomical terminology</li>
      <li>Rare disease names and medical conditions</li>
      <li>Procedural and diagnostic terminology</li>
      </ul>
      <p>Standard conversational language showed minimal improvement,
      suggesting that benefits in this synthetic evaluation may derive
      from domain expertise rather than general language modeling
      enhancement.</p>
      <h2 id="reflection-and-future-directions">Reflection and Future
      Directions</h2>
      <h3 id="addressing-current-limitations">Addressing Current
      Limitations</h3>
      <p>The experimental results highlight several areas for
      improvement that point toward promising future research
      directions:</p>
      <h4 id="real-world-dataset-validation">Real-World Dataset
      Validation</h4>
      <p>The synthetic evaluation dataset, while useful for
      proof-of-concept demonstration, limits the generalizability of
      these findings. Future work should incorporate authentic clinical
      dictations such as the <a
      href="https://marketplace.databricks.com/details/8eb39dd5-ffc4-4e8d-8f89-25d91bf1774b/Shaip_Physician-Dictation-Data-Radiology">Shaip
      Physician Dictation Dataset</a>, which requires Databricks account
      permissions. Real clinical speech presents challenges absent in
      synthetic data: background noise, speaker variations,
      interruptions, and the full complexity of clinical communication
      patterns.</p>
      <h4 id="coverage-penalty">Coverage Penalty</h4>
      <p>The current implementation does not make use of a coverage
      penalty to address premature sequence terminations. This penalty
      term measures how much of the audio the model actually attended to
      and candidates that skip large stretches are penalized, while
      transcripts that cover the whole utterance are preferred. This
      strategy follows Chorowski &amp; Jaitly (2016) and Kannan et
      al.¬†(2017) and can be incorporated during generation.</p>
      <h4 id="learned-gating-mechanisms">Learned Gating Mechanisms</h4>
      <p>The static Œª weighting approach represents a significant
      limitation. A more sophisticated system would dynamically adjust
      the influence of the external language model based on acoustic
      confidence and contextual cues. When Whisper exhibits high
      confidence in its predictions, the domain model should have
      minimal influence. Conversely, during periods of acoustic
      uncertainty, particularly around medical terminology, the fusion
      weight should increase. Implementing this would likely involve
      training a small gating network that learns to predict optimal Œª
      values given acoustic features and partial transcript context.</p>
      <h4 id="advanced-fusion-architectures">Advanced Fusion
      Architectures</h4>
      <p>Beyond shallow fusion, <strong>deep fusion</strong> and
      <strong>cold fusion</strong> approaches warrant investigation.
      Deep fusion could learn more sophisticated integration by
      combining hidden states and tuning a task-specific fusion
      function. Cold fusion could be explored by integrating the domain
      language model during Whisper‚Äôs training process, though this
      would require more substantial computational resources and
      training data.</p>
      <h3 id="broader-implications">Broader Implications</h3>
      <p>This work connects to several important trends in contemporary
      AI development:</p>
      <p><strong>Ensemble and Mixture-of-Experts Architectures:</strong>
      Shallow fusion represents a simple form of ensemble modeling,
      where specialized models contribute their expertise to improve
      overall performance. This aligns with the broader trend toward
      Mixture-of-Experts architectures that dynamically route inputs to
      specialized sub-networks.</p>
      <p><strong>Multimodal Integration Challenges:</strong> The fusion
      of acoustic and textual information highlights fundamental
      challenges in multimodal AI systems. Different modalities often
      have distinct statistical properties and optimal representations,
      requiring careful integration strategies.</p>
      <p><strong>Domain Adaptation Strategies:</strong> As AI systems
      deploy across increasingly specialized domains, the tension
      between general capability and domain expertise becomes more
      pronounced. Shallow fusion offers one approach to leveraging
      domain-specific knowledge without extensive retraining of large
      general-purpose models.</p>
      <h2 id="conclusion">Conclusion</h2>
      <p>This exploration of shallow fusion for medical ASR demonstrates
      both the promise and limitations of combining general-purpose
      models with domain-specific expertise. The key insight is that
      each model type hits distinct ‚Äúdata walls‚Äù:</p>
      <p><strong>Whisper (Generalist Model)</strong> excels at
      acoustic-to-text mapping and handles diverse speakers, accents,
      and recording conditions effectively. However, its broad training
      distribution means medical terminology remains under-represented,
      leading to systematic errors on specialized vocabulary despite
      strong general performance.</p>
      <p><strong>GPT-2 (Domain Specialist),</strong> trained on PubMed
      abstracts, develops rich representations of medical terminology
      and context through self-supervised learning on abundant textual
      data. However, it remains completely blind to acoustic signals and
      exhibits biases toward formal written language rather than
      conversational speech patterns.</p>
      <p>The preliminary synthetic evaluation, showing up to 8.5% WER
      reduction, suggests shallow fusion may have potential for
      improvement, though further validation on clinical data is needed.
      Additionally, the failure modes (abbreviation mismatches,
      punctuation insertion, and premature terminations) reveal the
      challenges of bridging modalities with different statistical
      properties and stylistic conventions.</p>
      <p>The observed improvements appeared concentrated on medical
      terminology recognition, suggesting that the benefits may derive
      from genuine domain expertise rather than general language
      modeling improvements. This specificity, while limiting the
      approach‚Äôs broad applicability, makes it particularly valuable for
      specialized transcription applications where domain terminology
      accuracy is critical.</p>
      <p>Future work towards learned gating mechanisms, advanced fusion
      architectures, and validation on authentic clinical datasets will
      help address current limitations. More broadly, this work
      illustrates the ongoing evolution of AI system architectures from
      monolithic models toward composite systems that combine
      specialized expertise, a trend likely to accelerate as AI
      deployment expands across diverse professional domains.</p>
      <h2 id="resources">Resources</h2>
      <ul>
      <li><a href="https://arxiv.org/pdf/1503.03535">On Using
      Monolingual Corpora in Neural Machine Translation</a> ‚Äî Gulcehre
      et al., 2015<br />
      </li>
      <li><a href="https://arxiv.org/pdf/1708.06426">Cold Fusion:
      Training Seq2Seq Models Together with Language Models</a> ‚Äî Sriram
      et al., 2017<br />
      </li>
      <li><a href="https://arxiv.org/pdf/1612.02695">Towards Better
      Decoding and Language Model Integration in Sequence-to-Sequence
      Models</a> ‚Äî Chorowski &amp; Jaitly, 2016<br />
      </li>
      <li><a href="https://arxiv.org/pdf/1712.01996">Analysis of
      Incorporating an External Language Model‚Ä¶</a> ‚Äî Kannan et al.,
      2017<br />
      </li>
      <li><a href="https://arxiv.org/pdf/2212.04356">Robust Speech
      Recognition via Large-Scale Weak Supervision</a> ‚Äî Radford et al.,
      2022<br />
      </li>
      <li><a
      href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language
      Models are Unsupervised Multitask Learners</a> ‚Äî OpenAI,
      2019<br />
      </li>
      <li><a href="https://arxiv.org/pdf/2005.14165">Language Models are
      Few-Shot Learners</a> ‚Äî Brown et al., 2020</li>
      </ul>
      <hr />
      <ol type="1">
      <li><p><a
      href="https://en.wikipedia.org/wiki/Catastrophic_interference">Catastrophic
      forgetting</a> occurs when a neural network loses previously
      learned information upon learning new tasks or data.</p></li>
      <li><p>Several variations exist to reduce the inference cost of
      shallow fusion, including N-best rescoring (applying the LM only
      to candidate transcripts) and using smaller or distilled domain
      LMs.</p></li>
      <li><p><a
      href="https://en.wikipedia.org/wiki/Word_error_rate">Word Error
      Rate (WER)</a> is the standard metric for evaluating ASR systems,
      calculated as the minimum number of word-level edits (insertions,
      deletions, substitutions) required to transform the hypothesis
      into the reference, divided by the total number of words in the
      reference.</p></li>
      <li><p>A shuffle test (permutation test) is a nonparametric
      significance test where labels (baseline vs.¬†fused) are randomly
      swapped to see how often a difference as large as the observed one
      would arise by chance.</p></li>
      </ol>
    </article>
  </main>

  <!-- article JS (dark mode only) -->
  <script defer src="../../assets/js/article.js"></script>
</body>
</html>