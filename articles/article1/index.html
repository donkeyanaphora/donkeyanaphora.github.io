<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Shallow Fusion: Bridging Data Scarcity and AI Integration
Challenges - Collins Westnedge</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no, viewport-fit=cover">
  
  <!-- Mobile app settings -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- SEO essentials -->
  <meta name="google-site-verification" content="hmW81pKLkPLJXY0kFe-I1vNXno-xD9XbEWyZyGfz-SA" />
  <meta name="description" content="A minor rant about ‚ÄòAI adoption‚Äô and
exploration of shallow fusion as a method to address data scarcity and
integration in specialized domains">
  <meta name="author" content="Collins Westnedge">
  <link rel="canonical" href="https://donkeyanaphora.github.io/articles/article1/">
  
  <!-- Hide drafts from search engines -->
  <meta name="robots" content="noindex, nofollow">

  <!-- Site look & feel -->
  <link rel="stylesheet" href="../../assets/css/main.css"/>
  <link rel="stylesheet" href="../../assets/css/article.css"/>
  
  <!-- Favicons -->
  <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
  <link rel="manifest" href="../../site.webmanifest">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://donkeyanaphora.github.io/articles/article1/">
  <meta property="og:title" content="Shallow Fusion: Bridging Data
Scarcity and AI Integration Challenges">
  <meta property="og:description" content="A minor rant about ‚ÄòAI
adoption‚Äô and exploration of shallow fusion as a method to address data
scarcity and integration in specialized domains">
  <meta property="og:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">
  <meta property="og:site_name" content="The Latent Realm">
  <meta property="article:author" content="Collins Westnedge">
  <meta property="article:published_time" content="2025-04-16">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:url" content="https://donkeyanaphora.github.io/articles/article1/">
  <meta name="twitter:title" content="Shallow Fusion: Bridging Data
Scarcity and AI Integration Challenges">
  <meta name="twitter:description" content="A minor rant about ‚ÄòAI
adoption‚Äô and exploration of shallow fusion as a method to address data
scarcity and integration in specialized domains">
  <meta name="twitter:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">

  <!-- MathJax config -->
  <script>
    window.MathJax = {
      tex:   { displayIndent: '0em',  displayAlign: 'left' },
      chtml: { mtextFont: 'Times' }
    };
  </script>
  
  <!-- MathJax -->
  <script defer id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>
</head>

<body class="article">
  <!-- floating buttons -->
  <button id="menuBtn" class="float-btn" aria-label="Open menu">üß∞</button>
  <button class="float-btn back-btn" onclick="window.location.href='../../'" aria-label="Back to home">üè°</button>
  <button id="toggleDark" class="float-btn" aria-label="Toggle dark mode">üåô</button>

  <!-- Markdown content -->
  <main class="content">
    <article class="markdown-body">
      <h1
      id="shallow-fusion-bridging-data-scarcity-and-ai-integration-challenges">üîó
      Shallow Fusion: Bridging Data Scarcity and AI Integration
      Challenges</h1>
      <p><strong>COLLINS WESTNEDGE</strong><br />
      <em>APRIL 16, 2025</em></p>
      <h2 id="introduction">Introduction</h2>
      <p>AI adoption and integration have become focal points in
      seemingly every earnings call, linkedin post, townhall and
      industry keynote. However, most of these conversations exist to
      highlight revenue potential, promote products and services, or
      bolster positive consumer sentiment, which is likely why they tend
      to gloss over or abstract away the technical challenges that stand
      in the way of effective adoption. One of the fundamental
      challenges is the gap between available data and the data needed
      for a domain-specific task.</p>
      <p>Consider for example applying a large generalist model to a
      highly specialized task that barely surfaces in its pretraining
      data if at all. For the generalist model to succeed, it must first
      grasp dense company prospectuses, specialized jargon, and the
      nuances of the business problem itself. To address this gap
      companies often resort to standard recipes e.g.¬†‚Äúexciting‚Äù the
      right activations through few-shot examples, dumping streams of
      internal documents into the model‚Äôs context, or ambitious attempts
      at fine-tuning on small internal datasets. However, with these
      approaches there‚Äôs often no optimization signal or gradient to
      move against and progress if there‚Äôs any to be had involves a good
      deal of guesswork, trial, and error.</p>
      <p><strong>Automatic Speech Recongition (ASR)</strong> exemplifies
      this challenge. Many domains, such as medicine, law, financial
      services, etc contain specialized terminology that is typically
      outside the distribution or under-represented in the pretraining
      for general purpose models. A model trained on everyday speech
      will struggle with phrases like ‚Äúothostatic tachycardia‚Äù or domain
      specific acronyms. Traditional solutions to this issue involve
      collecting domain-specific audio and ground truth transcriptions
      (often hand labeled) which can be cost prohibitive. Open source
      datasets on specialized domains are becoming more common but their
      volume and variety remain too limited, keeping them
      out-of-distribution for many business use cases.</p>
      <p>This distribution gap motivated me to explore the concept of
      <strong>shallow fusion</strong>: combining general-purpose ASR
      model with domain-specific language models during inference.
      Rather than requiring extensive retraining, shallow fusion
      leverages existing domain expertise during inference from an
      external language model. The questions I would like to explore in
      this article are: Can a language model trained on domain-specific
      text meaningfully improve speech-to-text transcription accuracy
      within an adjacent domain? And critically, what are the failure
      modes associated with this type of integration?</p>
      <!-- In contrast to the popular narrative, my day-to-day experience has shown me that these discussions routinely overlook the many technical challenges mentioned above. As someone stuck in the generalist specialist divide, it's hard not to feel frustration when leaders boast an AI integration victory that is nothing more than an API call. This disconnect motivated me to develop a case study leveraging **shallow fusion** in order to mitigate data-distribution shifts and the shortage of structured training data in Automatic Speech Recognition (ASR). In this write-up, I introduce shallow fusion and ask: Can a language model trained on a domain-specific corpus meaningfully improve speech-to-text transcription within that domain and to what extent can the text be detached? -->
      <h2 id="a-formal-example-shallow-fusion">A Formal Example: Shallow
      Fusion</h2>
      <p>But first, what is shallow fusion? Consider for example, a
      person listening to audio of a phone call with a customer and
      customer service agent at an insurance claims calls center. The
      sole function of this person is to transcribe what they hear into
      text. The caveat, however, is that they only know very little
      about the domain and the types of technical issues and medical
      terminology e.g.¬†(procedures diagnoses etc) that representatives
      and customers are mentioning. Now consider a second person who has
      worked in this industry for many years and has a deep
      understanding of the domain, but is hard of hearing.</p>
      <p>Shallow fusion can be thought of as a process of integrating
      each person‚Äôs expertise to offset the errors of one another and
      bridge modalities the other does not have access to. With this
      analogy we can now formally describe this process. In the example
      below think of <span class="math inline">\(P_{\text{ASR}}\)</span>
      as the person listening to the audio and <span
      class="math inline">\(P_{\text{LM}}\)</span> as the domain expert
      that is hard of hearing but deeply understands the context.</p>
      <h4 id="mathematical-formulation">Mathematical Formulation</h4>
      <p>At each decoding step for some audio input, we select the most
      probable token <span class="math inline">\(y_{t}\)</span> using
      information from the Automatic Speech Recognition model (ASR) and
      the Language Model (LM)</p>
      <p><span class="math display">\[
      y^* = \arg\max_{y_t}\;
      \left[
      \log P_{\text{ASR}}\!\bigl(y_t \mid x,\, y_{\lt t}\bigr)
      \;+\;
      \lambda\,\log P_{\text{LM}}\!\bigl(y_t \mid y_{\lt t}\bigr)
      \right]
      \]</span></p>
      <p>where:<br />
      - <span class="math inline">\(t\)</span> is the decoding step
      (0-based).<br />
      - <span class="math inline">\(y_t\)</span> is the chosen token at
      step <span class="math inline">\(t\)</span> and <span
      class="math inline">\(y_{\lt  t}\)</span> are previously generated
      tokens.<br />
      - <span class="math inline">\(x\)</span> represents the acoustic
      features (e.g.¬†raw audio input).<br />
      - <span class="math inline">\(P_{\text{ASR}}\)</span> depends on
      both <span class="math inline">\(x\)</span> and <span
      class="math inline">\(y_{\lt  t}\)</span>, while <span
      class="math inline">\(P_{\text{LM}}\)</span> depends on <span
      class="math inline">\(y_{\lt  t}\)</span> only.<br />
      - <span class="math inline">\(\lambda\)</span> is the weighting
      factor to determine the language model‚Äôs influence.</p>
      <p>The idea is that the ASR model understands phonetics and
      language in a general sense while the LM model understands the
      specialized domain in its written form, but has no access to the
      audio signal. Just like in the analogy from earlier by fusing
      their predictions, we combine phonetic understand with domain
      expertise, leading to more accurate transcriptions for our domain.
      Without careful integration or synergy between the two, both
      models carry major limitations.</p>
      <h4 id="process-diagram">Process Diagram:</h4>
      <p><img src="assets/viz.png" alt="diagram" /> Reference: <a
      href="https://arxiv.org/pdf/1712.01996">Kannan et al.¬†2017</a></p>
      <h4 id="in-practice">In Practice</h4>
      <p>In the workflow that I dive into, token prediction is
      implemented in stages. Early on we rely solely on the ASR (the
      listening expert) model and then gradually introduce the language
      model (the domaine xpert) as more context becomes available. For
      example:</p>
      <p><strong>Token Selection at Step (t)</strong></p>
      <p><span class="math display">\[
      \mathbf{F}(x,t)=
      \begin{cases}
      \displaystyle
      \operatorname*{arg\,max}\limits_{y_t}\;
            \log P_{\text{ASR}}\!\left(y_t \mid x,\; y_{&lt;t}\right),
            &amp; t &lt; \text{n-steps},\\[0.75em]
      \displaystyle
      \operatorname*{arg\,max}\limits_{y_t}\;
            \Bigl[
              \log P_{\text{ASR}}\!\left(y_t \mid x,\; y_{&lt;t}\right)
              + \lambda\,\log P_{\text{LM}}\!\left(y_t \mid
      y_{&lt;t}\right)
            \Bigr],
            &amp; t \ge \text{n-steps}.
      \end{cases}
      \]</span></p>
      <p>This piecewise approach allows the system to build confidence
      from the raw audio transcription initially before incorporating
      the domain expert corrections, namely because our starting point
      should be conditionalized on something observed e.g.¬†we gotta
      start somewhere.</p>
      <h2 id="case-study-in-transcription-a-concrete-example">Case Study
      in Transcription: A Concrete Example</h2>
      <p>For our models let‚Äôs take Whisper to be our listening expert
      and GTP2 to be our domain-language expert. In practice these
      models share a tokenizer making the process of integrating their
      predictions fairly seamless at least for the english version of
      Whisper (<a href="https://arxiv.org/pdf/2212.04356">Radford
      2.2</a>). Now let‚Äôs consider a claims call center transcript where
      an ASR model misinterprets a specialized medical term.</p>
      <p><strong>Input Audio (Ground Truth):</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs <code>melanoma</code>.‚Äù‚úîÔ∏è</p>
      <p><strong>Whisper Initial Output:</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs <code>diploma</code>.‚Äùüö´</p>
      <h4 id="whisper-initial-decoding">1. <strong>Whisper Initial
      Decoding:</strong></h4>
      <p>Whisper produces logits at each step:</p>
      <ul>
      <li>Token: ‚ÄúThe‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äúprocedure‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äúclaimant‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äú‚Äôs‚Äù ‚Üí high confidence<br />
      </li>
      <li>At the final subword, Whisper may exhibit uncertainty,
      spreading probabilities across candidates: ‚Äúdiploma‚Äù, ‚Äúaroma‚Äù,
      ‚Äúmelanoma‚Äù</li>
      </ul>
      <h4 id="domain-gpt-2-predictions">2. <strong>Domain GPT-2
      Predictions:</strong></h4>
      <p>At this ambiguous decoding step, GPT-2 (the domain-adapted LM)
      produces logits based on the following context:</p>
      <ul>
      <li><p>‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs <code>_____</code>‚Äù</p></li>
      <li><p>GPT-2 which as been fine tuned on medical literature
      strongly favors the correct token (produces log probabilities
      closer to 0 for melanoma) while Whisper, which had minimal access
      to medical terminology, assigns it a much lower likelihood (log
      probabilities that are more negative).</p></li>
      </ul>
      <table>
      <thead>
      <tr>
      <th>Next Token</th>
      <th>Whisper Log Probs</th>
      <th>GPT-2 Log Probs</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>melanoma</strong></td>
      <td><strong>‚Äì1.8</strong></td>
      <td><strong>‚Äì0.3</strong></td>
      </tr>
      <tr>
      <td>diploma</td>
      <td>‚Äì1.0</td>
      <td>‚Äì5.0</td>
      </tr>
      <tr>
      <td>aroma</td>
      <td>‚Äì3.5</td>
      <td>‚Äì3.8</td>
      </tr>
      </tbody>
      </table>
      <h4 id="shallow-fusion-combining-logits">3. <strong>Shallow Fusion
      (Combining Logits):</strong></h4>
      <p>We combine each model‚Äôs logits using a weighted sum in the
      following way:</p>
      <p><span class="math display">\[
      \log P_{\text{combined}}(y_t)=
      \log P_{\text{Whisper}}\!\bigl(y_t \mid x,\, y_{\lt t}\bigr)
      +\lambda\,\log P_{\text{GPT2}}\!\bigl(y_t \mid y_{\lt t}\bigr)
      \]</span></p>
      <table>
      <colgroup>
      <col style="width: 24%" />
      <col style="width: 24%" />
      <col style="width: 22%" />
      <col style="width: 28%" />
      </colgroup>
      <thead>
      <tr>
      <th>Next Token</th>
      <th>Whisper Score</th>
      <th>GPT-2 Score</th>
      <th>Combined Score (Œª = 0.2)</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>melanoma</strong></td>
      <td><strong>‚Äì1.8</strong></td>
      <td><strong>‚Äì0.3</strong></td>
      <td><strong>‚Äì1.8 + 0.2 <span class="math inline">\(\times\)</span>
      (‚Äì0.3) = ‚Äì1.86</strong></td>
      </tr>
      <tr>
      <td>diploma</td>
      <td>‚Äì1.0</td>
      <td>‚Äì5.0</td>
      <td>‚Äì1.0 + 0.2 <span class="math inline">\(\times\)</span> (‚Äì5.0)
      = ‚Äì2.0</td>
      </tr>
      <tr>
      <td>aroma</td>
      <td>‚Äì3.5</td>
      <td>‚Äì3.8</td>
      <td>‚Äì3.5 + 0.2 <span class="math inline">\(\times\)</span> (‚Äì3.8)
      = ‚Äì4.26</td>
      </tr>
      </tbody>
      </table>
      <blockquote>
      <p><em>Note: The numbers are illustrative. In practice additional
      context and scaling would favor the correct token ‚Äúmelanoma‚Äù;
      additionally, rare words are likely split into multiple tokens but
      the intuition remains the same.</em></p>
      </blockquote>
      <p>‚Äúmelanoma‚Äù now has the highest combined score.</p>
      <h4 id="final-corrected-output"><strong>Final Corrected
      Output:</strong></h4>
      <p>‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs <code>melanoma</code>.‚Äù‚úîÔ∏è</p>
      <h4 id="key-takeaway">Key Takeaway:</h4>
      <ul>
      <li><strong>Without GPT-2:</strong> The model misrecognized
      domain-specific terms.<br />
      </li>
      <li><strong>With shallow fusion:</strong> The external domain LM
      (GPT-2) provided strong guidance toward the correct
      domain-specific vocabulary, correcting Whisper‚Äôs initial
      mistakes.</li>
      </ul>
      <p>This demonstrates how <strong>domain-aware shallow
      fusion</strong> can significantly improve ASR output in
      specialized contexts.</p>
      <h2 id="overcoming-the-data-wall">Overcoming the Data Wall</h2>
      <p>In the example above we demonstrated how the integration looks
      in practice, but we also assumed the language model (GPT-2) in our
      case has already been adapted to the appropriate domain. In
      practice we can fairly assume that out-of-the-box ASR models
      perform poorly on ‚Ä¶</p>
      <h2 id="reflection-and-future-directions">Reflection and Future
      Directions</h2>
      <ul>
      <li>learnable or dynamic <span
      class="math inline">\(\lambda\)</span><br />
      </li>
      <li>MoE<br />
      </li>
      <li>Cold Fusion<br />
      </li>
      <li>Deep Fusion</li>
      </ul>
      <h2 id="conclusion">Conclusion</h2>
      <ul>
      <li>articulate each model‚Äôs strength and weakness, emphasizing how
      each model hits its own data wall separately:
      <ul>
      <li>Whisper (generalist): broadly trained acoustic-to-text model,
      struggles with specialized terminology.<br />
      </li>
      <li>GPT-2 (specialist): trained in a self-supervised way solely on
      textual domain data, rich in domain-specific vocabulary but blind
      to acoustic signals.<br />
      </li>
      </ul></li>
      <li>dive into fusion related setbacks/metrics as well as possible
      future directions</li>
      <li>dive into fusion related improvements and qualitative
      benefits</li>
      <li>illustrate how this process extends or relates to broader
      trends within AI e.g.¬†Ensemble Architectures like Mixture of
      Experts, multimodal integration, domain adaptation and evolution
      of fusion techniques (cold &amp; deep).</li>
      </ul>
      <h2 id="resources">Resources</h2>
      <ul>
      <li><a
      href="https://research.facebook.com/file/551805355910423/Deep-Shallow-Fusion-for-RNN-T-Personalization.pdf">Deep
      Shallow Fusion for RNN-T Personalization</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/1712.01996">Analysis of
      Incorporating an External Language Model‚Ä¶</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/2212.04356">Robust Speech
      Recognition via Large-Scale Weak Supervision</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/1503.03535">On Using
      Monolingual Corpora in Neural Machine Translation</a><br />
      </li>
      <li><a
      href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language
      Models are Unsupervised Multitask Learners</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/2005.14165">Language Models are
      Few-Shot Learners</a></li>
      </ul>
    </article>
  </main>

  <!-- article JS (dark mode only) -->
  <script defer src="../../assets/js/article.js"></script>
</body>
</html>