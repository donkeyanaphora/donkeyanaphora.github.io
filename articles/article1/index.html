<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Shallow Fusion: Bridging Data Scarcity and AI Integration
Challenges - Collins Westnedge</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no, viewport-fit=cover">
  
  <!-- Mobile app settings -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- SEO essentials -->
  <meta name="google-site-verification" content="hmW81pKLkPLJXY0kFe-I1vNXno-xD9XbEWyZyGfz-SA" />
  <meta name="description" content="An exploration of shallow fusion as
a method to address data scarcity and integration in specialized
domains">
  <meta name="author" content="Collins Westnedge">
  <link rel="canonical" href="https://donkeyanaphora.github.io/articles/article1/">
  
  <!-- Hide drafts from search engines -->
  <meta name="robots" content="noindex, nofollow">

  <!-- Site look & feel -->
  <link rel="stylesheet" href="../../assets/css/main.css"/>
  <link rel="stylesheet" href="../../assets/css/article.css"/>
  
  <!-- Favicons -->
  <link rel="apple-touch-icon" sizes="180x180" href="../../favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../../favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../../favicon/favicon-16x16.png">
  <link rel="manifest" href="../../favicon/site.webmanifest">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://donkeyanaphora.github.io/articles/article1/">
  <meta property="og:title" content="Shallow Fusion: Bridging Data
Scarcity and AI Integration Challenges">
  <meta property="og:description" content="An exploration of shallow
fusion as a method to address data scarcity and integration in
specialized domains">
  <meta property="og:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">
  <meta property="og:site_name" content="The Latent Realm">
  <meta property="article:author" content="Collins Westnedge">
  <meta property="article:published_time" content="2025-08-26">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:url" content="https://donkeyanaphora.github.io/articles/article1/">
  <meta name="twitter:title" content="Shallow Fusion: Bridging Data
Scarcity and AI Integration Challenges">
  <meta name="twitter:description" content="An exploration of shallow
fusion as a method to address data scarcity and integration in
specialized domains">
  <meta name="twitter:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">

  <!-- MathJax config -->
  <!-- <script>
    window.MathJax = {
      tex:   { displayIndent: '0em',  displayAlign: 'center' },
      chtml: { mtextFont: 'Times' }
    };
  </script> -->
  
  <script>
    window.MathJax = {
      tex: {},  // Empty is fine
      chtml: { 
        mtextFont: 'Times',
        displayAlign: 'center',    // ‚úÖ Moved here
        displayIndent: '0em'       // ‚úÖ Moved here
      }
    };
  </script>
  <!-- MathJax -->
  <script defer id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js">
  </script>
</head>

<body class="article">
  <!-- floating buttons -->
  <button id="menuBtn" class="float-btn" aria-label="Open menu">üß∞</button>
  <button class="float-btn back-btn" onclick="window.location.href='../../'" aria-label="Back to home">üè°</button>
  <button id="toggleDark" class="float-btn" aria-label="Toggle dark mode">üåô</button>

  <!-- Markdown content -->
  <main class="content">
    <article class="markdown-body">
      <h1
      id="shallow-fusion-bridging-data-scarcity-and-ai-integration-challenges">üîó
      Shallow Fusion: Bridging Data Scarcity and AI Integration
      Challenges</h1>
      <p><strong>COLLINS WESTNEDGE</strong><br />
      <em>AUGUST 26, 2025</em></p>
      <h2 id="introduction">Introduction</h2>
      <p>AI adoption and integration have become focal points in
      seemingly every earnings call, LinkedIn post, townhall and
      industry keynote. However, most of these conversations exist to
      highlight revenue potential, promote products and services, or
      bolster positive consumer sentiment, which is likely why they tend
      to gloss over or abstract away the technical challenges that stand
      in the way of effective adoption. One of the fundamental
      challenges is the gap between available data and the data needed
      for a domain-specific task.</p>
      <p>Consider, for example, applying a large generalist model to a
      highly specialized task that barely surfaces in its pretraining
      data if at all. For the generalist model to succeed, it must first
      grasp dense company prospectuses, specialized jargon, and the
      nuances of the business problem itself. To address this gap
      companies often resort to standard recipes, e.g., ‚Äúexciting‚Äù the
      right activations through few-shot examples, dumping streams of
      internal documents into the model‚Äôs context, or ambitious attempts
      at fine-tuning on small internal datasets. However, with most of
      these approaches there‚Äôs often no optimization signal, or gradient
      to move against and progress, if any, involves a good deal of
      guesswork, trial, and error.</p>
      <p><strong>Automatic Speech Recognition (ASR)</strong> exemplifies
      this challenge. Many domains, such as medicine, law, and financial
      services contain specialized terminology that is typically outside
      the distribution or under-represented in the pretraining for
      general purpose models. A model trained on everyday speech will
      struggle with phrases like ‚Äúorthostatic tachycardia‚Äù or
      specialized phonemes that are difficult to disambiguate, such as
      ‚ÄúICU‚Äù versus ‚ÄúI see you‚Äù. Traditional solutions to this issue
      involve collecting domain-specific audio and ground truth
      transcriptions (often hand labeled) which can be cost prohibitive.
      Open source datasets on specialized domains are becoming more
      common but their volume and variety remain limited, keeping them
      tangential to many business use cases.</p>
      <p>This distribution gap has motivated researchers and
      practitioners to explore the concept of <strong>shallow
      fusion</strong>: combining general-purpose ASR models with
      domain-specific language models during inference. Rather than
      requiring extensive retraining, shallow fusion leverages existing
      domain expertise from an external language model at inference
      time. While the approach has shown promise in various
      implementations, the questions I would like to explore in this
      article are: Can a language model trained on domain-specific text
      meaningfully improve speech-to-text transcription quality within
      an adjacent domain? And critically, what are the failure modes
      associated with this type of integration?</p>
      <h2 id="background-existing-approaches">Background &amp; Existing
      Approaches</h2>
      <p>The challenge of domain adaptation in ASR has prompted several
      approaches, each with distinct trade-offs in cost, performance,
      and implementation complexity. Before diving into my
      implementation, I‚Äôll examine how the research community has
      approached this domain mismatch problem and where shallow fusion
      fits among existing solutions.</p>
      <h3 id="traditional-domain-adaptation">Traditional Domain
      Adaptation</h3>
      <p><strong>Traditional domain adaptation</strong> typically
      requires collecting domain-specific audio paired with ground truth
      transcriptions, then fine-tuning pretrained models on this data.
      While effective, this approach faces significant barriers:
      domain-specific audio is expensive to collect, transcription
      labeling is labor-intensive, and the resulting datasets often
      remain small and brittle compared to the large scale datasets that
      the base model was trained on. This approach runs the risk of
      <strong>catastrophic forgetting</strong><sup>1</sup> where the
      model loses its general capabilities when adapting to the specific
      domain.</p>
      <h3 id="context-injection-methods">Context Injection Methods</h3>
      <p><strong>Context injection methods</strong> attempt to bridge
      the gap by incorporating domain-specific text directly into the
      model‚Äôs context window, essentially ‚Äúprompting‚Äù the ASR system
      with relevant terminology. However, these approaches offer no
      optimization signal and rely heavily on trial and error to achieve
      meaningful improvements. They are also architecture dependent and
      rely on the decoder‚Äôs prompting capacity, which may be limited in
      models not explicitly designed for such conditioning.</p>
      <h3 id="fusion-techniques">Fusion Techniques</h3>
      <p><strong>Fusion techniques</strong> represent a middle ground,
      combining predictions from multiple models during inference rather
      than requiring extensive retraining. The research community has
      explored three primary variants:</p>
      <ul>
      <li><p><strong>Shallow fusion</strong> combines model predictions
      at inference time via a weighted average of ASR and LM scores,
      requiring no additional training (Gulcehre et al., 2015).</p></li>
      <li><p><strong>Deep fusion</strong> augments the decoder with a
      small gating network that learns to merge hidden representations
      from the ASR and LM while keeping both models frozen (Gulcehre et
      al., 2015).</p></li>
      <li><p><strong>Cold fusion</strong> builds on the idea of deep
      fusion but with a key difference: instead of training the ASR
      model first and then adding a language model later, the ASR model
      is trained from scratch alongside a fixed, pretrained LM (Sriram
      et al., 2017).</p></li>
      </ul>
      <!-- Because the ASR model is exposed to the LM throughout training, it learns to rely on the LM for linguistic information while dedicating its own capacity to mapping acoustic features into text. This disentanglement allows even relatively small decoders to perform well.   -->
      <p>Shallow fusion‚Äôs appeal lies in its simplicity and flexibility,
      as it requires no additional training of the base ASR model.
      Instead, you incorporate predictions from an external language
      model directly at inference time, blending the acoustic model‚Äôs
      view of the audio with the language model‚Äôs understanding of
      domain-specific text. Importantly, the only data needed to build
      or adapt the external language model is unstructured text, which
      can be collected far more easily than audio transcriptions.</p>
      <p>However, the approach introduces its own challenges. If the
      language model is weighted too heavily, it may bias transcriptions
      toward plausible but incorrect tokens; too lightly, and the domain
      benefits are lost. Tuning the weighting factor for the external
      model often requires domain-specific adjustment. In addition,
      shallow fusion increases inference cost since predictions must run
      through a second model<sup>2</sup>. These trade-offs make it
      essential to understand the method‚Äôs failure modes before
      deploying it in practice.</p>
      <h2
      id="implementation-medical-domain-fusion-pipeline">Implementation:
      Medical Domain Fusion Pipeline</h2>
      <p>Having established the landscape of existing approaches, we can
      now detail the implementation of shallow fusion for medical ASR,
      combining Whisper (our ASR model) with a domain-adapted GPT-2
      model (our external language model). However, before going into
      the specifics let us first build some intuition on the topic by
      analogy.</p>
      <h3 id="conceptual-framework">Conceptual Framework</h3>
      <p>Consider, for example, a person tasked with transcribing audio
      from a phone call between a customer and a claims representative
      at an insurance call center. This transcriber can hear the
      conversation clearly, but they have very little knowledge of the
      domain, e.g., the technical issues, procedures, and medical
      terminology that often come up. Now imagine a second person who
      has worked in this industry for years and has deep familiarity
      with the jargon and context, but who is hard of hearing.</p>
      <p>In practice, the first person might hear a phrase like
      ‚Äúmyocardial infarction‚Äù but misrecognize or misspell it. The
      domain expert, although unable to hear the audio, would
      immediately recognize the intended term and correct the
      transcript.</p>
      <p>Shallow fusion can be thought of as a process of integrating
      each person‚Äôs expertise to offset the errors of one another and
      bridge modalities the other does not have access to. With this
      analogy, we can now formally describe this process. In the example
      below think of <span class="math inline">\(P_{\text{ASR}}\)</span>
      as the person listening to the audio and <span
      class="math inline">\(P_{\text{LM}}\)</span> as the domain expert
      that is hard of hearing but deeply understands the context.</p>
      <h3 id="mathematical-formulation">Mathematical Formulation</h3>
      <p>At each decoding step for some audio input, we select the most
      probable token <span class="math inline">\(y_{t}\)</span> using
      information from the Automatic Speech Recognition model (ASR) and
      the Language Model (LM)</p>
      <p><span class="math display">\[
      y^* = \arg\max_{y_t}\;
      \left[
      \log P_{\text{ASR}}\!\bigl(y_t \mid x,\, y_{&lt;t}\bigr)
      \;+\;
      \lambda\,\log P_{\text{LM}}\!\bigl(y_t \mid y_{&lt;t}\bigr)
      \right]
      \]</span></p>
      <p>where:<br />
      - <span class="math inline">\(t\)</span> is the decoding step
      (0-based).<br />
      - <span class="math inline">\(y_t\)</span> is the chosen token at
      step <span class="math inline">\(t\)</span> and <span
      class="math inline">\(y_{&lt;t}\)</span> are previously generated
      tokens.<br />
      - <span class="math inline">\(x\)</span> represents the acoustic
      features (e.g., raw audio input).<br />
      - <span class="math inline">\(P_{\text{ASR}}\)</span> depends on
      both <span class="math inline">\(x\)</span> and <span
      class="math inline">\(y_{&lt;t}\)</span>, while <span
      class="math inline">\(P_{\text{LM}}\)</span> depends on <span
      class="math inline">\(y_{&lt;t}\)</span> only.<br />
      - <span class="math inline">\(\lambda\)</span> is the weighting
      factor to determine the language model‚Äôs influence.</p>
      <p>The idea is that the ASR model understands phonetics and
      language in a general sense while the LM model understands the
      specialized domain in its written form, but has no access to the
      audio signal. Just like in the analogy from earlier by fusing
      their predictions, we combine phonetic understanding with domain
      expertise, aiming to improve the quality of transcriptions for
      domain-specific terms. Without careful integration or synergy
      between the two, both models can carry major limitations.</p>
      <h4 id="process-diagram">Process Diagram:</h4>
      <p><img src="assets/viz.png" alt="diagram" /> Reference: Kannan et
      al.¬†(2017)</p>
      <h3 id="practical-example">Practical Example</h3>
      <p>Consider an example where Whisper serves as our listening
      expert and GPT-2 as our domain-language expert. In practice these
      models share a tokenizer making the process of integrating their
      predictions fairly seamless at least for the English version of
      Whisper (Radford et al., 2022). Now let‚Äôs consider a claims call
      center transcript where an ASR model misinterprets a specialized
      medical term.</p>
      <p><strong>Input Audio (Ground Truth):</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs Tetralogy of Fallot.‚Äù‚úîÔ∏è</p>
      <p><strong>Whisper Initial Output:</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs Tetralogy of below.‚Äùüö´</p>
      <h4 id="step-by-step-fusion-process">Step-by-Step Fusion
      Process</h4>
      <p><strong>1. Whisper Initial Decoding:</strong></p>
      <p>Whisper produces logits at each step:</p>
      <ul>
      <li>Token: ‚ÄúThe‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äúprocedure‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äúclaimant‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äú‚Äôs‚Äù ‚Üí high confidence<br />
      </li>
      <li>At the final subword, Whisper may exhibit uncertainty,
      spreading probabilities across candidates: ‚Äúbelow‚Äù, ‚Äúfollow‚Äù,
      ‚ÄúFallot‚Äù</li>
      </ul>
      <p><strong>2. Domain GPT-2 Predictions:</strong><br />
      At the ambiguous decoding step in ‚ÄúThe procedure was medically
      necessary for the treatment of claimant‚Äôs Tetralogy of ____‚Äù, each
      model produces different log probabilities:</p>
      <table>
      <thead>
      <tr>
      <th>Next Token</th>
      <th>Whisper Log Probs</th>
      <th>GPT-2 Log Probs</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>Fallot</strong></td>
      <td><strong>‚Äì1.8</strong></td>
      <td><strong>‚Äì0.3</strong></td>
      </tr>
      <tr>
      <td>below</td>
      <td>‚Äì1.0</td>
      <td>‚Äì5.0</td>
      </tr>
      <tr>
      <td>follow</td>
      <td>‚Äì3.5</td>
      <td>‚Äì3.8</td>
      </tr>
      </tbody>
      </table>
      <p><br></p>
      <blockquote>
      <p><em>Note: GPT-2, which has been fine tuned on medical
      literature, strongly favors the correct token (produces log
      probabilities closer to 0 for Fallot) while Whisper, which had
      minimal access to medical terminology, assigns it a much lower
      likelihood (log probabilities that are more negative).</em></p>
      </blockquote>
      <p><strong>3. Shallow Fusion (Combining Logits):</strong></p>
      <p><strong>Fusion Equation:</strong></p>
      <p>We combine each model‚Äôs logits using a weighted sum in the
      following way:</p>
      <p><span class="math display">\[
      \log P_{\text{combined}}(y_t) = \log P_{\text{Whisper}}(y_t \mid
      x, y_{&lt;t}) + \lambda \log P_{\text{GPT2}}(y_t \mid y_{&lt;t})
      \]</span></p>
      <p><strong>Example:</strong></p>
      <table>
      <colgroup>
      <col style="width: 24%" />
      <col style="width: 24%" />
      <col style="width: 22%" />
      <col style="width: 28%" />
      </colgroup>
      <thead>
      <tr>
      <th>Next Token</th>
      <th>Whisper Score</th>
      <th>GPT-2 Score</th>
      <th>Combined Score (Œª = 0.2)</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>Fallot</strong></td>
      <td><strong>‚Äì1.8</strong></td>
      <td><strong>‚Äì0.3</strong></td>
      <td><strong>‚Äì1.8 + 0.2 √ó (‚Äì0.3) = ‚Äì1.86</strong></td>
      </tr>
      <tr>
      <td>below</td>
      <td>‚Äì1.0</td>
      <td>‚Äì5.0</td>
      <td>‚Äì1.0 + 0.2 √ó (‚Äì5.0) = ‚Äì2.0</td>
      </tr>
      <tr>
      <td>follow</td>
      <td>‚Äì3.5</td>
      <td>‚Äì3.8</td>
      <td>‚Äì3.5 + 0.2 √ó (‚Äì3.8) = ‚Äì4.26</td>
      </tr>
      </tbody>
      </table>
      <blockquote>
      <p><em>Note: The numbers here are illustrative. In practice
      additional context and scaling would favor the correct token
      ‚ÄúFallot‚Äù; additionally, rare words are likely split into multiple
      tokens but the intuition remains the same.</em></p>
      </blockquote>
      <p>‚ÄúFallot‚Äù now has the highest combined score.</p>
      <p><strong>Final Corrected Output:</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs Tetralogy of Fallot.‚Äù‚úîÔ∏è</p>
      <p>This illustrates how <strong>domain-aware shallow
      fusion</strong> could potentially improve ASR output in
      specialized contexts.</p>
      <h2 id="experimental-setup">Experimental Setup</h2>
      <h3 id="model-selection-and-preparation">Model Selection and
      Preparation</h3>
      <p>For this implementation, I chose <strong>Whisper</strong> as
      the base ASR model due to its strong general-purpose performance
      and <strong>GPT-2</strong> as the domain-specific language model.
      The external model selected for this fusion process was GPT-2
      small, medium, and large. The reason for selecting these models
      was partly due to convenience, because pre-trained versions are
      widely available and they share a tokenizer/vocabulary with
      Whisper‚Äôs decoder. The shared vocabulary means we do not have to
      learn a mapping from one model‚Äôs vocabulary to another.</p>
      <p>While Bio-GPT represents an existing medical language model, it
      uses a different tokenizer that would require learning a mapping
      function between tokenization schemes. To avoid potential errors
      and implementation complexity, I opted to train custom GPT-2
      variants on medical data while preserving Whisper‚Äôs tokenizer
      compatibility.</p>
      <h3 id="training-domain-specific-language-models">Training
      Domain-Specific Language Models</h3>
      <p>To adapt an external language model to the medical domain, the
      PubMed dataset was used. Multiple versions of GPT-2 were tuned on
      roughly <strong>3.63 billion tokens</strong> from PubMed
      abstracts. Three GPT-2 variants were trained to create the
      following domain-adapted language models:</p>
      <ul>
      <li><strong>GPT-2 Small</strong> (124M parameters)</li>
      <li><strong>GPT-2 Medium</strong> (355M parameters)</li>
      <li><strong>GPT-2 Large</strong> (774M parameters)</li>
      </ul>
      <p>The models were trained using standard autoregressive language
      modeling objectives on this large corpus of medical abstracts.
      Retaining Whisper‚Äôs tokenizer ensured seamless fusion, eliminating
      any need for token mapping or vocabulary alignment.</p>
      <p><strong>Training pipeline and tuned models:</strong></p>
      <ul>
      <li><a
      href="https://github.com/donkeyanaphora/SHALLOW_FUSION">View on
      GitHub</a><br />
      </li>
      <li><a href="https://huggingface.co/cwestnedge/models">Hugging
      Face Models</a></li>
      </ul>
      <h3 id="fusion-pipeline-architecture">Fusion Pipeline
      Architecture</h3>
      <p>The fusion pipeline operates selectively on relevant tokens
      only. Whisper contains special task-related tokens (language
      identifiers, task specifiers, timestamps) that are outside the
      scope of GPT-2‚Äôs vocabulary and training domain. However, for the
      English transcription task, Whisper should not emit these special
      tokens during normal operation, making this a non-issue in
      practice.</p>
      <p>The implementation performs fusion by:</p>
      <ol type="1">
      <li>Running Whisper‚Äôs encoder to generate audio features</li>
      <li>At each decoding step, computing logit distributions from both
      Whisper‚Äôs decoder and the domain-adapted GPT-2</li>
      <li>Combining logits using the weighted sum formulation described
      earlier</li>
      <li>Selecting tokens based on the fused probability
      distribution</li>
      </ol>
      <h3 id="evaluation-framework">Evaluation Framework</h3>
      <p>Testing was conducted on 85 synthetic radiology report
      dictations (each under 30 seconds). While ideally evaluation would
      occur on authentic clinical dictations, access to such datasets
      typically requires institutional permissions and agreements. To
      generate the synthetic dataset, I prompted a language model to
      create realistic radiology report dictations that mirror the
      style, terminology, and content patterns found in actual clinical
      documentation. While this limited dataset demonstrates
      feasibility, production deployment would require validation on
      larger, authentic clinical datasets.</p>
      <p>The primary evaluation metric was Word Error Rate
      (WER)<sup>3</sup>, which measures the percentage of incorrectly
      transcribed words. Testing compared transcriptions from: -
      Whisper-only baseline - Shallow fusion with various Œª weighting
      values - Different GPT-2 model sizes (small, medium, large)</p>
      <h2 id="results-analysis">Results &amp; Analysis</h2>
      <h3 id="overall-performance">Overall Performance</h3>
      <p>In preliminary synthetic evaluations, shallow fusion showed
      consistent WER reductions across different model sizes on the
      synthetic radiology dataset. For Whisper Small + GPT-2 PubMed
      Small, WER decreased from <strong>8.31%</strong> to
      <strong>7.31%</strong> at optimal Œª values‚Äîa <strong>12% relative
      reduction in errors</strong>. The Whisper Medium + GPT-2 PubMed
      Medium configuration showed even stronger results, reducing WER
      from <strong>6.18%</strong> to <strong>5.30%</strong> at Œª =
      0.12‚Äîa <strong>14.8% relative improvement</strong>. These
      preliminary results show patterns similar to prior work: Kannan et
      al.¬†(2017) reported a <strong>9.1% relative WER reduction</strong>
      on Google Voice Search using shallow fusion with a neural LM.</p>
      <h3 id="hyperparameter-sensitivity-Œª-lambda-weight">Hyperparameter
      Sensitivity (Œª / Lambda Weight)</h3>
      <p>To evaluate the effect of the fusion weight Œª, it was varied
      between 0.03 and 0.30 using two model configurations. Although
      different model sizes could be mixed and matched (e.g., GPT-2
      Medium with Whisper Tiny), matching model sizes were used to
      ensure improvements reflected fusion rather than raw model
      capacity differences.</p>
      <p><strong>Table 1. WER vs.¬†Œª for Whisper Small + GPT-2 PubMed
      Small</strong><br />
      <em>Baseline WER = 0.0831</em></p>
      <table style="width:100%;">
      <colgroup>
      <col style="width: 20%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      </colgroup>
      <thead>
      <tr>
      <th>Œª (Fusion Weight)</th>
      <th>0.03</th>
      <th>0.06</th>
      <th>0.09</th>
      <th>0.12</th>
      <th>0.15</th>
      <th>0.18</th>
      <th>0.21</th>
      <th>0.24</th>
      <th>0.27</th>
      <th>0.30</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>Fused WER</strong></td>
      <td>0.079</td>
      <td>0.076</td>
      <td>0.073</td>
      <td>0.074</td>
      <td>0.074</td>
      <td>0.075</td>
      <td>0.073</td>
      <td>0.082</td>
      <td>0.084</td>
      <td>0.087</td>
      </tr>
      <tr>
      <td><strong>Relative Improvement (%)</strong></td>
      <td>4.4</td>
      <td>8.9</td>
      <td>12.0</td>
      <td>10.8</td>
      <td>10.9</td>
      <td>9.4</td>
      <td>12.1</td>
      <td>1.2</td>
      <td>-1.4</td>
      <td>-4.8</td>
      </tr>
      </tbody>
      </table>
      <p><br></p>
      <p><strong>Table 2. WER vs.¬†Œª for Whisper Medium + GPT-2 PubMed
      Medium</strong><br />
      <em>Baseline WER = 0.0618</em></p>
      <table style="width:100%;">
      <colgroup>
      <col style="width: 20%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      <col style="width: 7%" />
      </colgroup>
      <thead>
      <tr>
      <th>Œª (Fusion Weight)</th>
      <th>0.03</th>
      <th>0.06</th>
      <th>0.09</th>
      <th>0.12</th>
      <th>0.15</th>
      <th>0.18</th>
      <th>0.21</th>
      <th>0.24</th>
      <th>0.27</th>
      <th>0.30</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>Fused WER</strong></td>
      <td>0.057</td>
      <td>0.056</td>
      <td>0.056</td>
      <td>0.053</td>
      <td>0.054</td>
      <td>0.055</td>
      <td>0.054</td>
      <td>0.054</td>
      <td>0.055</td>
      <td>0.058</td>
      </tr>
      <tr>
      <td><strong>Relative Improvement (%)</strong></td>
      <td>7.7</td>
      <td>9.2</td>
      <td>9.1</td>
      <td>14.8</td>
      <td>12.7</td>
      <td>11.5</td>
      <td>13.2</td>
      <td>12.0</td>
      <td>10.3</td>
      <td>6.8</td>
      </tr>
      </tbody>
      </table>
      <blockquote>
      <p><em>Note: Variability across Œª values likely reflects the small
      synthetic evaluation set (85 samples); see statistical
      significance analysis below.</em></p>
      </blockquote>
      <p>In synthetic testing, the Small model configuration showed
      optimal results at Œª = 0.09 and Œª = 0.21, yielding approximately
      12% relative WER reduction. The Medium configuration shows optimal
      performance at Œª = 0.12 with 14.8% improvement. Across both
      configurations improvements are observed in the 0.06-0.21 range.
      Higher fusion weights (Œª ‚â• 0.24) show degraded performance, with Œª
      = 0.30 performing worse than baseline in the Small configuration
      (-4.8%).</p>
      <h3 id="statistical-significance">Statistical Significance</h3>
      <p>We tested two versions of the system on 85 of the same audio
      clips. The new (fused) system made <strong>about 12% fewer word
      mistakes overall</strong> (from 5.60% down to 4.92%). When we do a
      fair ‚Äúshuffle‚Äù check to see if that difference could be luck,
      results like this show up <strong>about 1 in 5 times</strong> (or
      <strong>about 1 in 10</strong> if you only ask whether the new
      system is better), so the win is <strong>suggestive but not
      definitive</strong>. Looking clip-by-clip (treating each clip
      equally), errors drop <strong>about 15%</strong>, with checks
      indicating <strong>roughly a 1 in 12 to 1 in 19</strong> chance
      it‚Äôs just noise‚Äîagain <strong>borderline</strong>. Overall: a
      <strong>small, consistent improvement</strong> that likely
      benefits from <strong>more data</strong> to be conclusive.</p>
      <h3 id="error-pattern-analysis-and-failure-modes">Error Pattern
      Analysis and Failure Modes</h3>
      <p>While the synthetic evaluation showed promising patterns,
      analysis revealed specific failure modes that illuminate the
      method‚Äôs limitations:</p>
      <p><strong>1. Abbreviation Expansion Mismatches</strong><br />
      The fusion system frequently ‚Äúover-corrected‚Äù spoken abbreviations
      into their formal written equivalents. For example: - Audio:
      ‚Äúcentimeters‚Äù ‚Üí Whisper: ‚Äúcentimeters‚Äù ‚Üí Fused output: ‚Äúcm‚Äù - This
      reflects the domain language model‚Äôs bias toward written medical
      documentation style</p>
      <p><strong>2. Punctuation Insertion</strong><br />
      The GPT-2 model, trained on formatted medical abstracts,
      introduced punctuation that wasn‚Äôt present in the spoken audio.
      This created a stylistic mismatch between transcribed speech and
      formal written medical language.</p>
      <p><strong>3. Premature Termination and Incomplete
      Transcripts</strong><br />
      When Œª (the LM weight) was set too high, beam search decoding
      often produced incomplete transcripts. Chorowski &amp; Jaitly
      (2016) reported that external LMs can cause seq2seq systems to
      skip words or drop parts of an utterance during decoding, unless a
      coverage term is added to the beam search criterion. In our
      experiments, higher Œª coupled with wide beam searches similarly
      led to premature terminations, with the LM assigning high
      probability to end-of-sequence tokens once a transcript appeared
      semantically complete, even while audio continued.</p>
      <h3 id="domain-specific-improvements">Domain-Specific
      Improvements</h3>
      <p>The fusion approach‚Äôs benefits were concentrated almost
      exclusively in medical terminology recognition. Examples of
      successful corrections included:</p>
      <ul>
      <li>Complex pharmaceutical names</li>
      <li>Anatomical terminology</li>
      <li>Rare disease names and medical conditions</li>
      <li>Procedural and diagnostic terminology</li>
      </ul>
      <p>Standard conversational language showed minimal improvement,
      suggesting that benefits in this synthetic evaluation may derive
      from domain expertise rather than general language modeling
      enhancement.</p>
      <h2 id="reflection-and-future-directions">Reflection and Future
      Directions</h2>
      <h3 id="addressing-current-limitations">Addressing Current
      Limitations</h3>
      <p>The experimental results highlight several areas for
      improvement that point toward promising future research
      directions:</p>
      <h4 id="real-world-dataset-validation">Real-World Dataset
      Validation</h4>
      <p>The synthetic evaluation dataset, while useful for
      proof-of-concept demonstration, limits the generalizability of
      these findings. Future work should incorporate authentic clinical
      dictations such as the <a
      href="https://marketplace.databricks.com/details/8eb39dd5-ffc4-4e8d-8f89-25d91bf1774b/Shaip_Physician-Dictation-Data-Radiology">Shaip
      Physician Dictation Dataset</a>, which requires Databricks account
      permissions. Real clinical speech presents challenges absent in
      synthetic data: background noise, speaker variations,
      interruptions, and the full complexity of clinical communication
      patterns.</p>
      <h4 id="learned-gating-mechanisms">Learned Gating Mechanisms</h4>
      <p>The static Œª weighting approach represents a significant
      limitation. A more sophisticated system would dynamically adjust
      the influence of the external language model based on acoustic
      confidence and contextual cues. When Whisper exhibits high
      confidence in its predictions, the domain model should have
      minimal influence. Conversely, during periods of acoustic
      uncertainty‚Äîparticularly around medical terminology‚Äîthe fusion
      weight should increase. Implementing this would likely involve
      training a small gating network that learns to predict optimal Œª
      values given acoustic features and partial transcript context.</p>
      <h4 id="advanced-fusion-architectures">Advanced Fusion
      Architectures</h4>
      <p>Beyond shallow fusion, <strong>deep fusion</strong> and
      <strong>cold fusion</strong> approaches warrant investigation.
      Deep fusion could learn more sophisticated integration by
      combining hidden states and tuning a task-specific fusion
      function. Cold fusion could be explored by integrating the domain
      language model during Whisper‚Äôs training process, though this
      would require more substantial computational resources and
      training data.</p>
      <h3 id="broader-implications">Broader Implications</h3>
      <p>This work connects to several important trends in contemporary
      AI development:</p>
      <p><strong>Ensemble and Mixture-of-Experts Architectures:</strong>
      Shallow fusion represents a simple form of ensemble modeling,
      where specialized models contribute their expertise to improve
      overall performance. This aligns with the broader trend toward
      Mixture-of-Experts architectures that dynamically route inputs to
      specialized sub-networks.</p>
      <p><strong>Multimodal Integration Challenges:</strong> The fusion
      of acoustic and textual information highlights fundamental
      challenges in multimodal AI systems. Different modalities often
      have distinct statistical properties and optimal representations,
      requiring careful integration strategies.</p>
      <p><strong>Domain Adaptation Strategies:</strong> As AI systems
      deploy across increasingly specialized domains, the tension
      between general capability and domain expertise becomes more
      pronounced. Shallow fusion offers one approach to leveraging
      domain-specific knowledge without extensive retraining of large
      general-purpose models.</p>
      <h2 id="conclusion">Conclusion</h2>
      <p>This exploration of shallow fusion for medical ASR demonstrates
      both the promise and limitations of combining general-purpose
      models with domain-specific expertise. The key insight is that
      each model type hits distinct ‚Äúdata walls‚Äù:</p>
      <p><strong>Whisper (Generalist Model)</strong> excels at
      acoustic-to-text mapping and handles diverse speakers, accents,
      and recording conditions effectively. However, its broad training
      distribution means medical terminology remains under-represented,
      leading to systematic errors on specialized vocabulary despite
      strong general performance.</p>
      <p><strong>GPT-2 (Domain Specialist),</strong> trained on PubMed
      abstracts, develops rich representations of medical terminology
      and context through self-supervised learning on abundant textual
      data. However, it remains completely blind to acoustic signals and
      exhibits biases toward formal written language rather than
      conversational speech patterns.</p>
      <p>The preliminary synthetic evaluation, showing up to 14.8% WER
      reduction, suggests shallow fusion may have potential for
      improvement, though further validation on clinical data is needed.
      Additionally, the failure modes (abbreviation mismatches,
      punctuation insertion, and premature terminations) reveal the
      challenges of bridging modalities with different statistical
      properties and stylistic conventions.</p>
      <p>The observed improvements appeared concentrated on medical
      terminology recognition, suggesting that the benefits may derive
      from genuine domain expertise rather than general language
      modeling improvements. This specificity, while limiting the
      approach‚Äôs broad applicability, makes it particularly valuable for
      specialized transcription applications where domain terminology
      accuracy is critical.</p>
      <p>Future work towards learned gating mechanisms, advanced fusion
      architectures, and validation on authentic clinical datasets will
      help address current limitations. More broadly, this work
      illustrates the ongoing evolution of AI system architectures from
      monolithic models toward composite systems that combine
      specialized expertise, a trend likely to accelerate as AI
      deployment expands across diverse professional domains.</p>
      <h2 id="resources">Resources</h2>
      <ul>
      <li><a href="https://arxiv.org/pdf/1503.03535">On Using
      Monolingual Corpora in Neural Machine Translation</a> ‚Äî Gulcehre
      et al., 2015<br />
      </li>
      <li><a href="https://arxiv.org/pdf/1708.06426">Cold Fusion:
      Training Seq2Seq Models Together with Language Models</a> ‚Äî Sriram
      et al., 2017<br />
      </li>
      <li><a href="https://arxiv.org/pdf/1612.02695">Towards Better
      Decoding and Language Model Integration in Sequence-to-Sequence
      Models</a> ‚Äî Chorowski &amp; Jaitly, 2016<br />
      </li>
      <li><a href="https://arxiv.org/pdf/1712.01996">Analysis of
      Incorporating an External Language Model‚Ä¶</a> ‚Äî Kannan et al.,
      2017<br />
      </li>
      <li><a href="https://arxiv.org/pdf/2212.04356">Robust Speech
      Recognition via Large-Scale Weak Supervision</a> ‚Äî Radford et al.,
      2022<br />
      </li>
      <li><a
      href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language
      Models are Unsupervised Multitask Learners</a> ‚Äî OpenAI,
      2019<br />
      </li>
      <li><a href="https://arxiv.org/pdf/2005.14165">Language Models are
      Few-Shot Learners</a> ‚Äî Brown et al., 2020</li>
      </ul>
      <hr />
      <ol type="1">
      <li><p><a
      href="https://en.wikipedia.org/wiki/Catastrophic_interference">Catastrophic
      forgetting</a> occurs when a neural network loses previously
      learned information upon learning new tasks or data.</p></li>
      <li><p>Several variations exist to reduce the inference cost of
      shallow fusion, including N-best rescoring (applying the LM only
      to candidate transcripts) and using smaller or distilled domain
      LMs.</p></li>
      <li><p><a
      href="https://en.wikipedia.org/wiki/Word_error_rate">Word Error
      Rate (WER)</a> is the standard metric for evaluating ASR systems,
      calculated as the minimum number of word-level edits (insertions,
      deletions, substitutions) required to transform the hypothesis
      into the reference, divided by the total number of words in the
      reference.</p></li>
      </ol>
    </article>
  </main>

  <!-- article JS (dark mode only) -->
  <script defer src="../../assets/js/article.js"></script>
</body>
</html>