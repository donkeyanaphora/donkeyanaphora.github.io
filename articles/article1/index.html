<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Shallow Fusion: Bridging Data Scarcity and AI Integration
Challenges - Collins Westnedge</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no, viewport-fit=cover">
  
  <!-- Mobile app settings -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- SEO essentials -->
  <meta name="google-site-verification" content="hmW81pKLkPLJXY0kFe-I1vNXno-xD9XbEWyZyGfz-SA" />
  <meta name="description" content="A minor rant about ‚ÄòAI adoption‚Äô and
exploration of shallow fusion as a method to address data scarcity and
integration in specialized domains">
  <meta name="author" content="Collins Westnedge">
  <link rel="canonical" href="https://donkeyanaphora.github.io/articles/article1/">
  
  <!-- Hide drafts from search engines -->
  <meta name="robots" content="noindex, nofollow">

  <!-- Site look & feel -->
  <link rel="stylesheet" href="../../assets/css/main.css"/>
  <link rel="stylesheet" href="../../assets/css/article.css"/>
  
  <!-- Favicons -->
  <link rel="apple-touch-icon" sizes="180x180" href="../../favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../../favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../../favicon/favicon-16x16.png">
  <link rel="manifest" href="../../favicon/site.webmanifest">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://donkeyanaphora.github.io/articles/article1/">
  <meta property="og:title" content="Shallow Fusion: Bridging Data
Scarcity and AI Integration Challenges">
  <meta property="og:description" content="A minor rant about ‚ÄòAI
adoption‚Äô and exploration of shallow fusion as a method to address data
scarcity and integration in specialized domains">
  <meta property="og:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">
  <meta property="og:site_name" content="The Latent Realm">
  <meta property="article:author" content="Collins Westnedge">
  <meta property="article:published_time" content="2025-04-16">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:url" content="https://donkeyanaphora.github.io/articles/article1/">
  <meta name="twitter:title" content="Shallow Fusion: Bridging Data
Scarcity and AI Integration Challenges">
  <meta name="twitter:description" content="A minor rant about ‚ÄòAI
adoption‚Äô and exploration of shallow fusion as a method to address data
scarcity and integration in specialized domains">
  <meta name="twitter:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">

  <!-- MathJax config -->
  <!-- <script>
    window.MathJax = {
      tex:   { displayIndent: '0em',  displayAlign: 'center' },
      chtml: { mtextFont: 'Times' }
    };
  </script> -->
  
  <script>
    window.MathJax = {
      tex: {},  // Empty is fine
      chtml: { 
        mtextFont: 'Times',
        displayAlign: 'center',    // ‚úÖ Moved here
        displayIndent: '0em'       // ‚úÖ Moved here
      }
    };
  </script>
  <!-- MathJax -->
  <script defer id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js">
  </script>
</head>

<body class="article">
  <!-- floating buttons -->
  <button id="menuBtn" class="float-btn" aria-label="Open menu">üß∞</button>
  <button class="float-btn back-btn" onclick="window.location.href='../../'" aria-label="Back to home">üè°</button>
  <button id="toggleDark" class="float-btn" aria-label="Toggle dark mode">üåô</button>

  <!-- Markdown content -->
  <main class="content">
    <article class="markdown-body">
      <h1
      id="shallow-fusion-bridging-data-scarcity-and-ai-integration-challenges">üîó
      Shallow Fusion: Bridging Data Scarcity and AI Integration
      Challenges</h1>
      <p><strong>COLLINS WESTNEDGE</strong><br />
      <em>APRIL 16, 2025</em></p>
      <h2 id="introduction">Introduction</h2>
      <p>AI adoption and integration have become focal points in
      seemingly every earnings call, linkedin post, townhall and
      industry keynote. However, most of these conversations exist to
      highlight revenue potential, promote products and services, or
      bolster positive consumer sentiment, which is likely why they tend
      to gloss over or abstract away the technical challenges that stand
      in the way of effective adoption. One of the fundamental
      challenges is the gap between available data and the data needed
      for a domain-specific task.</p>
      <p>Consider for example applying a large generalist model to a
      highly specialized task that barely surfaces in its pretraining
      data if at all. For the generalist model to succeed, it must first
      grasp dense company prospectuses, specialized jargon, and the
      nuances of the business problem itself. To address this gap
      companies often resort to standard recipes e.g.¬†‚Äúexciting‚Äù the
      right activations through few-shot examples, dumping streams of
      internal documents into the model‚Äôs context, or ambitious attempts
      at fine-tuning on small internal datasets. However, with most of
      these approaches there‚Äôs often no optimization signal, or gradient
      to move against and progress if there‚Äôs any to be had involves a
      good deal of guesswork, trial, and error.</p>
      <p><strong>Automatic Speech Recognition (ASR)</strong> exemplifies
      this challenge. Many domains, such as medicine, law, financial
      services, etc contain specialized terminology that is typically
      outside the distribution or under-represented in the pretraining
      for general purpose models. A model trained on everyday speech
      will struggle with phrases like ‚Äúorthostatic tachycardia‚Äù or
      specialized phonemes that are difficult to disambiguate, such as
      ‚ÄúICU‚Äù vs ‚ÄúI see you‚Äù. Traditional solutions to this issue involve
      collecting domain-specific audio and ground truth transcriptions
      (often hand labeled) which can be cost prohibitive. Open source
      datasets on specialized domains are becoming more common but their
      volume and variety remain limited, keeping them tangential to many
      business use cases.</p>
      <p>This distribution gap has motivated researchers and
      practitioners (myself included) to explore the concept of
      <strong>shallow fusion</strong>: combining general-purpose ASR
      models with domain-specific language models during inference.
      Rather than requiring extensive retraining, shallow fusion
      leverages existing domain expertise from an external language
      model at inference time. While the approach has shown promise in
      various implementations, the questions I would like to explore in
      this article are: Can a language model trained on domain-specific
      text meaningfully improve speech-to-text transcription quality
      within an adjacent domain? And critically, what are the failure
      modes associated with this type of integration?</p>
      <h2
      id="background-existing-approaches-needs-refinement">Background
      &amp; Existing Approaches (needs refinement)</h2>
      <p>The challenge of domain adaptation in ASR has prompted several
      approaches, each with distinct trade-offs in cost, performance,
      and implementation complexity. So before diving into my
      implementation, I‚Äôll examine how the research community has
      approached this domain mismatch problem and where shallow fusion
      fits among existing solutions.</p>
      <p><strong>Traditional domain adaptation</strong> typically
      requires collecting domain-specific audio paired with ground truth
      transcriptions, then fine-tuning pretrained models on this data.
      While effective, this approach faces significant barriers:
      domain-specific audio is expensive to collect, transcription
      labeling is labor-intensive, and the resulting datasets often
      remain small and brittle compared to the large scale datasets that
      the base model was trained on. This approach also runs the risk of
      catastrophic forgetting<a href="#fn1" class="footnote-ref"
      id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
      <p><strong>Context injection methods</strong> attempt to bridge
      the gap by incorporating domain-specific text directly into the
      model‚Äôs context window, essentially ‚Äúprompting‚Äù the ASR system
      with relevant terminology. However, these approaches offer no
      optimization signal and rely heavily on trial and error to achieve
      meaningful improvements. They are also architecture dependent and
      rely on the decoder‚Äôs prompting capacity.</p>
      <p><strong>Fusion techniques</strong> represent a middle ground,
      combining predictions from multiple models during inference rather
      than requiring extensive retraining. The research community has
      explored three primary variants:</p>
      <ul>
      <li><strong>Cold fusion</strong> ???</li>
      <li><strong>Deep fusion</strong> learns the combination weights
      through additional neural network layers<br />
      </li>
      <li><strong>Shallow fusion</strong> combines model predictions
      through simple weighted averaging during inference</li>
      </ul>
      <p>Shallow fusion‚Äôs appeal lies in its simplicity and flexibility,
      as it requires no additional training of the base ASR model.
      Instead, you incorporate predictions from an external language
      model directly at inference time, blending the acoustic model‚Äôs
      view of the audio with the language model‚Äôs understanding of
      domain-specific text. Importantly, the only data needed to build
      or adapt the external language model is unstructured text, which
      can be collected far more easily than transcribed audio and used
      in a self-supervised training setup.</p>
      <p>However, the approach introduces its own challenges. If the
      language model is weighted too heavily, it may bias transcriptions
      toward plausible but incorrect tokens; too lightly, and the domain
      benefits are lost. Tuning the weighting factor for the external
      model often requires domain-specific adjustment. In addition,
      shallow fusion increases inference cost since predictions must run
      through a second model<a href="#fn2" class="footnote-ref"
      id="fnref2" role="doc-noteref"><sup>2</sup></a>. These trade-offs
      make it essential to understand the method‚Äôs failure modes before
      deploying it in practice.</p>
      <h2
      id="implementation-medical-domain-fusion-pipeline">Implementation:
      Medical Domain Fusion Pipeline</h2>
      <p>Having established the landscape of existing approaches, we can
      now detail the implementation of shallow fusion for medical ASR,
      combining Whisper with a domain-adapted GPT-2 model. However,
      before going into the specifics let us first build some intuition
      on the topic by analogy.</p>
      <p>Consider for example, a person tasked with transcribing audio
      from a phone call between a customer and a claims representative
      at an insurance call center. This transcriber can hear the
      conversation clearly, but they have very little knowledge of the
      domain e.g.¬†the technical issues, procedures, and medical
      terminology that often come up. Now imagine a second person who
      has worked in this industry for years and has deep familiarity
      with the jargon and context, but who is hard of hearing.</p>
      <p>In practice, the first person might hear a phrase like
      ‚Äúmyocardial infarction‚Äù but misrecognize or misspell it. The
      domain expert, although unable to hear the audio, would
      immediately recognize the intended term and correct the
      transcript.</p>
      <!-- give an example of listener expert error -->
      <p>Shallow fusion can be thought of as a process of integrating
      each person‚Äôs expertise to offset the errors of one another and
      bridge modalities the other does not have access to. With this
      analogy we can now formally describe this process. In the example
      below think of <span class="math inline">\(P_{\text{ASR}}\)</span>
      as the person listening to the audio and <span
      class="math inline">\(P_{\text{LM}}\)</span> as the domain expert
      that is hard of hearing but deeply understands the context.</p>
      <h3 id="mathematical-formulation">Mathematical Formulation</h3>
      <p>At each decoding step for some audio input, we select the most
      probable token <span class="math inline">\(y_{t}\)</span> using
      information from the Automatic Speech Recognition model (ASR) and
      the Language Model (LM)</p>
      <p><span class="math display">\[
      y^* = \arg\max_{y_t}\;
      \left[
      \log P_{\text{ASR}}\!\bigl(y_t \mid x,\, y_{\lt t}\bigr)
      \;+\;
      \lambda\,\log P_{\text{LM}}\!\bigl(y_t \mid y_{\lt t}\bigr)
      \right]
      \]</span></p>
      <p>where:<br />
      - <span class="math inline">\(t\)</span> is the decoding step
      (0-based).<br />
      - <span class="math inline">\(y_t\)</span> is the chosen token at
      step <span class="math inline">\(t\)</span> and <span
      class="math inline">\(y_{\lt  t}\)</span> are previously generated
      tokens.<br />
      - <span class="math inline">\(x\)</span> represents the acoustic
      features (e.g.¬†raw audio input).<br />
      - <span class="math inline">\(P_{\text{ASR}}\)</span> depends on
      both <span class="math inline">\(x\)</span> and <span
      class="math inline">\(y_{\lt  t}\)</span>, while <span
      class="math inline">\(P_{\text{LM}}\)</span> depends on <span
      class="math inline">\(y_{\lt  t}\)</span> only.<br />
      - <span class="math inline">\(\lambda\)</span> is the weighting
      factor to determine the language model‚Äôs influence.</p>
      <p>The idea is that the ASR model understands phonetics and
      language in a general sense while the LM model understands the
      specialized domain in its written form, but has no access to the
      audio signal. Just like in the analogy from earlier by fusing
      their predictions, we combine phonetic understanding with domain
      expertise, aiming to improve the quality of transcriptions for
      domain specific terms. Without careful integration or synergy
      between the two, both models can carry major limitations.</p>
      <h4 id="process-diagram">Process Diagram:</h4>
      <p><img src="assets/viz.png" alt="diagram" /> Reference: <a
      href="https://arxiv.org/pdf/1712.01996">Kannan et al.¬†2017</a></p>
      <p>Consider an example where Whisper serves as our listening
      expert and GPT-2 as our domain-language expert. In practice these
      models share a tokenizer making the process of integrating their
      predictions fairly seamless at least for the english version of
      Whisper (<a href="https://arxiv.org/pdf/2212.04356">Radford
      2.2</a>). Now let‚Äôs consider a claims call center transcript where
      an ASR model misinterprets a specialized medical term.</p>
      <p><strong>Input Audio (Ground Truth):</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs Tetralogy of <code>Fallot</code>.‚Äù‚úîÔ∏è</p>
      <p><strong>Whisper Initial Output:</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs Tetralogy of <code>below</code>.‚Äùüö´</p>
      <h4 id="whisper-initial-decoding">1. <strong>Whisper Initial
      Decoding:</strong></h4>
      <p>Whisper produces logits at each step:</p>
      <ul>
      <li>Token: ‚ÄúThe‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äúprocedure‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äúclaimant‚Äù ‚Üí high confidence<br />
      </li>
      <li>Token: ‚Äú‚Äôs‚Äù ‚Üí high confidence<br />
      </li>
      <li>At the final subword, Whisper may exhibit uncertainty,
      spreading probabilities across candidates: ‚Äúbelow‚Äù, ‚Äúfollow‚Äù,
      ‚ÄúFallot‚Äù</li>
      </ul>
      <h4 id="domain-gpt-2-predictions">2. <strong>Domain GPT-2
      Predictions:</strong></h4>
      <p>At this ambiguous decoding step, GPT-2 (the domain-adapted LM)
      produces logits based on the following context:</p>
      <ul>
      <li><p>‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs <code>_____</code>‚Äù</p></li>
      <li><p>GPT-2 which has been fine tuned on medical literature
      strongly favors the correct token (produces log probabilities
      closer to 0 for Fallot) while Whisper, which had minimal access to
      medical terminology, assigns it a much lower likelihood (log
      probabilities that are more negative).</p></li>
      </ul>
      <table>
      <thead>
      <tr>
      <th>Next Token</th>
      <th>Whisper Log Probs</th>
      <th>GPT-2 Log Probs</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>Fallot</strong></td>
      <td><strong>‚Äì1.8</strong></td>
      <td><strong>‚Äì0.3</strong></td>
      </tr>
      <tr>
      <td>below</td>
      <td>‚Äì1.0</td>
      <td>‚Äì5.0</td>
      </tr>
      <tr>
      <td>follow</td>
      <td>‚Äì3.5</td>
      <td>‚Äì3.8</td>
      </tr>
      </tbody>
      </table>
      <h4 id="shallow-fusion-combining-logits">3. <strong>Shallow Fusion
      (Combining Logits):</strong></h4>
      <p>We combine each model‚Äôs logits using a weighted sum in the
      following way:</p>
      <p><span class="math display">\[
      \log P_{\text{combined}}\bigl(y_t\bigr)
        = \log P_{\text{Whisper}}\bigl(y_t \mid x,\, y_{&lt;t}\bigr)
        + \lambda\,\log P_{\text{GPT2}}\bigl(y_t \mid y_{&lt;t}\bigr)
      \]</span></p>
      <table>
      <colgroup>
      <col style="width: 24%" />
      <col style="width: 24%" />
      <col style="width: 22%" />
      <col style="width: 28%" />
      </colgroup>
      <thead>
      <tr>
      <th>Next Token</th>
      <th>Whisper Score</th>
      <th>GPT-2 Score</th>
      <th>Combined Score (Œª = 0.2)</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><strong>Fallot</strong></td>
      <td><strong>‚Äì1.8</strong></td>
      <td><strong>‚Äì0.3</strong></td>
      <td><strong>‚Äì1.8 + 0.2 <span class="math inline">\(\times\)</span>
      (‚Äì0.3) = ‚Äì1.86</strong></td>
      </tr>
      <tr>
      <td>below</td>
      <td>‚Äì1.0</td>
      <td>‚Äì5.0</td>
      <td>‚Äì1.0 + 0.2 <span class="math inline">\(\times\)</span> (‚Äì5.0)
      = ‚Äì2.0</td>
      </tr>
      <tr>
      <td>follow</td>
      <td>‚Äì3.5</td>
      <td>‚Äì3.8</td>
      <td>‚Äì3.5 + 0.2 <span class="math inline">\(\times\)</span> (‚Äì3.8)
      = ‚Äì4.26</td>
      </tr>
      </tbody>
      </table>
      <blockquote>
      <p><em>Note: The numbers here are illustrative. In practice
      additional context and scaling would favor the correct token
      ‚ÄúFallot‚Äù; additionally, rare words are likely split into multiple
      tokens but the intuition remains the same.</em></p>
      </blockquote>
      <p>‚ÄúFallot‚Äù now has the highest combined score.</p>
      <p><strong>Final Corrected Output:</strong><br />
      ‚ÄúThe procedure was medically necessary for the treatment of
      claimant‚Äôs Tetralogy of <code>Fallot</code>.‚Äù‚úîÔ∏è</p>
      <p>This demonstrates how <strong>domain-aware shallow
      fusion</strong> can significantly improve ASR output in
      specialized contexts.</p>
      <h2 id="experimental-setup">Experimental Setup</h2>
      <p><em>[This section should detail your specific methodology
      including:]</em> - <em>Model selection and preparation (Whisper +
      GPT-2)</em> - <em>Training GPT-2 on PubMed dataset while
      preserving Whisper tokenizer compatibility</em> - <em>Fusion
      pipeline architecture and implementation details</em> -
      <em>Evaluation framework and metrics (WER, domain-specific
      accuracy, etc.)</em> - <em>Dataset preparation and testing
      procedures</em></p>
      <h2 id="results-analysis">Results &amp; Analysis</h2>
      <p><em>[present findings such as:]</em> - <em>Overall WER
      performance comparison</em> - <em>Analysis of failure modes: early
      terminations, stylistic domain mismatches (punctuation,
      abbreviations like ‚Äúcentimeters ‚Üí cm‚Äù)</em> - <em>Improvements in
      medical terminology recognition</em> - <em>Discussion of gating
      mechanisms and dynamic lambda approaches</em> - <em>Impact of
      various generation parameters</em> - <em>Various decoding
      approaches</em></p>
      <h2 id="reflection-and-future-directions">Reflection and Future
      Directions</h2>
      <p><em>[section should discuss:]</em> - <em>Non-synthetic datasets
      mentioned in eval repo</em> - <em>Learnable or dynamic Œª
      approaches</em> - <em>Mixture of Experts (MoE) architectures</em>
      - <em>Cold Fusion implementations</em> - <em>Deep Fusion
      alternatives</em> - <em>Broader implications for multimodal AI
      systems</em></p>
      <h2 id="conclusion">Conclusion</h2>
      <p><em>[This section should:]</em> - <em>Articulate each model‚Äôs
      strengths and weaknesses, emphasizing how each model hits its own
      data wall separately:</em> - <em>Whisper (generalist): broadly
      trained acoustic-to-text model, struggles with specialized
      terminology</em> - <em>GPT-2 (specialist): trained in a
      self-supervised way solely on textual domain data, rich in
      domain-specific vocabulary but blind to acoustic signals</em> -
      <em>Summarize fusion-related setbacks/metrics as well as
      qualitative benefits</em> - <em>Connect this work to broader AI
      trends: Ensemble Architectures like Mixture of Experts, multimodal
      integration, domain adaptation, and evolution of fusion techniques
      (cold &amp; deep)</em></p>
      <h2 id="resources">Resources</h2>
      <ul>
      <li><a
      href="https://apxml.com/courses/speech-recognition-synthesis-asr-tts/chapter-3-language-modeling-adaptation-asr/lm-fusion-techniques">Shallow
      Fusion and Deep Fusion</a></li>
      <li><a
      href="https://research.facebook.com/file/551805355910423/Deep-Shallow-Fusion-for-RNN-T-Personalization.pdf">Deep
      Shallow Fusion for RNN-T Personalization</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/1712.01996">Analysis of
      Incorporating an External Language Model‚Ä¶</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/2212.04356">Robust Speech
      Recognition via Large-Scale Weak Supervision</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/1503.03535">On Using
      Monolingual Corpora in Neural Machine Translation</a><br />
      </li>
      <li><a
      href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language
      Models are Unsupervised Multitask Learners</a><br />
      </li>
      <li><a href="https://arxiv.org/pdf/2005.14165">Language Models are
      Few-Shot Learners</a></li>
      </ul>
      <section id="footnotes"
      class="footnotes footnotes-end-of-document" role="doc-endnotes">
      <hr />
      <ol>
      <li id="fn1"><p><a
      href="https://en.wikipedia.org/wiki/Catastrophic_interference">Catastrophic
      forgetting</a> occurs when a neural network loses previously
      learned information upon learning new tasks or data.<a
      href="#fnref1" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn2"><p>Several variations exist to reduce the inference
      cost of shallow fusion, including N-best rescoring (applying the
      LM only to candidate transcripts), using smaller or distilled
      domain LMs etc.<a href="#fnref2" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      </ol>
      </section>
    </article>
  </main>

  <!-- article JS (dark mode only) -->
  <script defer src="../../assets/js/article.js"></script>
</body>
</html>