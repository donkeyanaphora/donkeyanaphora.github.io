<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <!-- mobile layout -->
  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <title></title>

  <!-- site look & feel -->
  <link rel="stylesheet" href="../../assets/css/main.css"/>

  <!-- MathJax -->
  <script defer id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body class="article">
  <!-- dark-mode toggle -->
  <button id="toggleDark" class="float-btn" aria-label="Toggle dark mode">üåô</button>

  <!-- Markdown content -->
  <main class="content">
    <article class="markdown-body">
      <h1 id="using-linear-models-in-modern-applications">Using Linear
      Models in Modern Applications</h1>
      <p><strong>COLLINS WESTNEDGE</strong><br />
      <em>MAY 16, 2025</em></p>
      <h2 id="introduction">Introduction</h2>
      <p>I‚Äôve been really busy so this will be a little informal but
      <strong>STILL WORTH THE READ</strong> because in the day to day
      landscape of people talking agentic AI, RAG systems, vector
      databases etc things like least squares regression and linear
      models tend to feel like dusty sklearn imports or the stuff of
      technical coding interviews. Despite the heavy focus on foundation
      model integration classic linear tools like ordinary least squares
      regression and its projection formula corrolary remain
      surprisingly powerful. By framing modern tasks like semantic
      filtering and bias removal as geometric problems, it becomes clear
      why simple formulas like <span class="math display">\[
      X(X^{\mathsf T}X)^{+}X^{\mathsf T}
      \]</span> work so well. The focus of this article is show how
      ordinary least squares offers insights as well as stable and
      theoretically grounded solution to semantic filtering in modern
      search and retrieval systems.</p>
      <h2 id="linalg-recap">Linalg Recap</h2>
      <p><em>lorem ipsum</em></p>
      <h3 id="key-formulas">Key Formulas</h3>
      <table>
      <colgroup>
      <col style="width: 22%" />
      <col style="width: 56%" />
      <col style="width: 21%" />
      </colgroup>
      <thead>
      <tr>
      <th>Formula</th>
      <th>Intuitive Role</th>
      <th>Visual Cue</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><span class="math inline">\(X^{+} =
      (X^{\!\top}X)^{-1}X^{\!\top}\)</span></td>
      <td><strong>Recipe for weights.</strong> Solves <span
      class="math inline">\(\min_\beta\|y - X\beta\|_2^2\)</span>.</td>
      <td>‚ÄúHow much of each arrow do I need?‚Äù</td>
      </tr>
      <tr>
      <td><span class="math inline">\(\hat{\beta}=X^{+}y\)</span></td>
      <td><strong>Coordinates along the arrows.</strong> Scalar steps to
      reach the closest point.</td>
      <td>Labels on the subspace axes.</td>
      </tr>
      <tr>
      <td><span class="math inline">\(P_X = X\,X^{+}\)</span></td>
      <td><strong>Snap-to-span operator.</strong> Orthogonal projector:
      symmetric, idempotent.</td>
      <td>Elevator lowering you to the sheet.</td>
      </tr>
      <tr>
      <td><span class="math inline">\(\hat y = P_X\,y =
      X\hat{\beta}\)</span></td>
      <td><strong>Foot of the perpendicular.</strong> Unique point in
      <span class="math inline">\(\mathrm{span}\{X\}\)</span> minimizing
      <span class="math inline">\(\|y - \hat y\|\)</span>.</td>
      <td>Black dot where the plumb-line hits.</td>
      </tr>
      <tr>
      <td><span class="math inline">\(r = y-\hat y =
      (I-P_X)\,y\)</span></td>
      <td><strong>Residual vector.</strong> Always orthogonal to every
      column of <span class="math inline">\(X\)</span> (<span
      class="math inline">\(X^{\!\top}r=0\)</span>).</td>
      <td>Red straight-down arrow.</td>
      </tr>
      </tbody>
      </table>
      <h3 id="theorems">Theorems</h3>
      <p><strong>Best Approximation Theorem</strong> Let <span
      class="math inline">\(W\)</span> be a subspace of <span
      class="math inline">\(\mathbb{R}^n\)</span>, <span
      class="math inline">\(y\in\mathbb{R}^n\)</span>, and <span
      class="math inline">\(\widehat y = \mathrm{proj}_W(y)\)</span> its
      orthogonal projection onto <span class="math inline">\(W\)</span>.
      Then</p>
      <p><span class="math display">\[
      \|y - \widehat{y}\|_2 \;&lt;\; \|y - v\|_2
      \quad\text{for all }v\in W,\;v\neq \widehat{y}.
      \]</span></p>
      <blockquote>
      <p><em>From this it follows that the ‚Äúremainder‚Äù <span
      class="math inline">\(y - \widehat{y}\)</span> lies in <span
      class="math inline">\(W^\perp\)</span> and is the closest point in
      that orthogonal complement to <span
      class="math inline">\(y\)</span>.</em></p>
      </blockquote>
      <h3 id="takeaway-corollary">Takeaway Corollary</h3>
      <p>If <span class="math inline">\(\mathbf{u}_{\text{proj}} =
      \mathrm{proj}_V(\mathbf{u})\)</span>, then</p>
      <p><span class="math display">\[
      \mathbf{u}_{\mathrm{new}}
      \;=\;\mathbf{u} - \mathbf{u}_{\text{proj}}
      \;\in\;V^\perp
      \]</span></p>
      <p>is the closest-possible embedding to <span
      class="math inline">\(\mathbf{u}\)</span> that‚Äôs perfectly
      orthogonal to <span class="math inline">\(V\)</span>.</p>
      <h4 id="tldr">TLDR</h4>
      <p>Define</p>
      <p><span class="math display">\[
      \mathbf{u}_{\mathrm{new}}
      \;=\;\arg\min_{w\in V^\perp}\|\mathbf{u}-w\|
      \;=\;\mathrm{proj}_{V^\perp}(\mathbf{u}).
      \]</span></p>
      <ol type="1">
      <li><strong>Minimal distortion</strong> <span
      class="math inline">\(\mathbf{u}_{\mathrm{new}}\in
      V^\perp\)</span> is the closest vector in <span
      class="math inline">\(V^\perp\)</span> to <span
      class="math inline">\(\mathbf{u}\)</span>.</li>
      <li><strong>Guaranteed orthogonality</strong> <span
      class="math inline">\(\langle \mathbf{u}_{\mathrm{new}},\,v\rangle
      = 0\)</span> for every <span class="math inline">\(v\in
      V\)</span>.</li>
      </ol>
      <h2 id="visual-intuition">Visual Intuition</h2>
      <h2 id="mean-subtraction-vs.-projection-residuals">Mean
      Subtraction vs.¬†Projection Residuals</h2>
      <h2 id="examples">Examples</h2>
      <h2 id="conclusion">Conclusion</h2>
    </article>
  </main>

  <!-- site JS -->
  <script defer src="../../assets/js/site.js"></script>
</body>
</html>
