## Research Notes on RLHF Diversity

### Premises
1. RHLF reduces diversity of model generations, effectively squishing probability masses for next token prediction
2. RHLF improves generalization and robustness on many benchmark tasks
3. RHLF may squash probability mass even within the space of valid solutions
4. Fluid intelligence is defined by efficient generalization
5. A well-generalizing model should exhibit no preference among **equally** valid solutions while strongly suppressing invalid ones.
     - *Equivalently: Optimal generalization implies uniform probability distribution over the set of valid solutions and near-zero probability for invalid solutions*

### Questions
1. Solution Space Constraint: For verifiable tasks that admit many correct answers, does RHLF reduce entropy across valid solutions? Do models converge on fewer, more conventional answers, if so for which task types?
2. Creativity Correlation: Does reduced entropy within valid solution spaces correlate with diminished creative reasoning?

### Illustrative Examples
Pre-RHLF behavior (Base Model):
- What primes sum to 20,000 -> broad probability spread over many valid primes
- What is 2+2? -> narrow probability peak at 4 or generations leading to 4
- Word ladder from "COLD" to "WARM" in 5 steps -> Wide distribution over distinct valid ladders.
- Seat 12 guests (A–L) at two tables of 6 with constraints: 
     - A and B not together
     - C&D together
     - each table ≥2 from {E,F,G,H}) 
     
     -> Wide distribution over distinct valid seatings

Post-RHLF behavior (Aligned Model):
- what primes sum to 20,000 -> Probability sharply concentrated on one canonical primes/solutions.
- what is 2+2? -> narrow probability peak at “4” (unchanged).
- Word ladder from "COLD" to "WARM" in 5 steps -> Probability mass collapses onto the canonical path (COLD->CORD->WORD->WORM->WARM)
- Seat 12 guests (A–L) at two tables of 6 with same constraints -> Probability mass collapses onto a few canonical partitions; sharply reduced variety.

### Relationship to ARC: 
Following Francois Chollet's work on ARC and fluid intelligence is marked by efficient skill acquisition, or generalization from exposure to few examples. Efficient generalization requires uniform probability mass (equally likely) across all valid solutions, given minimal exposure. 
- If a model is shown a single instance of "find primes that sum to 20,000" its generalization power should be marked by a uniform probability mass across all valid primes that produce this sum
- Outside the valid solution space, the model's probability mass should be near zero. 

Thus, a hallmark of fluid intelligence is selective entropy: wide distribution across correct solutions, narrow distribution against incorrect ones. RLHF may interfere with this balance by collapsing entropy within the valid space.

### H1 (In progress)
For tasks that admit many verifiable correct answers, the distribution of *first-choice* valid solutions generated by a base checkpoint is broader than that of its RLHF-aligned sibling.

### Possible abstract

Investigate whether preference alignment (RLHF and related methods) collapses diversity within the set of valid solutions for tasks that admit many verifiable correct answers. Define and introduce metric (Selective Entropy Index) that rewards high entropy over valid solutions while penalizing probability mass assigned to invalid ones. Across task families with enumerable valid sets (Goldbach partitions, word ladders, seating under constraints, constrained anagrams, N-Queens placements, etc), compare base LLMs against their aligned siblings under matched decoding. Estimate the valid-space distributions via sampling, and test paired differences in SEI. Further examine correlations between SEI and creativity metrics on semi-verifiable tasks. Preliminary work suggests a generalization–diversity trade off after RLHF; the goal is to sharpen this by isolating diversity collapse within valid solution set for tasks that admit many verifiable correct answers. Findings inform diversity-preserving alignment objectives and evaluations that complement ARC-style measures of fluid intelligence.


<!-- Across task families with enumerable valid sets (Goldbach partitions, shortest-path enumeration, DAG topological orders, N-Queens, constrained anagrams, and integer partitions) -->
