<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <!-- mobile layout -->
  <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <title></title>

  <!-- site look & feel -->
  <link rel="stylesheet" href="../../assets/css/main.css"/>
  <link rel="stylesheet" href="../../assets/css/article.css"/>

    <!-- ‚îÄ‚îÄ MathJax config: no indent + fix \text{} baseline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ -->
  <script>
    window.MathJax = {
      /* remove default ‚Äì0.25 em display indent and keep left-aligned   */
      tex:   { displayIndent: '0em',  displayAlign: 'left' },

      /* inherit the surrounding page font for \text{‚Ä¶} so the baseline
         matches math italic on hi-DPI screens (stops 1-px jog)        */
      /* chtml: { mtextInheritFont: true } */
         chtml: { mtextFont: 'Times' }
    };
  </script>
  
  <!-- MathJax -->
  <script defer id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js"></script>
</head>

<body class="article">
  <!-- dark-mode toggle -->
  <nav class="article-nav">
    <a href="../../" class="back-link">‚Üê Home</a>
  </nav>
  <button id="toggleDark" class="float-btn" aria-label="Toggle dark mode">üåô</button>

  <!-- Markdown content -->
  <main class="content">
    <article class="markdown-body">
      <h1 id="using-linear-models-in-modern-applications">Using Linear
      Models in Modern Applications</h1>
      <p><strong>COLLINS WESTNEDGE</strong><br />
      <em>MAY 16, 2025</em></p>
      <h2 id="introduction">Introduction</h2>
      <p>I‚Äôve been really busy so this will be a little informal but
      <strong>STILL WORTH THE READ</strong> because in the day to day
      landscape of people talking agentic AI, RAG systems, vector
      databases etc things like least squares regression and linear
      models tend to feel like dusty sklearn imports or the stuff of
      technical coding interviews. Despite the heavy focus on foundation
      model integration classic linear tools like ordinary least squares
      regression and its projection formula corrolary remain
      surprisingly powerful. By framing modern tasks like semantic
      filtering and bias removal as geometric problems, it becomes clear
      why simple formulas like <span class="math display">\[
      X(X^{\mathsf T}X)^{+}X^{\mathsf T}
      \]</span> work so well. The focus of this article is show how
      ordinary least squares offers insights as well as stable and
      theoretically grounded solution to semantic filtering in modern
      search and retrieval systems.</p>
      <h2 id="linalg-recap">Linalg Recap</h2>
      <p><em>lorem ipsum</em></p>
      <h3 id="key-formulas">Key Formulas</h3>
      <table>
      <colgroup>
      <col style="width: 33%" />
      <col style="width: 33%" />
      <col style="width: 33%" />
      </colgroup>
      <thead>
      <tr>
      <th>Formula</th>
      <th>Intuitive role</th>
      <th>Visual cue</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><span class="math inline">\(X^{+} =
      (X^{\!\top}X)^{-1}X^{\!\top}\)</span></td>
      <td>Recipe for weights. Solves <span
      class="math inline">\(\displaystyle\min_{\beta}\,\lVert y -
      X\beta\rVert_2^{\,2}\)</span>.</td>
      <td>How much of each arrow do I need?</td>
      </tr>
      <tr>
      <td><span class="math inline">\(\hat{\beta}=X^{+}y\)</span></td>
      <td>Coordinates along the arrows (scalar steps to reach the
      closest point).</td>
      <td>Labels on the sub-space axes.</td>
      </tr>
      <tr>
      <td><span class="math inline">\(P_X = XX^{+}\)</span></td>
      <td>Snap-to-span operator (orthogonal projector: symmetric,
      idempotent).</td>
      <td>Elevator lowering you to the sheet.</td>
      </tr>
      <tr>
      <td><span class="math inline">\(\hat{y}=P_Xy =
      X\hat{\beta}\)</span></td>
      <td>Foot of the perpendicular ‚Äî the unique point in <span
      class="math inline">\(\operatorname{span}(X)\)</span> minimizing
      <span class="math inline">\(\lVert
      y-\hat{y}\rVert_2\)</span>.</td>
      <td>Black dot where the plumb-line hits.</td>
      </tr>
      <tr>
      <td><span class="math inline">\(r = y-\hat{y} =
      (I-P_X)\,y\)</span></td>
      <td>Residual, orthogonal to every column of <span
      class="math inline">\(X\)</span> (<span
      class="math inline">\(X^{\!\top}r=0\)</span>).</td>
      <td>Red straight-down arrow.</td>
      </tr>
      </tbody>
      </table>
      <h3 id="theorems">Theorems</h3>
      <p><strong>Best Approximation Theorem</strong></p>
      <p>Let <span
      class="math inline">\(W\subseteq\mathbb{R}^{n}\)</span>, <span
      class="math inline">\(y\in\mathbb{R}^{n}\)</span>, and <span
      class="math inline">\(\widehat{y}=\operatorname{proj}_{W}(y)\)</span>.
      Then <span class="math display">\[
      \lVert y-\widehat{y}\rVert_2 \;&lt;\; \lVert y-v\rVert_2,
      \qquad\forall\,v\in W,\;v\neq\widehat{y}.
      \]</span> Hence <span class="math inline">\(y-\widehat{y}\in
      W^{\perp}\)</span> and is the closest vector in <span
      class="math inline">\(W^{\perp}\)</span> to <span
      class="math inline">\(y\)</span>.</p>
      <h3 id="take-away-corollary">Take-away Corollary</h3>
      <p>If <span
      class="math inline">\(\mathbf{u}_{\mathrm{proj}}=\operatorname{proj}_{V}(\mathbf{u})\)</span>,
      then <span class="math display">\[
      \mathbf{u}_{\mathrm{new}}
      =\mathbf{u}-\mathbf{u}_{\mathrm{proj}}
      \in V^{\perp}.
      \]</span> Thus <span
      class="math inline">\(\mathbf{u}_{\mathrm{new}}\)</span> is the
      closest-possible embedding to <span
      class="math inline">\(\mathbf{u}\)</span> that is perfectly
      orthogonal to <span class="math inline">\(V\)</span>.</p>
      <h4 id="tldr">TL;DR</h4>
      <p>Define <span class="math display">\[
      \mathbf{u}_{\mathrm{new}}
      =\arg\min_{w\in V^{\perp}}\lVert\mathbf{u}-w\rVert
      =\operatorname{proj}_{V^{\perp}}(\mathbf{u}).
      \]</span></p>
      <ol type="1">
      <li><strong>Minimal distortion</strong> ‚Äî <span
      class="math inline">\(\mathbf{u}_{\mathrm{new}}\)</span> is the
      closest vector in <span class="math inline">\(V^{\perp}\)</span>
      to <span class="math inline">\(\mathbf{u}\)</span>.<br />
      </li>
      <li><strong>Guaranteed orthogonality</strong> ‚Äî <span
      class="math inline">\(\langle\mathbf{u}_{\mathrm{new}}, v\rangle =
      0\)</span> for every <span class="math inline">\(v\in
      V\)</span>.</li>
      </ol>
      <h2 id="visual-intuition">Visual Intuition</h2>
      <h2 id="mean-subtraction-vs.-projection-residuals">Mean
      Subtraction vs.¬†Projection Residuals</h2>
      <h2 id="examples">Examples</h2>
      <h2 id="conclusion">Conclusion</h2>
    </article>
  </main>

  <!-- site JS -->
  <script defer src="../../assets/js/site.js"></script>
</body>
</html>
