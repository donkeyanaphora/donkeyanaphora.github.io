<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Extra-Ordinary Language - Collins Westnedge</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no, viewport-fit=cover">
  
  <!-- Mobile app settings -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- SEO essentials -->
  <meta name="google-site-verification" content="hmW81pKLkPLJXY0kFe-I1vNXno-xD9XbEWyZyGfz-SA" />
  <meta name="description" content="An exploration of literary and
creative intelligence and things I wish language models did better.">
  <meta name="author" content="Collins Westnedge">
  <link rel="canonical" href="https://donkeyanaphora.github.io/articles/article3/">
  
  <!-- Hide drafts from search engines -->
  

  <!-- Site look & feel -->
  <link rel="stylesheet" href="../../assets/css/main.css"/>
  <link rel="stylesheet" href="../../assets/css/article.css"/>
  
  <!-- Favicons -->
  <link rel="apple-touch-icon" sizes="180x180" href="../../favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../../favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../../favicon/favicon-16x16.png">
  <link rel="manifest" href="../../favicon/site.webmanifest">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://donkeyanaphora.github.io/articles/article3/">
  <meta property="og:title" content="Extra-Ordinary Language">
  <meta property="og:description" content="An exploration of literary
and creative intelligence and things I wish language models did
better.">
  <meta property="og:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">
  <meta property="og:site_name" content="The Latent Realm">
  <meta property="article:author" content="Collins Westnedge">
  <meta property="article:published_time" content="2026-01-11">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:url" content="https://donkeyanaphora.github.io/articles/article3/">
  <meta name="twitter:title" content="Extra-Ordinary Language">
  <meta name="twitter:description" content="An exploration of literary
and creative intelligence and things I wish language models did
better.">
  <meta name="twitter:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">

  <!-- MathJax config -->
  <!-- <script>
    window.MathJax = {
      tex:   { displayIndent: '0em',  displayAlign: 'center' },
      chtml: { mtextFont: 'Times' }
    };
  </script> -->
  
  <script>
    window.MathJax = {
      tex: {},  // Empty is fine
      chtml: { 
        mtextFont: 'Times',
        displayAlign: 'center',    // ‚úÖ Moved here
        displayIndent: '0em'       // ‚úÖ Moved here
      }
    };
  </script>
  <!-- MathJax -->
  <script defer id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js">
  </script>
</head>

<body class="article">
  <!-- floating buttons -->
  <button id="menuBtn" class="float-btn" aria-label="Open menu">üß∞</button>
  <button class="float-btn back-btn" onclick="window.location.href='../../'" aria-label="Back to home">üè°</button>
  <button id="toggleDark" class="float-btn" aria-label="Toggle dark mode">üåô</button>

  <!-- Markdown content -->
  <main class="content">
    <article class="markdown-body">
      <h1 id="extra-ordinary-language">üé≠ Extra-Ordinary Language</h1>
      <p><strong>COLLINS WESTNEDGE</strong><br />
      <em>JAN 11, 2026</em></p>
      <h2 id="introduction">Introduction</h2>
      <p>I‚Äôve been thinking about a conversation I had with a former
      colleague about language that defies ordinary usage. More
      specifically, expressions that push back against the core tenet of
      <a
      href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional
      hypothesis</a> and yet carry high degrees of intentionality and
      depth.</p>
      <p>When Kafka wrote ‚ÄúA cage went in search of a bird,‚Äù he created
      something that on the surface seems impossible and yet expresses a
      profound and insidious truth. Current AI systems, for all their
      linguistic sophistication, rarely produce such language
      spontaneously. They excel at coherent, informative prose but
      struggle with the kind of intentional violations that define great
      literature.</p>
      <p>In this post, I‚Äôm dog-earing these thoughts to revisit later.
      The aim here is to understand what makes these expressions work
      and more critically, the mechanisms by which models may surface or
      suppress them.</p>
      <h2 id="literary-examples">Literary Examples</h2>
      <blockquote>
      <p>So they lov‚Äôd, as love in twain<br />
      Had the essence but in one;<br />
      Two distincts, division none:<br />
      Number there in love was slain.</p>
      <p>Hearts remote, yet not asunder;<br />
      Distance and no space was seen<br />
      Twixt this Turtle and his queen</p>
      <p>‚Äî <em>Shakespeare, <strong>The Phoenix and the
      Turtle</strong></em></p>
      </blockquote>
      <blockquote>
      <p>A cage went in search of a bird.</p>
      <p>‚Äî <em>Kafka, <strong>Aphorisms</strong></em></p>
      </blockquote>
      <blockquote>
      <p>You must go on. I can‚Äôt go on. I‚Äôll go on.</p>
      <p>‚Äî <em>Beckett, <strong>The Unnamable</strong></em></p>
      </blockquote>
      <blockquote>
      <p>Merry and tragical! Tedious and brief!<br />
      That is, hot ice and wondrous strange snow.<br />
      How shall we find the concord of this discord?</p>
      <p>‚Äî <em>Shakespeare, <strong>A Midsummer Night‚Äôs
      Dream</strong></em></p>
      </blockquote>
      <h3 id="core-characteristics">Core Characteristics</h3>
      <p>Because of their intentionality and depth, I‚Äôm going to call
      these violations of ‚Äúordinary use‚Äù <strong>extra-ordinary
      use</strong>, or <strong>ExO language</strong>. They don‚Äôt obscure
      meaning; instead, they elucidate by way of contradiction or
      violated expectation. These literary examples share a unifying
      feature: they present a <strong>literal semantic
      breakdown</strong> in one domain that creates insightful or
      profound resonance in another (metaphorical, allegorical, or
      abstract).</p>
      <p>As David Foster Wallace said, ‚Äúwe all know there‚Äôs no quicker
      way to empty a joke of its peculiar magic than to explain it,‚Äù and
      the same is true of figurative language. Still, it‚Äôs worth
      illustrating how these expressions <em>may</em> resolve through a
      figurative reinterpretation, albeit with some loss of ‚Äúmagic‚Äù:</p>
      <ul>
      <li>‚ÄúTwo distincts, division none‚Äù - two bodies, one soul</li>
      <li>‚ÄúA cage went in search of a bird‚Äù - oppression seeks
      freedom</li>
      <li>‚ÄúYou must go on. I can‚Äôt go on. I‚Äôll go on.‚Äù - I don‚Äôt choose
      to continue; it‚Äôs compulsory</li>
      </ul>
      <p>In each case, the reader encounters a jarring violation of
      expectation that is only resolved through a figurative
      reinterpretation. One way to capture that pattern is:</p>
      <p><span class="math display">\[
      \text{‚ü¶œÜ‚üß}_{\text{literal}} = \text{‚äò} \quad ; \quad
      \text{‚ü¶œÜ‚üß}_{\text{figurative}} = \psi
      \]</span></p>
      <p>Here, ‚ü¶œÜ‚üß denotes the interpretation of expression œÜ, ‚äò
      indicates a semantic anomaly (a violation of expectation), and œà
      the emergent or resolved meaning. Framed this way, two of ExO
      language‚Äôs core properties are built directly into the
      notation:</p>
      <ul>
      <li><strong>Surface-level anomaly</strong>: under a literal
      reading, expectations are violated, i.e., ( <span
      class="math inline">\(\text{‚ü¶œÜ‚üß}_{\text{lit}}=\text{‚äò}\)</span> )
      (a cage can‚Äôt search, ice can‚Äôt be hot, distincts can‚Äôt lack
      division etc).</li>
      <li><strong>Recoverability</strong>: despite that violation or
      collapse, a coherent reading exists, i.e., there is some (<span
      class="math inline">\(\psi\)</span>) such that (<span
      class="math inline">\(\text{‚ü¶œÜ‚üß}_{\text{fig}}=\psi\)</span>).</li>
      </ul>
      <p>However, in practice, strong ExO lines also tend to carry:</p>
      <ul>
      <li><strong>Intent signal</strong>: the violation feels deliberate
      and motivated within the larger context.</li>
      <li><strong>Irreducibility</strong>: paraphrase diminishes the
      associative effects (the deviation is doing semantic work that a
      literal restatement can‚Äôt).</li>
      </ul>
      <p>Ultimately, these examples illustrate how expressions can
      fracture under a literal reading yet resolve in an imaginative
      one. The purpose of the notation is not to serve as a definition,
      but to make that fracture/violation explicit thus clarifying how
      base model objectives rooted in next token prediction or
      post-training strategies optimized on user preferences
      (helpfulness, coherence, etc.) could steer models away from ExO
      language. If we want systems that embrace deliberate, meaningful
      rule-bending, we‚Äôll need objectives, benchmarks, or sampling
      techniques that more actively encourage it.</p>
      <h2 id="why-might-current-ai-struggle-here">Why Might Current AI
      Struggle Here?</h2>
      <p>Current language models face several systematic barriers to
      producing ExO language; at this point many of these are my own
      thoughts or fan theory rather than concrete fact, but nevertheless
      here they are:</p>
      <h3 id="base-training">Base Training</h3>
      <p>Though profound literature and instances of ExO exist in
      pretraining corpora (Project Gutenberg, Books1-3, etc), against
      which many base models like GPT and Llama have been tuned, it‚Äôs
      statistically underrepresented. By definition, semantic violations
      are rare. Even within Pulitzer Prize winning articles, books, etc
      the instances of profound subversions of meaning and use (as
      impactful as they may be) are few and far between. That being
      said, even if a model encountered notable instances of ExO
      language they are by definition violations of <em>ordinary
      usage</em>, the very pattern the model is optimizing
      <em>towards</em>. Given the prefix ‚ÄúThat is, hot‚Äù the continuation
      ‚Äù ice‚Äù is unlikely, lying on the tail end of the distribution for
      the proposed words to come. From a decoding perspective sequences
      like ‚Äúhot ice‚Äù are absolutely possible, but not probable, so then
      what sort of sampling strategy could surface them?</p>
      <p>At this point the easiest way to turn the improbable into
      probable (as in the case of ‚Äúhot ice‚Äù or ‚ÄúI can‚Äôt go on, I‚Äôll go
      on‚Äù) is to condition the model on the exact instances in which
      they have <em>already</em> occurred. Insofar as we care about
      seeing ExO on the page our job here is done, but if we care about
      the generation of <em>new</em> forms of ExO, new intentional rule
      breaking, e.g., the meaningful violations that extend beyond
      imitation, we have made 0 progress.</p>
      <p>ExO sequences are in fact hiding somewhere in the set of
      possible generations, but the question remains what objectives or
      sampling techniques could possibly surface them? This might be as
      good a time as any to question the very foundation, the model
      architecture, and at the very least consider whether next token
      prediction is even the right framework for ExO language. For
      example, does it make sense to say that we cogitate left to right
      one word at a time, and then make some sort of value judgement
      accept/reject on the final form these sequential trajectories? Or
      are there alternative strategies that may better align with our
      own ideation process?<a href="#fn1" class="footnote-ref"
      id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
      <h3 id="post-training">Post-training</h3>
      <p>From a post-training perspective there are likely a few
      culprits or rather one culprit that hurts in different ways,
      preventing the emergence of ExO. The first of which is as simple
      as task absence, e.g., the under-representation of ExO
      demonstrations in the instruction/demonstration datasets upon
      which the base models we discussed earlier are further optimized.
      Honestly, what sort of standalone questions would even elicit a
      demonstration of intentional semantic rule breaking ‚Äúplease
      reshape my perspective of X?‚Äù</p>
      <p>My gut tells me that ExO emerges from a very different kind of
      language game, one that is culturally and historically situated<a
      href="#fn2" class="footnote-ref" id="fnref2"
      role="doc-noteref"><sup>2</sup></a> and doesn‚Äôt cleanly fit into
      the box of <em>instruction following</em> (a game whose economic
      utility maps more directly to an ROI). To further this point, I
      would argue that Kafka (or any writer, poet, activist) for that
      matter was not following an instruction. The aim was to subvert
      meaning, to reshape the linguistic pathways through which we talk
      and <em>think</em> about the world.<a href="#fn3"
      class="footnote-ref" id="fnref3"
      role="doc-noteref"><sup>3</sup></a> Though you could argue that
      someone probably instructed Kafka to ‚Äúwrite more of XYZ‚Äù or that
      speech writers are instructed to put some idea into the form of
      compelling rhetoric, but even so it‚Äôs very hard to imagine these
      instructions stripped of the socio-political landscape in which
      they are motivated and dropped into some instruction/demonstration
      dataset. Just to be clear this isn‚Äôt to say there‚Äôs some
      irreducibly human or non-optimizable dimension of ExO, as I
      mentioned before these are just my ramblings.</p>
      <p>Before moving onto the next topic in post-training it‚Äôs worth
      reiterating our point from earlier about ExO expressions like ‚Äúhot
      ice‚Äù surfacing as our prior gets closer to the original passage in
      which it occurs:</p>
      <table>
      <thead>
      <tr>
      <th>Context</th>
      <th>P(‚Äúice‚Äù)</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td>‚ÄúThat is, hot ___‚Äù</td>
      <td>‚ñ™‚óã‚óã‚óã‚óã</td>
      </tr>
      <tr>
      <td>‚ÄúTedious and brief! That is, hot ___‚Äù</td>
      <td>‚ñ™‚ñ™‚ñ™‚óã‚óã</td>
      </tr>
      <tr>
      <td>[Full Shakespeare]‚Ä¶ ‚Äúhot ___‚Äù</td>
      <td>‚ñ™‚ñ™‚ñ™‚ñ™‚ñ™</td>
      </tr>
      </tbody>
      </table>
      <p><br></p>
      <!-- The model is only ever predicting a single word ahead, but as that prior increases and more closely resembles the excerpt in which our target word occurred the probability increases. This is why I mentioned sampling techniques earlier because they offer an alternative to building up that prior, instead focusing on strategies to leverage the information we have about the distribution to pick words that may not be the most likely. -->
      <p>From an optimization standpoint, the model is only ever
      rewarded/penalized for predicting the next token. As the
      conditioning context grows closer to the original passage in which
      a target word occurred, the probability mass assigned to that word
      increases. This is why I mentioned sampling techniques earlier
      because they offer an alternative to building up that prior,
      instead focusing on strategies to leverage the information we have
      about the distribution to pick words that may not be the most
      likely.<a href="#fn4" class="footnote-ref" id="fnref4"
      role="doc-noteref"><sup>4</sup></a></p>
      <p>However, the question remains how would we sample for ExO, and
      what recipe could we concoct (based on token probability masses)
      to reliably produce instances of it. Overall, these strategies
      can‚Äôt encode ‚Äútake a low probability path here, because it
      <em>will</em> conceptually or rhetorically pay off later‚Äù because
      as the table illustrates there is no notion of later. But what if
      we could learn one?</p>
      <p>This brings us to RLHF (and preference optimization more
      broadly), a strategy by which we aren‚Äôt just conditioning on some
      previously seen ExO prior, but incorporating information about
      where individual token choices <em>may</em> take us. It‚Äôs
      important to disclose that the shift is not that the model stops
      being a next-token predictor, it‚Äôs that we train it using feedback
      that is defined over the entire sequence.</p>
      <p>The reward model has seen complete sequences and encodes
      information about how these trajectories resolve. But the base
      model still has to learn which token-level decisions lead to this
      reward. The effect feels like pruning a tree, until the
      probability mass for a given token is concentrated around tokens
      that take us in directions humans tend to prefer. <!-- 
      This seems like an interesting way to overcome the hurdles we discussed earlier, however, there remains one major issue: we are still playing the wrong language game. As interesting as optimization toward something as ineffable as human preference may be, it's still rooted in a context that doesn't quite fit ExO, primarily because it's the context in which preferences map to coherence, rule following, legibility, helpfulness, etc.  --></p>
      <p>This seems like an interesting way to overcome the hurdles we
      discussed earlier, however, a deeper problem remains: aren‚Äôt we
      still playing the wrong language game? As interesting as
      optimization toward something as ineffable as human preference may
      be, it‚Äôs still rooted in a context that doesn‚Äôt quite fit ExO, a
      context in which preferences likely map to coherence,
      rule-following, legibility, helpfulness, etc.</p>
      <h2 id="closing-thoughts">Closing Thoughts</h2>
      <p>Just because models can exhibit surprisal or violate semantic
      expectations (under certain constraints) doesn‚Äôt mean they possess
      the ability to do so meaningfully. Ultimately, the goal is to
      understand whether machines can develop the kind of novel,
      conceptual re-framings that define ExO language and to build
      evaluation frameworks that recognize this kind of rhetorical force
      when it emerges. In short, we need benchmarks and methodologies
      that do not suppress, but instead reward ‚Äúwondrous strange
      snow.‚Äù<a href="#fn5" class="footnote-ref" id="fnref5"
      role="doc-noteref"><sup>5</sup></a></p>
      <h2 id="key-resources">Key Resources</h2>
      <p><strong>Philosophy of Language &amp; Logic</strong></p>
      <ul>
      <li>‚≠ê <a
      href="https://sites.pitt.edu/~rbrandom/Courses/Antirepresentationalism%20(2020)/Texts/rorty-contingency-irony-and-solidarity-1989.pdf">Richard
      Rorty, <em>Contingency, Irony, and Solidarity</em> (1989)</a> ‚Äî
      PDF</li>
      <li><a
      href="https://hal.science/hal-02381339/file/Abrusan.SemanticAnomaly.Ungrammaticality.pdf">Marta
      Abrus√°n, <em>Semantic Anomaly, Pragmatic Infelicity and
      Ungrammaticality</em> (2019)</a> ‚Äî PDF</li>
      <li><a
      href="https://plato.stanford.edu/entries/metaphor/">Metaphor</a> -
      <em>Stanford Encyclopedia of Philosophy</em></li>
      <li><a
      href="https://plato.stanford.edu/entries/contradiction/">Contradiction</a>
      - <em>Stanford Encyclopedia of Philosophy</em></li>
      <li><a
      href="https://en.wikipedia.org/wiki/Catachresis">Catachresis</a> -
      Wikipedia</li>
      <li><a href="https://en.wikipedia.org/wiki/Chiasmus">Chiasmus</a>
      - Wikipedia</li>
      <li><a
      href="https://en.wikipedia.org/wiki/Category_mistake">Category
      Mistake</a> - Wikipedia</li>
      <li><a
      href="https://en.wikiversity.org/wiki/Information">Exformation</a>
      - Wikiversity</li>
      </ul>
      <p><strong>RLHF &amp; Output Diversity</strong></p>
      <ul>
      <li>‚≠êÔ∏è <a
      href="https://openai.com/index/instruction-following/">Aligning
      language models to follow instructions</a> - <em>OpenAI</em></li>
      <li>‚≠êÔ∏è <a
      href="https://openreview.net/pdf?id=PXD3FAVHJT">Understanding the
      Effects of RLHF on LLM Generalisation and Diversity</a> - <em>ICLR
      / OpenReview</em></li>
      <li>‚≠êÔ∏è <a
      href="https://aclanthology.org/2025.naacl-long.561.pdf">One fish,
      two fish, but not the whole sea: Alignment reduces language
      models‚Äô conceptual diversity</a> - <em>NAACL 2025</em></li>
      </ul>
      <p><strong>Fluid/Intelligence Reasoning</strong></p>
      <ul>
      <li>‚≠êÔ∏è <a href="https://arxiv.org/abs/1911.01547">On the Measure
      of Intelligence</a> - <em>arXiv</em> (Chollet, 2019)</li>
      <li><a href="https://gwern.net/creative-benchmark">Towards
      Benchmarking LLM Diversity &amp; Creativity</a> -
      <em>Gwern.net</em></li>
      </ul>
      <p><strong>Sampling, Diversity &amp; Generation
      Mechanics</strong></p>
      <ul>
      <li>‚≠êÔ∏è <a href="https://arxiv.org/abs/1904.09751">The Curious Case
      of Neural Text Degeneration</a> - <em>arXiv</em> (Holtzman et al.,
      2019) <em>(Introduces nucleus / top-p sampling)</em></li>
      </ul>
      <p><strong>Diffusion Based Approaches</strong></p>
      <ul>
      <li>[COMING SOON]</li>
      </ul>
      <section id="footnotes"
      class="footnotes footnotes-end-of-document" role="doc-endnotes">
      <hr />
      <ol>
      <li id="fn1"><p>Diffusion-based text generation offers an
      alternative to autoregressive prediction, see resources section
      (coming soon).<a href="#fnref1" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn2"><p><strong>Sociological Influence:</strong> How do we
      account for the way social and historical contexts shape judgments
      of novelty and creativity and is it a moving target?<br><br>
      Novelty and creativity are often historically and socially
      situated. A good deal of what constitutes creativity and novelty
      is dependent on the historical context in which artistic
      expressions are judged. Citizen Kane, for example, is often cited
      as one of the greatest films of all time due to its innovative
      cinematography and storytelling. However, the cinematic
      innovations that define this film, such as Toland‚Äôs use of depth
      of field, is now a staple in most introductory film courses.
      Fashion often follows a similar arc, innovative and fresh designs
      that mark the runway one season saturate the shelves of
      fast-fashion retailers the next.<br><br> Though judgments about
      creativity and artistic merit are heavily influenced by the social
      and historical factors there is still a sense in which great works
      are able to stand the test of time. When evaluating creative
      intelligence, we must consider how social and historical contexts
      shape our aesthetic judgments and distinguish between those that
      are fleeting and those that endure.<a href="#fnref2"
      class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn3"><p>Rorty‚Äôs <em>Contingency, Irony, and
      Solidarity</em> (1989) develops this idea at length, arguing that
      genuinely novel metaphors create new language games rather than
      play within existing ones.<a href="#fnref3" class="footnote-back"
      role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn4"><p>Holtzman et al.‚Äôs <em>The Curious Case of Neural
      Text Degeneration</em> (2019) explores this problem and introduces
      nucleus sampling as one approach.<a href="#fnref4"
      class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      <li id="fn5"><p>‚≠ê <em>Shoutout to my good friend Joshua for the
      stimulating convo, the amazing Shakespeare examples and great
      resources on literary devices + Richard Rorty. And shout-out to
      Henry too for the great convos on AGI/ARC and thoughts on
      diffusion-based and RL approaches. And last but not least shoutout
      to Noel for her core contributions on aesthetics and philosophical
      insights on creativity and intelligence</em><a href="#fnref5"
      class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
      </ol>
      </section>
    </article>
  </main>

  <!-- article JS (dark mode only) -->
  <script defer src="../../assets/js/article.js"></script>
</body>
</html>