<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Using Linear Models in Modern
Applications - Collins Westnedge</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no, viewport-fit=cover">
  
  <!-- Mobile app settings -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- SEO essentials -->
  <meta name="google-site-verification" content="hmW81pKLkPLJXY0kFe-I1vNXno-xD9XbEWyZyGfz-SA" />
  <meta name="description" content="A closer look at how using least
squares projection can offer a stable alternative to semantic
filtering.">
  <meta name="author" content="Collins Westnedge">
  <link rel="canonical" href="https://donkeyanaphora.github.io/articles/article2/">
  
  <!-- Hide drafts from search engines -->
  <meta name="robots" content="noindex, nofollow">

  <!-- Site look & feel -->
  <link rel="stylesheet" href="../../assets/css/main.css"/>
  <link rel="stylesheet" href="../../assets/css/article.css"/>
  
  <!-- Favicons -->
  <link rel="apple-touch-icon" sizes="180x180" href="../../favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../../favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../../favicon/favicon-16x16.png">
  <link rel="manifest" href="../../favicon/site.webmanifest">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://donkeyanaphora.github.io/articles/article2/">
  <meta property="og:title" content="Using Linear Models in Modern
Applications">
  <meta property="og:description" content="A closer look at how using
least squares projection can offer a stable alternative to semantic
filtering.">
  <meta property="og:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">
  <meta property="og:site_name" content="The Latent Realm">
  <meta property="article:author" content="Collins Westnedge">
  <meta property="article:published_time" content="2025-05-16">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:url" content="https://donkeyanaphora.github.io/articles/article2/">
  <meta name="twitter:title" content="Using Linear Models in Modern
Applications">
  <meta name="twitter:description" content="A closer look at how using
least squares projection can offer a stable alternative to semantic
filtering.">
  <meta name="twitter:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">

  <!-- MathJax config -->
  <!-- <script>
    window.MathJax = {
      tex:   { displayIndent: '0em',  displayAlign: 'center' },
      chtml: { mtextFont: 'Times' }
    };
  </script> -->
  
  <script>
    window.MathJax = {
      tex: {},  // Empty is fine
      chtml: { 
        mtextFont: 'Times',
        displayAlign: 'center',    // ‚úÖ Moved here
        displayIndent: '0em'       // ‚úÖ Moved here
      }
    };
  </script>
  <!-- MathJax -->
  <script defer id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js">
  </script>
</head>

<body class="article">
  <!-- floating buttons -->
  <button id="menuBtn" class="float-btn" aria-label="Open menu">üß∞</button>
  <button class="float-btn back-btn" onclick="window.location.href='../../'" aria-label="Back to home">üè°</button>
  <button id="toggleDark" class="float-btn" aria-label="Toggle dark mode">üåô</button>

  <!-- Markdown content -->
  <main class="content">
    <article class="markdown-body">
      <h1 id="using-linear-models-in-modern-applications">üìê Using
      Linear Models in Modern Applications</h1>
      <p><strong>COLLINS WESTNEDGE</strong><br />
      <em>MAY 16, 2025</em></p>
      <h2 id="introduction">Introduction</h2>
      <p>I‚Äôve been really busy so this will be a little informal but
      <strong>STILL WORTH THE READ</strong> because in the day to day
      landscape of people talking agentic AI, RAG systems, vector
      databases etc things like least squares regression and linear
      models tend to feel like dusty sklearn imports or the stuff of
      technical coding interviews. Despite the heavy focus on foundation
      model integration classic linear tools like ordinary least squares
      regression and its projection formula corrolary remain
      surprisingly powerful. By framing modern tasks like semantic
      filtering and bias removal as geometric problems, it becomes clear
      why simple formulas like <span class="math display">\[
      X\,(X^{\!\top}X)^{+}X^{\!\top}
      \]</span> work so well. The focus of this article is show how
      ordinary least squares offers insights as well as stable and
      theoretically grounded solution to semantic filtering in modern
      search and retrieval systems.</p>
      <h2 id="linalg-recap">Linalg Recap</h2>
      <p><em>lorem ipsum</em></p>
      <h3 id="key-formulas">Key Formulas</h3>
      <table>
      <colgroup>
      <col style="width: 33%" />
      <col style="width: 33%" />
      <col style="width: 33%" />
      </colgroup>
      <thead>
      <tr>
      <th>Formula</th>
      <th>Intuitive role</th>
      <th>Visual cue</th>
      </tr>
      </thead>
      <tbody>
      <tr>
      <td><span class="math inline">\(X^{+} =
      (X^{\!\top}X)^{-1}X^{\!\top}\)</span></td>
      <td>Recipe for weights. Solves <span
      class="math inline">\(\displaystyle\min_{\beta}\,\lVert y -
      X\beta\rVert_2^{\,2}\)</span>.</td>
      <td>How much of each arrow do I need?</td>
      </tr>
      <tr>
      <td><span class="math inline">\(\hat{\beta}=X^{+}y\)</span></td>
      <td>Coordinates along the arrows (scalar steps to reach the
      closest point).</td>
      <td>Labels on the sub-space axes.</td>
      </tr>
      <tr>
      <td><span class="math inline">\(P_X = X\,X^{+}\)</span></td>
      <td>Snap-to-span operator (orthogonal projector: symmetric,
      idempotent).</td>
      <td>Elevator lowering you to the sheet.</td>
      </tr>
      <tr>
      <td><span class="math inline">\(\hat{y}=P_Xy =
      X\hat{\beta}\)</span></td>
      <td>Foot of the perpendicular ‚Äî the unique point in <span
      class="math inline">\(\operatorname{span}(X)\)</span> minimizing
      <span class="math inline">\(\lVert
      y-\hat{y}\rVert_2\)</span>.</td>
      <td>Black dot where the plumb-line hits.</td>
      </tr>
      <tr>
      <td><span class="math inline">\(r = y-\hat{y} =
      (I-P_X)\,y\)</span></td>
      <td>Residual, orthogonal to every column of <span
      class="math inline">\(X\)</span> (<span
      class="math inline">\(X^{\!\top}r=0\)</span>).</td>
      <td>Red straight-down arrow.</td>
      </tr>
      </tbody>
      </table>
      <h3 id="theorems">Theorems</h3>
      <p><strong>Best Approximation Theorem</strong></p>
      <p>Let <span
      class="math inline">\(W\subseteq\mathbb{R}^{n}\)</span>, <span
      class="math inline">\(y\in\mathbb{R}^{n}\)</span>, and <span
      class="math inline">\(\widehat{y}=\operatorname{proj}_{W}(y)\)</span>.
      Then <span class="math display">\[
      \lVert y-\widehat{y}\rVert_2 \;&lt;\; \lVert y-v\rVert_2,
      \qquad\forall\,v\in W,\;v\neq\widehat{y}.
      \]</span> Hence <span class="math inline">\(y-\widehat{y}\in
      W^{\perp}\)</span> and is the closest vector in <span
      class="math inline">\(W^{\perp}\)</span> to <span
      class="math inline">\(y\)</span>.</p>
      <h3 id="take-away-corollary">Take-away Corollary</h3>
      <p>If <span
      class="math inline">\(\mathbf{u}_{\mathrm{proj}}=\operatorname{proj}_{V}(\mathbf{u})\)</span>,
      then <span class="math display">\[
      \mathbf{u}_{\mathrm{new}}
      =\mathbf{u}-\mathbf{u}_{\mathrm{proj}}
      \in V^{\perp}.
      \]</span> Thus <span
      class="math inline">\(\mathbf{u}_{\mathrm{new}}\)</span> is the
      closest-possible embedding to <span
      class="math inline">\(\mathbf{u}\)</span> that is perfectly
      orthogonal to <span class="math inline">\(V\)</span>.</p>
      <h4 id="tldr">TL;DR</h4>
      <p>Define <span class="math display">\[
      \mathbf{u}_{\mathrm{new}}
      =\arg\min_{w\in V^{\perp}}\lVert\mathbf{u}-w\rVert
      =\operatorname{proj}_{V^{\perp}}(\mathbf{u}).
      \]</span></p>
      <ol type="1">
      <li><strong>Minimal distortion</strong> ‚Äî <span
      class="math inline">\(\mathbf{u}_{\mathrm{new}}\)</span> is the
      closest vector in <span class="math inline">\(V^{\perp}\)</span>
      to <span class="math inline">\(\mathbf{u}\)</span>.<br />
      </li>
      <li><strong>Guaranteed orthogonality</strong> ‚Äî <span
      class="math inline">\(\langle\mathbf{u}_{\mathrm{new}}, v\rangle =
      0\)</span> for every <span class="math inline">\(v\in
      V\)</span>.</li>
      </ol>
      <h2 id="visual-intuition">Visual Intuition</h2>
      <h2 id="mean-subtraction-vs.-projection-residuals">Mean
      Subtraction vs.¬†Projection Residuals</h2>
      <h2 id="examples">Examples</h2>
      <h2 id="conclusion">Conclusion</h2>
    </article>
  </main>

  <!-- article JS (dark mode only) -->
  <script defer src="../../assets/js/article.js"></script>
</body>
</html>