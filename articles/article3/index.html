<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Extra-Ordinary Language - Collins Westnedge</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no, viewport-fit=cover">
  
  <!-- Mobile app settings -->
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <!-- SEO essentials -->
  <meta name="google-site-verification" content="hmW81pKLkPLJXY0kFe-I1vNXno-xD9XbEWyZyGfz-SA" />
  <meta name="description" content="An exploration of literary and
creative intelligence and things I wish language models did better.">
  <meta name="author" content="Collins Westnedge">
  <link rel="canonical" href="https://donkeyanaphora.github.io/articles/article3/">
  
  <!-- Hide drafts from search engines -->
  

  <!-- Site look & feel -->
  <link rel="stylesheet" href="../../assets/css/main.css"/>
  <link rel="stylesheet" href="../../assets/css/article.css"/>
  
  <!-- Favicons -->
  <link rel="apple-touch-icon" sizes="180x180" href="../../favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../../favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../../favicon/favicon-16x16.png">
  <link rel="manifest" href="../../favicon/site.webmanifest">

  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:url" content="https://donkeyanaphora.github.io/articles/article3/">
  <meta property="og:title" content="Extra-Ordinary Language">
  <meta property="og:description" content="An exploration of literary
and creative intelligence and things I wish language models did
better.">
  <meta property="og:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">
  <meta property="og:site_name" content="The Latent Realm">
  <meta property="article:author" content="Collins Westnedge">
  <meta property="article:published_time" content="2025-07-02">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:url" content="https://donkeyanaphora.github.io/articles/article3/">
  <meta name="twitter:title" content="Extra-Ordinary Language">
  <meta name="twitter:description" content="An exploration of literary
and creative intelligence and things I wish language models did
better.">
  <meta name="twitter:image" content="https://donkeyanaphora.github.io/assets/images/thumbnail.png">

  <!-- MathJax config -->
  <!-- <script>
    window.MathJax = {
      tex:   { displayIndent: '0em',  displayAlign: 'center' },
      chtml: { mtextFont: 'Times' }
    };
  </script> -->
  
  <script>
    window.MathJax = {
      tex: {},  // Empty is fine
      chtml: { 
        mtextFont: 'Times',
        displayAlign: 'center',    // ‚úÖ Moved here
        displayIndent: '0em'       // ‚úÖ Moved here
      }
    };
  </script>
  <!-- MathJax -->
  <script defer id="MathJax-script"
          src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-chtml.js">
  </script>
</head>

<body class="article">
  <!-- floating buttons -->
  <button id="menuBtn" class="float-btn" aria-label="Open menu">üß∞</button>
  <button class="float-btn back-btn" onclick="window.location.href='../../'" aria-label="Back to home">üè°</button>
  <button id="toggleDark" class="float-btn" aria-label="Toggle dark mode">üåô</button>

  <!-- Markdown content -->
  <main class="content">
    <article class="markdown-body">
      <h1 id="extra-ordinary-language">üé≠ Extra-Ordinary Language</h1>
      <p><strong>COLLINS WESTNEDGE</strong><br />
      <em>JULY 2, 2025</em></p>
      <h2 id="introduction">Introduction</h2>
      <p>I‚Äôve been thinking about a conversation I had with a former
      colleague about language that defies ordinary usage. More
      specifically expressions that violate the <a
      href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional
      hypothesis</a> of language and yet carry high degrees of
      intentionality and depth.</p>
      <p>When Kafka wrote ‚ÄúA cage went in search of a bird,‚Äù he created
      something that on the surface seems impossible and yet expresses a
      profound and insidious truth. Current AI systems, for all their
      linguistic sophistication, rarely produce such language. They
      excel at coherent, informative prose but struggle with the kind of
      intentional violations that define great literature.</p>
      <p>In this post, I‚Äôm dog-earing these thoughts to revisit later.
      The aim here is to understand what makes these expressions work
      and more critically, the implications this has on how we measure
      and evaluate model intelligence.</p>
      <!-- Along the way, I'll attempt some preliminary formalizations and speculate about why current AI systems struggle with this capacity. -->
      <h2 id="literary-examples">Literary Examples</h2>
      <blockquote>
      <p>So they lov‚Äôd, as love in twain<br />
      Had the essence but in one;<br />
      Two distincts, division none:<br />
      Number there in love was slain.</p>
      <p>Hearts remote, yet not asunder;<br />
      Distance and no space was seen<br />
      Twixt this Turtle and his queen</p>
      <p>‚Äî <em>Shakespeare, <strong>The Phoenix and the
      Turtle</strong></em></p>
      </blockquote>
      <blockquote>
      <p>A cage went in search of a bird.</p>
      <p>‚Äî <em>Kafka, <strong>Aphorisms</strong></em></p>
      </blockquote>
      <blockquote>
      <p>I can‚Äôt go on. I‚Äôll go on.</p>
      <p>‚Äî <em>Beckett, <strong>The Unnamable</strong></em></p>
      </blockquote>
      <blockquote>
      <p>Merry and tragical! Tedious and brief!<br />
      That is, hot ice and wondrous strange snow.<br />
      How shall we find the concord of this discord?</p>
      <p>‚Äî <em>Shakespeare, <strong>A Midsummer Night‚Äôs
      Dream</strong></em></p>
      </blockquote>
      <h3 id="core-characteristics">Core Characteristics</h3>
      <p>Because of their intentionality and depth, I‚Äôm going to call
      these violations of ‚Äúordinary use‚Äù <strong>extra-ordinary
      use</strong>, or <strong>ExO language</strong>. They don‚Äôt obscure
      meaning; instead, they elucidate by way of contradiction or
      violated expectation. These literary examples share a unifying
      feature: they present a <strong>literal semantic failure</strong>
      in one domain that creates insightful or profound resonance in
      another (metaphorical, allegorical, or abstract).</p>
      <p>As David Foster Wallace said, ‚Äúwe all know there‚Äôs no quicker
      way to empty a joke of its particular magic than to explain it,‚Äù
      and the same is true for figurative language. Still, it‚Äôs worth
      illustrating how these expressions <em>may</em> resolve through a
      figurative interpretation, albeit with some loss of ‚Äúmagic‚Äù:</p>
      <ul>
      <li>‚ÄúTwo distincts, division none‚Äù - two bodies, one soul</li>
      <li>‚ÄúA cage went in search of a bird‚Äù - oppression seeks
      freedom</li>
      <li>‚ÄúI can‚Äôt go on. I‚Äôll go on.‚Äù - I don‚Äôt choose to continue;
      it‚Äôs compulsory</li>
      </ul>
      <p>In each case, the reader encounters a jarring violation of
      expectation that is only resolved through a figurative
      reinterpretation. One way to capture that pattern is:</p>
      <p><span class="math display">\[
      \text{‚ü¶œÜ‚üß}_{\text{literal}} = \bot \quad ; \quad
      \text{‚ü¶œÜ‚üß}_{\text{figurative}} = \psi
      \]</span></p>
      <p>Here, ‚ü¶œÜ‚üß denotes the interpretation of expression œÜ, ‚ä•
      indicates contradiction, and œà the emergent or resolved meaning.
      Framed this way, two of ExO language‚Äôs core properties are built
      directly into the equation:</p>
      <ul>
      <li><strong>Surface-level anomaly</strong>: under a literal
      reading, the expression collapses, i.e., ( <span
      class="math inline">\(\text{‚ü¶œÜ‚üß}_{\text{lit}}=\bot\)</span> ) (a
      clash of type, animacy, modality, physics, selectional
      restrictions, etc.).</li>
      <li><strong>Recoverability</strong>: despite that collapse, a
      coherent reading exists, i.e., there is some (<span
      class="math inline">\(\psi\)</span>) such that (<span
      class="math inline">\(\text{‚ü¶œÜ‚üß}_{\text{fig}}=\psi\)</span>).</li>
      </ul>
      <p>However, in practice, strong ExO lines also tend to carry:</p>
      <ul>
      <li><strong>Intent signal</strong>: the violation feels deliberate
      and motivated within the larger context.</li>
      <li><strong>Irreducibility</strong>: paraphrase diminishes the
      associative effects (the deviation is doing semantic work that a
      literal restatement can‚Äôt).</li>
      </ul>
      <p>Ultimately, these examples illustrate how expressions can
      fracture under a literal reading yet resolve in an imaginative
      one. Making that fracture or violation explicit clarifies how
      objectives rooted in the distributional hypothesis, e.g.,
      next-token prediction or RLHF tuned for coherence and readability
      could steer models away from ExO language. If we want systems that
      embrace deliberate, meaningful rule-bending, we‚Äôll need benchmarks
      that more actively reward it.</p>
      <h2 id="why-might-current-ai-struggle-here">Why Might Current AI
      Struggle Here?</h2>
      <p>Current language models face several systematic barriers to
      producing ExO language; at this point many of these are my own
      thoughts or fan theory rather than concrete fact, but nevertheless
      here they are:</p>
      <p><strong>Data Scarcity in Pretraining</strong>: Though profound
      literature exists in pretraining corpora (Google Books, etc.),
      it‚Äôs statistically underrepresented. By definition, novel writing
      is rare, and easily-licensed conventional text dominates the
      training mix. Even within Pulitzer Prize winning articles/books
      etc the instances of truly profound prose/ExO language (as
      impactful as they may be) are few and far between.</p>
      <p><strong>Objective Mismatch</strong>: From a causal language
      modeling perspective, next token prediction is less about encoding
      the abstract concepts or deep intentionality these examples are
      made up of and more so about emulation of style and prose. At this
      phase models learn to reproduce surface features without encoding
      the abstract concepts that necessarily drive literary innovation.
      Even though large causal models like GPT-3 begin to exhibit some
      few-shot behavior with sufficient examples, it seems unlikely that
      the causal training paradigm alone gets us the abstract
      associative power necessary for truly novel language.</p>
      <p><strong>Task Absence During Fine-tuning</strong>: When models
      are optimized for instruction following, there‚Äôs likely an absence
      of tasks that push them to not just learn ExO behavior, but more
      importantly exhibit it. The training emphasizes practical
      capabilities over creative linguistic reasoning. Though literary
      analysis and reading comprehension are a big part of this phase
      they are somewhat distinct from the task of exhibiting and
      producing novel prose. In short, there are more analyses of great
      works than great works, and the reading comprehension/literary
      analysis task itself aligns more intuitively with how we quantify
      intelligence in school.</p>
      <!-- 
      **RLHF Optimization Pressure**: This one is fun to think about. From a preference learning perspective, I doubt anyone wants to do full-blown Harold Bloom literary analysis to rate model outputs. Most annotators would favor accessible, Wikipedia-style entries over Joycean explorations of any topic. This optimization pressure could easily eliminate whatever literary capabilities emerge during pretraining. There are also many studies that suggest RLHF can reduce a model's output diversity, which can be found in the key readings section further down.

      Studies indicate RLHF improves generalization on benchmark tasks at the cost of reduced output diversity, suggesting a tradeoff between the two. The interesting case is what happens within the set of valid solutions. For tasks that admit many correct answers, does RLHF collapse probability mass onto a narrow set of conventional solutions/model responses? If so, this would work directly against fluid intelligence, which, following Chollet, should exhibit uniform probability across equally valid outputs while suppressing invalid ones. ExO language is precisely the kind of output that's valid but unconventional: high on the "correct but rare" end of the distribution, and thus vulnerable to being optimized away.


      **The Deeper Issue: Fluid Literary Intelligence**: The more I examine instances of impactful prose packed with intentionality and metaphysical depth, the more convinced I am that modeling such language requires what could be called fluid literary intelligence. This goes beyond pattern matching toward adaptive generalization on out-of-distribution linguistic tasks. An ability to traverse distant conceptual pathways, pathways that have not surfaced during pretraining. This likely overlaps with the intuition that scaling up inference time compute improves reasoning models by allowing them to perform a more exhaustive search of the solution space. However, as far as I can tell this strategy mostly applies to verifiable tasks and thus creative analogical reasoning in a literary or ExO capacity may remain a blindspot. 

      In short, ExO language appears to depend on advanced, or at least highly creative, analogical reasoning. Its defining feature is the construction of meaningful and unexpected connections between disparate, and often seemingly contradictory, concepts, connections that resist literal validation but nonetheless produce insight.  -->
      <p><strong>RLHF Optimization Pressure</strong>: This one is fun to
      think about. From a preference learning perspective, I doubt
      anyone wants to do full-blown Harold Bloom literary analysis to
      rate model outputs. Most annotators would favor accessible,
      Wikipedia-style entries over Joycean explorations of any topic.
      That optimization pressure could easily eliminate whatever
      literary capabilities emerge during pretraining. There are also
      many studies that suggest RLHF can reduce a model‚Äôs output
      diversity (see key readings section below).</p>
      <p>Some studies frame this as a generalization‚Äìdiversity tradeoff
      rather than a defect. However, the interesting confound is what
      happens within the set of valid solutions. For tasks that admit
      many correct answers, does RLHF squish probability mass onto a
      narrow set of conventional solutions/model responses? If so, this
      would work directly against the kind of ‚Äúfluid‚Äù intelligence that
      (following Chollet) shouldn‚Äôt collapse onto a single normative
      solution when multiple answers are equally defensible. ExO
      language is precisely the kind of output that‚Äôs valid but
      unconventional, high on the ‚Äúvalid, meaningful and rare‚Äù end of
      the distribution, and thus vulnerable to being optimized away.</p>
      <p><strong>A Concrete Example: Analogical Reasoning on IQ
      Tests</strong>: Models already score well on linguistic similarity
      tasks (how are X and Y alike?) like those found on the WAIS, which
      are intended to measure analogical reasoning capabilities. Ask a
      model ‚Äúwhat is similar between a horse and a tiger?‚Äù, you‚Äôll
      likely get the sociologically correct ‚Äúhive mind answer‚Äù: ‚Äúboth
      are mammals.‚Äù RLHF likely helps the model here in that it pushes
      outputs toward sociological consensus. But when my friend was
      asked the same question, he joked: ‚Äúthey‚Äôre both one step away
      from a zebra.‚Äù That answer is wrong by IQ test scoring standards,
      but it‚Äôs arguably closer to ExO thinking: an unexpected connection
      that reframes both concepts through a third.</p>
      <p>If RLHF concentrates probability mass toward canonical answers
      even when multiple interpretations could be defensible and
      arguably more interesting, it may be penalizing the unconventional
      links that make ExO language distinctive and profound. To state
      this in our ExO terminology from earlier, similarity tests are
      built to reward convergence toward categorical abstraction, not
      divergence toward imaginative reframing.</p>
      <p><strong>The Deeper Issue: Fluid Literary Intelligence</strong>:
      The more I examine instances of impactful prose packed with
      intentionality and metaphysical depth, the more convinced I am
      that modeling such language requires what could be called fluid
      literary intelligence. This goes beyond pattern matching toward
      adaptive generalization on out-of-distribution linguistic tasks.
      An ability to traverse distant conceptual pathways, pathways that
      have not surfaced during pretraining. This likely overlaps with
      the intuition that scaling up inference time compute improves
      reasoning models by allowing them to perform a more exhaustive
      search of the solution space. However, as far as I can tell this
      strategy mostly applies to verifiable tasks and thus creative
      analogical reasoning in a literary or ExO capacity may remain a
      blindspot.</p>
      <p>In short, ExO language appears to depend on advanced, or at
      least highly creative, analogical reasoning. Its defining feature
      is the construction of meaningful and unexpected connections
      between disparate, and often seemingly contradictory or
      mismatched, concepts, connections that resist literal
      interpretations but nonetheless produce insight.</p>
      <p><strong>Missing Benchmarks</strong>: This leads to deeper
      questions: What constitutes literary novelty computationally and
      why are there no benchmarks on par with ARC that touch this axis
      of intelligence? The ARC datasets I‚Äôve looked at resemble the
      visual matrix reasoning tasks on WAIS-style IQ tests. However, as
      far as I‚Äôm aware there is no linguistic equivalent, specifically,
      one that rewards unconventional yet valid analogical reasoning.
      More generally, we likely need a linguistic variant of ARC in the
      spirit of measuring efficient generalization, capacities that
      can‚Äôt be easily ‚Äúbench-maxed‚Äù by practitioners targeting the test
      set. For now, reasoning evaluation has heavily favored verifiable
      tasks (coding, math) over creative reasoning.</p>
      <h2 id="key-readings">Key‚ÄØReadings</h2>
      <p>These papers and essays collectively help explore
      <strong>extra‚Äëordinary‚ÄØ(ExO) language</strong>: RLHF studies
      detail how reward shaping affects diversity, Chollet‚Äôs work
      addresses fluid intelligence and efficient generalization,
      lingustic/philosophical perspectives on metaphor, analogy, and
      contradiction concepts central to creativity and nuanced language
      use.</p>
      <p><strong>RLHF, Mode Collapse &amp; Output Diversity</strong></p>
      <ul>
      <li><a href="https://arxiv.org/abs/2406.05587">Creativity Has Left
      the Chat: The Price of Debiasing Language Models</a> -
      <em>arXiv</em> (2024)</li>
      <li><a
      href="https://openreview.net/pdf?id=PXD3FAVHJT">Understanding the
      Effects of RLHF on LLM Generalisation and Diversity</a> - <em>ICLR
      / OpenReview</em> ‚≠êÔ∏è</li>
      <li><a
      href="https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse">Mysteries
      of Mode Collapse</a> - <em>LessWrong essay</em></li>
      </ul>
      <p><strong>Exo-Language, Analogical &amp; Fluid
      Reasoning</strong></p>
      <ul>
      <li><p><a href="https://arxiv.org/abs/1911.01547">On the Measure
      of Intelligence</a> - <em>arXiv</em> (Chollet, 2019) ‚≠êÔ∏è</p></li>
      <li><p><a href="https://arxiv.org/abs/2403.11810">Metaphor
      Understanding Challenge Dataset for LLMs</a> - <em>arXiv</em>
      (2024)</p></li>
      <li><p><a
      href="https://aclanthology.org/2025.naacl-long.561.pdf">One fish,
      two fish, but not the whole sea: Alignment reduces language
      models‚Äô conceptual diversity</a> - <em>NAACL 2025</em> ‚≠êÔ∏è</p></li>
      <li><p><a href="https://gwern.net/creative-benchmark">Towards
      Benchmarking LLM Diversity &amp; Creativity</a> -
      <em>Gwern.net</em></p></li>
      </ul>
      <p><strong>Creative Language &amp; Literary Ability in
      LLMs</strong></p>
      <ul>
      <li><a href="https://arxiv.org/abs/2312.03746">Evaluating Large
      Language Model Creativity from a Literary Perspective</a> -
      <em>arXiv</em> (2023)</li>
      <li><a
      href="https://www.nature.com/articles/s41599-025-04999-2">Rethinking
      Literary Creativity in the Digital Age: Human vs.¬†AI
      Playwriting</a> ‚Äî <em>Humanities &amp; Social Sciences
      Communications (Nature)</em> ‚≠êÔ∏è</li>
      <li><a
      href="https://www.sciencedirect.com/science/article/pii/S1871187125001191">Large
      language models show both individual and collective creativity
      comparable to humans</a> - <em>ScienceDirect</em></li>
      </ul>
      <p><strong>Sampling, Diversity &amp; Generation
      Mechanics</strong></p>
      <ul>
      <li><a href="https://arxiv.org/abs/1904.09751">The Curious Case of
      Neural Text Degeneration</a> - <em>arXiv</em> (Holtzman et al.,
      2019) ‚≠êÔ∏è <em>(Introduces nucleus / top-p sampling)</em></li>
      </ul>
      <p><strong>Philosophy of Language &amp; Logic</strong></p>
      <ul>
      <li><a
      href="https://plato.stanford.edu/entries/metaphor/">Metaphor</a> -
      <em>Stanford Encyclopedia of Philosophy</em></li>
      <li><a
      href="https://plato.stanford.edu/entries/contradiction/">Contradiction</a>
      - <em>Stanford Encyclopedia of Philosophy</em></li>
      <li><a
      href="https://en.wikipedia.org/wiki/Catachresis">Catachresis</a> -
      Wikipedia</li>
      <li><a
      href="https://en.wikiversity.org/wiki/Information?utm_source=chatgpt.com">Exformation</a>
      - Wikiversity</li>
      </ul>
      <h2 id="closing-thoughts">Closing Thoughts</h2>
      <p><strong>Sociological Influence:</strong> How do we account for
      the way social and historical contexts shape judgements of novelty
      and creativity and is it a moving target?</p>
      <p>Novelty and creativity are often historically and socially
      situated. A good deal of what constitutes creativity and novelty
      is dependent on the historical context in which artistic
      expressions are judged. Citizen Kane, for example, is often cited
      as one of the greatest films of all time due to its innovative
      cinematography and storytelling. However, the cinematic
      innovations that define this film, such as Toland‚Äôs use of depth
      of field, is now a staple in most introductory film courses.
      Fashion often follows a similar arc, innovative and fresh designs
      that mark the runway one season saturate the shelves of
      fast-fashion retailers the next.</p>
      <p>Though judgements about creativity and artistic merit are
      heavily influenced by the social and historical factors there is
      still a sense in which great works are able to stand the test of
      time. When evaluating creative intelligence, we must consider how
      social and historical contexts shape our aesthetic judgements and
      distinguish between those that are fleeting and those that
      endure.</p>
      <p><strong>Final Thoughts:</strong> Just because models can
      exhibit surprisal or violate semantic expectations doesn‚Äôt always
      mean they possess the ability to do so meaningfully. Ultimately,
      the goal is to understand whether machines can develop the kind of
      flexible, creative intelligence that ExO language represents‚Äîand
      to build evaluation frameworks that recognize this intelligence
      when it emerges. In short, we need benchmarks that reward
      ‚Äúwondrous strange snow.‚Äù</p>
      <hr />
      <!-- [^1]: *Theseus sets up an open contradiction **hot** + **ice** only to ‚Äúresolve‚Äù it with the sneering flourish ‚Äúwondrous strange snow.‚Äù  
            The oxymoron stays physically impossible, but the new name lets the audience picture it for a moment as an imaginable marvel.  
            I think this imaginative license would still make it a Modal-Projection (MP).* -->
      <p><em>Shoutout to my good friend Joshua for the stimulating convo
      and amazing <strong>Midsummer</strong> example. And shout-out to
      Henry too for the great convos on AGI/ARC and thoughts on
      diffusion-based and RL approaches. And last but not least shoutout
      to Noel for her core contributions on aesthetics and philosophical
      insights on creativity and intelligence</em></p>
      <!-- [^2]: The table below shows loosely how the literary ExO examples behave across two interpretive models:

      - **M<sub>phys</sub>**: Physical/literal interpretation (common sense, ordinary meaning)
      - **M<sub>interp</sub>**: Creative interpretation (metaphor, allegory, figurative readings)

      | Passage | Pattern | Formal Notation | Plain English |
      |---------|---------|-----------------|---------------|
      | *"Two distincts, division none"* | Modal Clash | M<sub>phys</sub> ‚ä® Distinct(a,b) ‚àß M<sub>interp</sub> ‚ä® ¬¨Distinct(a,b) | Two bodies, one soul |
      | *"Hearts remote, yet not asunder"* | Modal Clash | M<sub>phys</sub> ‚ä® Remote(a,b) ‚àß M<sub>interp</sub> ‚ä® ¬¨Remote(a,b) | Spatially apart, spiritually united |
      | *"Hot ice / wondrous strange snow"* | Modal Projection | M<sub>phys</sub> ‚ä® ¬¨(Ice ‚àß Hot) ‚àß M<sub>interp</sub> ‚ä≠ ¬¨(Ice ‚àß Hot) | Physically impossible, imaginatively conceivable |
      | *"A cage went in search of a bird"* | Type Clash | M<sub>phys</sub>: ¬¨Animate(cage) ‚àß Search(cage, bird) ‚áí ‚ä• | Cages can't search; metaphorically, oppression pursues freedom |
      | *"I can't go on. I'll go on."* | Modal Clash | M<sub>phys</sub> ‚ä® ¬¨Able(x, continue) ‚àß M<sub>interp</sub> ‚ä® Continue(x) | Logical contradiction; existential persistence despite impossibility | -->
    </article>
  </main>

  <!-- article JS (dark mode only) -->
  <script defer src="../../assets/js/article.js"></script>
</body>
</html>