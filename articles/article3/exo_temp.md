## Revised Article Outline

**Title**: "Beyond Surface: Does RLHF Reduce Reasoning Diversity in Language Models?"
*Subtitle: "From Literary Creativity to Measurable Cognitive Constraints"*

### 1. Opening Hook
- RLHF models feel "stuck in attractors/gravity wells" - predictable, convergent outputs
- But does this surface uniformity reflect deeper reasoning constraints?
- We can test this with a simple observation: many problems have multiple correct answers

### 2. Motivating Examples (Streamlined)
- 2-3 ExO examples (Kafka's cage, Shakespeare's paradoxes)
- These represent extreme divergent reasoning - maximum semantic leaps
- Transition: "But literary creativity is subjective. Let's look at problems with verifiable answers."

### 3. The Core Insight
- **Current literature**: RLHF reduces output diversity (stylistic, lexical, semantic)
- **Missing distinction**: Surface diversity vs. reasoning diversity
- **Key question**: When multiple solutions are equally correct, do RLHF models still converge?

### 4. Hypothesis & Method

**H1 (Intrinsic Diversity-within-Validity)**  
For tasks that admit many verifiable correct answers, the distribution of first-choice valid solutions generated by a base checkpoint is broader than that of its RLHF-aligned sibling.

| Task | Prompt example | Deterministic validity oracle |
|------|----------------|------------------------------|
| Prime-pair decomposition | "Give one pair of primes that sums to 20,000." | Primality test + sum check |
| Regex word match | "Provide one English word that matches b[^aeiou]t." | re.fullmatch + dictionary lookup |
| Anagram retrieval | "Name an English anagram of 'SCARE'." | Spell-check + sorted-letters equality |

**Methodology**:
- Compare base vs RLHF versions of same model family
- Measure: distribution entropy, unique response counts, clustering patterns
- N=1000 samples per task, controlled temperature

### 5. Implications

**If confirmed** (RLHF reduces reasoning diversity):
- Impact on scientific discovery, creative problem-solving
- Need for alignment methods that preserve cognitive flexibility
- Rethink benchmarks - move beyond single-answer convergent tasks

**If null** (No difference in reasoning diversity):
- RLHF's diversity impact is primarily stylistic, not cognitive
- Creativity concerns may be overstated for actual problem-solving
- Literary challenges reflect different limitations than reasoning constraints

### 6. Conclusion
- Connect back to ExO language as extreme case
- Call for benchmarks that measure and reward valid divergence
- Position reasoning diversity as overlooked axis of model evaluation

## Possible Design

### **H1 (Intrinsic Diversity-within-Validity)**

> **For tasks that admit many verifiable correct answers, the distribution of *first-choice* valid solutions generated by a base checkpoint is broader than that of its RLHF-aligned sibling.**

---

### 1 . Operational definition

| Term                          | Precise meaning in this study                                                                                                                                                                                                                                |
| ----------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| *First-choice valid solution* | The model’s **single answer** to a prompt, produced under fixed decoding parameters (*T* = 0.8, top-p = 0.95, max\_len = 64) with no instruction to enumerate.                                                                                               |
| *Intrinsic diversity*         | The **number of distinct valid solutions** recovered when the same prompt is sampled **k = 10** times with different random seeds.  Variation is therefore driven by the model’s internal probability mass, not an explicit “give five answers” instruction. |
| *Broader distribution*        | Higher conditional-diversity metric: `distinct-n` (n = 4 on canonicalised answer tokens) or, equivalently, higher entropy of the empirical solution histogram.                                                                                               |

---

### 2 . Formal statistical test

Let $D_{\text{Base}}$ and $D_{\text{RLHF}}$ denote the mean distinct-4 counts across the prompt set (after filtering to valid answers).
Hypothesis pair:

* **H0**: $D_{\text{Base}} \le D_{\text{RLHF}}$
* **H1** (stated above): $D_{\text{Base}} > D_{\text{RLHF}}$

Assess with a paired bootstrap (10 000 resamples); report the 95 % CI and p-value.

---

### 3 . Tasks used **solely** for H1

| Task                         | Prompt example (single-answer)                        | Deterministic validity oracle         |
| ---------------------------- | ----------------------------------------------------- | ------------------------------------- |
| **Prime-pair decomposition** | “Give one pair of primes that sums to **20 000**.”    | Primality test + sum check            |
| **Regex word match**         | “Provide one English word that matches `b[^aeiou]t`.” | `re.fullmatch` + dictionary lookup    |
| **Anagram retrieval**        | “Name an English anagram of **‘SCARE’**.”             | Spell-check + sorted-letters equality |

*(Each prompt is sampled 10 ×; diversity measured across valid outputs.)*

---

### 4 . Why this framing fixes the “forced diversity” concern

* The prompt requests **exactly one** answer, so any variation across seeds reflects the **model’s own uncertainty** over the valid solution space.
* Compliance skill (ability to list five) no longer inflates diversity metrics.
* The validity filter guarantees that only correct answers enter the diversity count.

---

### 5 . Relationship to your other hypotheses

| ID                                 | Status after reframing                                                                              |
| ---------------------------------- | --------------------------------------------------------------------------------------------------- |
| **H2 (Metaphor Quality)**          | Unchanged. Diversity is not the metric; human apt+novel ratings are.                                |
| **H3 (Closed-form WAIS accuracy)** | Unchanged. Single-answer accuracy still tests whether RLHF helps or hurts conventional abstraction. |

---

## Objective rationale for reviewers

1. **Isolates intrinsic uncertainty** rather than instruction-induced enumeration.
2. **Aligns with prior diversity studies** that sample multiple times per prompt (e.g., *One fish, two fish*), allowing direct comparison.
3. **Keeps statistical power**: ten seeds × 50–100 prompts per task gives tight CIs with no extra human annotation.

---

**Action item:** replace the old H1 paragraph in the Methods section with this reframed version, update the sampling description for the three “many-answer” tasks, and leave all other sections intact.
